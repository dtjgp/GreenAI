{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ad276f",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 深度卷积神经网络（AlexNet）\n",
    ":label:`sec_alexnet`\n",
    "\n",
    "在LeNet提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但卷积神经网络并没有主导这些领域。这是因为虽然LeNet在小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究。事实上，在上世纪90年代初到2012年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机（support vector machines）。\n",
    "\n",
    "在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。\n",
    "\n",
    "虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。\n",
    "\n",
    "因此，与训练*端到端*（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：\n",
    "\n",
    "1. 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）。\n",
    "2. 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。\n",
    "3. 通过标准的特征提取算法，如SIFT（尺度不变特征变换） :cite:`Lowe.2004`和SURF（加速鲁棒特征） :cite:`Bay.Tuytelaars.Van-Gool.2006`或其他手动调整的流水线来输入数据。\n",
    "4. 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。\n",
    "\n",
    "当人们和机器学习研究人员交谈时，会发现机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而，当人们和计算机视觉研究人员交谈，会听到一个完全不同的故事。计算机视觉研究人员会告诉一个诡异事实————推动领域进步的是数据特征，而不是学习算法。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。\n",
    "\n",
    "## 学习表征\n",
    "\n",
    "另一种预测这个领域发展的方法————观察图像特征的提取方法。在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。SIFT :cite:`Lowe.2004`、SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`、HOG（定向梯度直方图） :cite:`Dalal.Triggs.2005`、[bags of visual words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)和类似的特征提取方法占据了主导地位。\n",
    "\n",
    "另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，想法则与众不同：他们认为特征本身应该被学习。此外，他们还认为，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。事实上，Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体*AlexNet*。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，他是论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`的第一作者。\n",
    "\n",
    "有趣的是，在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。 :numref:`fig_filters`是从AlexNet论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`复制的，描述了底层图像特征。\n",
    "\n",
    "![AlexNet第一层学习到的特征抽取器。](../img/filters.png)\n",
    ":width:`400px`\n",
    ":label:`fig_filters`\n",
    "\n",
    "AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如眼睛、鼻子、草叶等等。而更高的层可以检测整个物体，如人、飞机、狗或飞盘。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。\n",
    "\n",
    "### 缺少的成分：数据\n",
    "\n",
    "包含许多特征的深度模型需要大量的有标签数据，才能显著优于基于凸优化的传统方法（如线性方法和核方法）。\n",
    "然而，限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张在非自然环境下以低分辨率拍摄的图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。\n",
    "\n",
    "### 缺少的成分：硬件\n",
    "\n",
    "深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据。这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。然而，用GPU训练神经网络改变了这一格局。*图形处理器*（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的$4 \\times 4$矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为*通用GPU*（general-purpose GPUs，GPGPU）来销售。\n",
    "\n",
    "那么GPU比CPU强在哪里呢？\n",
    "\n",
    "首先，我们深度理解一下中央处理器（Central Processing Unit，CPU）的*核心*。\n",
    "CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。\n",
    "它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。\n",
    "然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。\n",
    "它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。\n",
    "现代笔记本电脑最多有4核，即使是高端服务器也很少超过64核，因为它们的性价比不高。\n",
    "\n",
    "相比于CPU，GPU由$100 \\sim 1000$个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。\n",
    "虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级。\n",
    "例如，NVIDIA最近一代的Ampere GPU架构为每个芯片提供了高达312 TFlops的浮点性能，而CPU的浮点性能到目前为止还没有超过1 TFlops。\n",
    "之所以有如此大的差距，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。\n",
    "对于一个CPU核心，假设它的运行速度比GPU快4倍，但可以使用16个GPU核代替，那么GPU的综合性能就是CPU的$16 \\times 1/4 = 4$倍。\n",
    "其次，GPU内核要简单得多，这使得它们更节能。\n",
    "此外，深度学习中的许多操作需要相对较高的内存带宽，而GPU拥有10倍于CPU的带宽。\n",
    "\n",
    "回到2012年的重大突破，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。\n",
    "于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)几年来它一直是行业标准，并推动了深度学习热潮。\n",
    "\n",
    "## AlexNet\n",
    "\n",
    "2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。\n",
    "AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。\n",
    "\n",
    "AlexNet和LeNet的架构非常相似，如 :numref:`fig_alexnet`所示。\n",
    "注意，本书在这里提供的是一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。\n",
    "\n",
    "![从LeNet（左）到AlexNet（右）](../img/alexnet.svg)\n",
    ":label:`fig_alexnet`\n",
    "\n",
    "AlexNet和LeNet的设计理念非常相似，但也存在显著差异。\n",
    "\n",
    "1. AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。\n",
    "2. AlexNet使用ReLU而不是sigmoid作为其激活函数。\n",
    "\n",
    "下面的内容将深入研究AlexNet的细节。\n",
    "\n",
    "### 模型设计\n",
    "\n",
    "在AlexNet的第一层，卷积窗口的形状是$11\\times11$。\n",
    "由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。\n",
    "第二层中的卷积窗口形状被缩减为$5\\times5$，然后是$3\\times3$。\n",
    "此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为$3\\times3$、步幅为2的最大汇聚层。\n",
    "而且，AlexNet的卷积通道数目是LeNet的10倍。\n",
    "\n",
    "在最后一个卷积层后有两个全连接层，分别有4096个输出。\n",
    "这两个巨大的全连接层拥有将近1GB的模型参数。\n",
    "由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。\n",
    "幸运的是，现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型（因此，本书的AlexNet模型在这方面与原始论文稍有不同）。\n",
    "\n",
    "### 激活函数\n",
    "\n",
    "此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。\n",
    "一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。\n",
    "另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。\n",
    "当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。\n",
    "相反，ReLU激活函数在正区间的梯度总是1。\n",
    "因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。\n",
    "\n",
    "### 容量控制和预处理\n",
    "\n",
    "AlexNet通过暂退法（ :numref:`sec_dropout`）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。\n",
    "为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。\n",
    "这使得模型更健壮，更大的样本量有效地减少了过拟合。\n",
    "在 :numref:`sec_image_augmentation`中更详细地讨论数据扩增。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ae34b7",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "    # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "    # 另外，输出通道的数目远大于LeNet\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "    # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "    nn.Linear(6400, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "    nn.Linear(4096, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcff903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片大小为224*224，核大小为11，步幅为4，填充为1，输出通道为96，每次输出的大小为\n",
    "# (224-11+4*1)/4+1=54，即54*54*96，参数数量为(11*11*1+1)*96=11712\n",
    "# 接下来输入通道为96，输出通道为256，核大小为5，填充为2，步幅为1，输出大小为\n",
    "# (54-5+2*2)/1+1=27，即27*27*256，参数数量为(5*5*96+1)*256=614656\n",
    "# 接下来输入通道为256，输出通道为384，核大小为3，填充为1，步幅为1，输出大小为\n",
    "# (27-3+2*1)/1+1=27，即27*27*384，参数数量为(3*3*256+1)*384=885120\n",
    "# 接下来输入通道为384，输出通道为384，核大小为3，填充为1，步幅为1，输出大小为\n",
    "# (27-3+2*1)/1+1=27，即27*27*384，参数数量为(3*3*384+1)*384=1327488\n",
    "# 接下来输入通道为384，输出通道为256，核大小为3，填充为1，步幅为1，输出大小为\n",
    "# (27-3+2*1)/1+1=27，即27*27*256，参数数量为(3*3*384+1)*256=884992\n",
    "# 卷积层参数数量为11712+614656+885120+1327488+884992=3712968\n",
    "# 卷积层能源消耗值为2.32957448e+01J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d97a07",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "[**我们构造一个**]高度和宽度都为224的(**单通道数据，来观察每一层输出的形状**)。\n",
    "它与 :numref:`fig_alexnet`中的AlexNet架构相匹配。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a7ec36",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 6400])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c79a7",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## 读取数据集\n",
    "\n",
    "尽管原文中AlexNet是在ImageNet上进行训练的，但本书在这里使用的是Fashion-MNIST数据集。因为即使在现代GPU上，训练ImageNet模型，同时使其收敛可能需要数小时或数天的时间。\n",
    "将AlexNet直接应用于Fashion-MNIST的一个问题是，[**Fashion-MNIST图像的分辨率**]（$28 \\times 28$像素）(**低于ImageNet图像。**)\n",
    "为了解决这个问题，(**我们将它们增加到$224 \\times 224$**)（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用AlexNet架构）。\n",
    "这里需要使用`d2l.load_data_fashion_mnist`函数中的`resize`参数执行此调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1552a8",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the train_iter is: (118,)\n",
      "the shape of the 0 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 1 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 2 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 3 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 4 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 5 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 6 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 7 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 8 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n",
      "the shape of the 9 batch of the train_iter is: torch.Size([512, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "# print the shape of the train_iter\n",
    "list_of_i = []\n",
    "for i, (X, y) in enumerate(train_iter):\n",
    "    list_of_i.append(i)\n",
    "\n",
    "print('the shape of the train_iter is:', np.array(list_of_i).shape)\n",
    "# print(list_of_i)\n",
    "# print the first 10 batch of the train_iter\n",
    "for i, (X, y) in enumerate(train_iter):\n",
    "    if i < 10:\n",
    "        print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484d7f3",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## [**训练AlexNet**]\n",
    "\n",
    "现在AlexNet可以开始被训练了。与 :numref:`sec_lenet`中的LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff5843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 定义一个forwar_hook函数，目标是在运行到该层的时候，进行显示\n",
    "# 更新：forward_hook函数的参数是固定的，不能随意更改，增加time模块不太好\n",
    "#     需要改为设定循环来一步步的进行计算\n",
    "# '''\n",
    "\n",
    "# def forward_hook(module, input, output):\n",
    "#     if isinstance(module, nn.Conv2d):\n",
    "#         print(\"正在运行卷积层!\")\n",
    "#     elif isinstance(module, nn.MaxPool2d):\n",
    "#         print(\"正在运行池化层!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843da99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\n",
    "\n",
    "    Defined in :numref:`sec_lenet`\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c78cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(data_iter, net, loss_fn, device):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    net.eval()\n",
    "    size = len(data_iter.dataset)\n",
    "    num_batches = len(data_iter)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            acc = d2l.accuracy(net(X), y)\n",
    "            size = d2l.size(y)\n",
    "    return acc/size\n",
    "    #         pred = net(X)\n",
    "    #         test_loss += loss_fn(pred, y).item()\n",
    "    #         correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d888ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 这段是将d2l中的train_ch6()函数拷贝过来，然后修改了一下，目的是为了能够显示在进行每一层的前向传播的时候，能够显示出来\n",
    "# '''\n",
    "# def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "#     \"\"\"用GPU训练模型(在第六章定义)\n",
    "\n",
    "#     Defined in :numref:`sec_lenet`\"\"\"\n",
    "#     def init_weights(m):\n",
    "#         if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "#     net.apply(init_weights)\n",
    "\n",
    "#     list_layer_name = ['Conv2d','ReLU', 'MaxPool2d','Linear','Dropout','Flatten']\n",
    "\n",
    "#     # create a numpy array to store the time and energy, the shape is (num_epochs, len(list_layer_name), 2)\n",
    "#     # the 2 means time and energy, respectively\n",
    "#     time_energy_data = np.zeros((num_epochs, len(list_layer_name), 2)) # for epoch = 1, the shape is (6,2)\n",
    "\n",
    "#     # create another numpy array to store the time and energy for loss, backward and optimizer\n",
    "#     # the shape is (num_epochs, 3)\n",
    "#     time_energy_data_loss_backward_optimizer = np.zeros((num_epochs, 3))\n",
    "\n",
    "#     print('training on', device)\n",
    "#     net.to(device)\n",
    "#     optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "#                             legend=['train loss', 'train acc', 'test acc'])\n",
    "#     timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # 训练损失之和，训练准确率之和，样本数\n",
    "#         metric = d2l.Accumulator(3)\n",
    "#         net.train() # 设置为训练模式\n",
    "\n",
    "#         # 将time_energy_data的第一个维度设置为epoch\n",
    "#         time_energy_data[epoch,:,:] = epoch\n",
    "\n",
    "#         # 将time_energy_data_backward_optimizer的第一个维度设置为epoch\n",
    "#         time_energy_data_loss_backward_optimizer[epoch,:] = epoch\n",
    "\n",
    "#         for i, (X, y) in enumerate(train_iter):\n",
    "#             timer.start()\n",
    "#             optimizer.zero_grad()\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "\n",
    "#             # 增加一个time_energy_data_i, 用于记录每一次前向传播的时间和能量\n",
    "#             time_energy_data_i = np.zeros((len(list_layer_name),2))\n",
    "\n",
    "#             # 增加一个time_energy_data_backward_optimizer_i, 用于记录每一次backward和optimizer的时间和能量\n",
    "#             time_energy_data_loss_backward_optimizer_i = np.zeros((3))\n",
    "\n",
    "#             # 将原本的y_hat = net(X)改为下面的形式，目的是为了让模型逐层运行，并且在这个过程中，记录起运行的时间以及消耗的能量\n",
    "#             y_hat = X\n",
    "#             for layer in net:\n",
    "                \n",
    "#                 # print(layer.__class__.__name__,'output shape:\\t',y_hat.shape)\n",
    "#                 layer_name = layer.__class__.__name__\n",
    "#                 # print(layer_name)\n",
    "#                 # # check if the layer_name is in the list_layer_name，if yes, then act the code below, else, continue\n",
    "#                 # if layer_name in list_layer_name:\n",
    "#                 # find out the layer name is in where of the list\n",
    "#                 layer_index = list_layer_name.index(layer_name)\n",
    "                \n",
    "#                 # calculate the energy and time\n",
    "#                 # 这里的时间和能量都是在每一层的前向传播的时候，进行计算的\n",
    "#                 time_start_layer = time.time()\n",
    "#                 '''\n",
    "#                 energy部分后续加入，先进行测试时间计算\n",
    "#                 '''\n",
    "#                 # energy_start = 0\n",
    "#                 y_hat = layer(y_hat)\n",
    "#                 time_end_layer = time.time()\n",
    "#                 time_cost_layer = time_end_layer - time_start_layer\n",
    "#                 # 将对应的layer_index的时间和能量加入到time_energy_data中\n",
    "#                 time_energy_data_i[layer_index,0] += time_cost_layer\n",
    "#                 # time_energy_data_i[layer_index,1] += energy_cost_layer\n",
    "\n",
    "#                 # print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "#                 # print(time_energy_data_i)\n",
    "            \n",
    "#             # 将time_energy_data_i加入到time_energy_data中\n",
    "#             time_energy_data[epoch,:,:] += time_energy_data_i\n",
    "\n",
    "#             # y_hat = net(X)\n",
    "\n",
    "#             # loss部分\n",
    "#             time_start_loss = time.time()\n",
    "#             l = loss(y_hat, y)\n",
    "#             time_end_loss = time.time()\n",
    "#             time_cost_loss = time_end_loss - time_start_loss\n",
    "#             # 对time_cost_loss数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[2] += time_cost_loss\n",
    "\n",
    "#             # backward部分\n",
    "#             time_start_backward = time.time()\n",
    "#             l.backward()\n",
    "#             time_end_backward = time.time()\n",
    "#             time_cost_backward = time_end_backward - time_start_backward\n",
    "#             # 对time_cost_backward数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[0] += time_cost_backward\n",
    "\n",
    "#             # optimizer部分\n",
    "#             time_start_optimizer = time.time()\n",
    "#             optimizer.step()\n",
    "#             time_end_optimizer = time.time()\n",
    "#             time_cost_optimizer = time_end_optimizer - time_start_optimizer\n",
    "#             # 对time_cost_optimizer数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[1] += time_cost_optimizer\n",
    "\n",
    "#             # 将time_energy_data_backward_optimizer_i加入到time_energy_data_backward_optimizer中\n",
    "#             time_energy_data_loss_backward_optimizer[epoch,:] += time_energy_data_loss_backward_optimizer_i\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "#             timer.stop()\n",
    "#             train_l = metric[0] / metric[2]\n",
    "#             train_acc = metric[1] / metric[2]\n",
    "#             if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "#                 animator.add(epoch + (i + 1) / num_batches,\n",
    "#                              (train_l, train_acc, None))\n",
    "#         test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "#         animator.add(epoch + 1, (None, None, test_acc))\n",
    "\n",
    "#         # 输出一下每种层的训练时间\n",
    "#         print('epoch %d, time %f sec' % (epoch, timer.sum()))\n",
    "#         for j in range(len(list_layer_name)):\n",
    "#             print(list_layer_name[j], 'time %f sec' % (time_energy_data[epoch,j,0]))\n",
    "\n",
    "#         # 输出一下backward和optimizer的时间\n",
    "#         print('backward time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,0]))\n",
    "#         print('optimizer time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,1]))\n",
    "#         print('loss time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,2]))\n",
    "#     print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "#           f'test acc {test_acc:.3f}')\n",
    "#     print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "#           f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56310da0",
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# lr, num_epochs = 0.01, 1\n",
    "# train_ch6(net, train_iter, test_iter, num_epochs, lr, 'mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee0bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 这段是将d2l中的train_ch6()函数拷贝过来，然后修改了一下，目的是为了能够显示在进行每一层的前向传播的时候，能够显示出来\n",
    "# 修改了其中的metric部分，对该部分进行了注释，而是调用了简单的optimizer.no_grad()\n",
    "# '''\n",
    "# lr, num_epochs = 0.01, 1\n",
    "# def init_weights(m):\n",
    "#     if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "#         nn.init.xavier_uniform_(m.weight)\n",
    "# net.apply(init_weights)\n",
    "\n",
    "# list_layer_name = ['Conv2d','ReLU', 'MaxPool2d','Linear','Dropout','Flatten']\n",
    "\n",
    "# # create a numpy array to store the time and energy, the shape is (num_epochs, len(list_layer_name), 2)\n",
    "# # the 2 means time and energy, respectively\n",
    "# time_energy_data = np.zeros((num_epochs, len(list_layer_name), 2)) # for epoch = 1, the shape is (6,2)\n",
    "\n",
    "# # create another numpy array to store the time and energy for loss, backward and optimizer\n",
    "# # the shape is (num_epochs, 3)\n",
    "# time_energy_data_loss_backward_optimizer = np.zeros((num_epochs, 3))\n",
    "\n",
    "# time_cost_other = 0\n",
    "\n",
    "# time_avg_cost = 0\n",
    "\n",
    "# device = 'mps'\n",
    "# print('training on', 'mps')\n",
    "# net.to('mps')\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "#                         legend=['train loss', 'train acc', 'test acc'])\n",
    "# timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "# for epoch in range(num_epochs):\n",
    "#     timer.start() # 开始计时\n",
    "#     # 训练损失之和，训练准确率之和，样本数\n",
    "#     metric = d2l.Accumulator(3)\n",
    "#     net.train() # 设置为训练模式\n",
    "\n",
    "#     # 将time_energy_data的第一个维度设置为epoch\n",
    "#     time_energy_data[epoch,:,:] = epoch\n",
    "\n",
    "#     # 将time_energy_data_backward_optimizer的第一个维度设置为epoch\n",
    "#     time_energy_data_loss_backward_optimizer[epoch,:] = epoch\n",
    "\n",
    "#     # initialize a time_to_device_cost to store the time cost of the device\n",
    "#     time_to_device_cost = 0\n",
    "\n",
    "\n",
    "#     for i, (X, y) in enumerate(train_iter):\n",
    "\n",
    "#         if i < 500:\n",
    "\n",
    "#             time_round = time.time()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             time_to_device = time.time()\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             time_to_device_end = time.time()\n",
    "#             time_to_device_cost_i = time_to_device_end - time_to_device\n",
    "#             time_to_device_cost += time_to_device_cost_i\n",
    "\n",
    "#             # 增加一个time_energy_data_i, 用于记录每一次前向传播的时间和能量\n",
    "#             time_energy_data_i = np.zeros((len(list_layer_name),2))\n",
    "\n",
    "#             # 增加一个time_energy_data_backward_optimizer_i, 用于记录每一次backward和optimizer的时间和能量\n",
    "#             time_energy_data_loss_backward_optimizer_i = np.zeros((3))\n",
    "\n",
    "#             time_cost_other_i = 0\n",
    "\n",
    "#             time_avg_cost_i = 0\n",
    "\n",
    "#             # 将原本的y_hat = net(X)改为下面的形式，目的是为了让模型逐层运行，并且在这个过程中，记录起运行的时间以及消耗的能量\n",
    "#             y_hat = X\n",
    "#             for layer in net:\n",
    "                \n",
    "#                 # print(layer.__class__.__name__,'output shape:\\t',y_hat.shape)\n",
    "#                 layer_name = layer.__class__.__name__\n",
    "#                 # print(layer_name)\n",
    "#                 # # check if the layer_name is in the list_layer_name，if yes, then act the code below, else, continue\n",
    "#                 # if layer_name in list_layer_name:\n",
    "#                 # find out the layer name is in where of the list\n",
    "#                 layer_index = list_layer_name.index(layer_name)\n",
    "                \n",
    "#                 # calculate the energy and time\n",
    "#                 # 这里的时间和能量都是在每一层的前向传播的时候，进行计算的\n",
    "#                 time_start_layer = time.time()\n",
    "#                 '''\n",
    "#                 energy部分后续加入，先进行测试时间计算\n",
    "#                 '''\n",
    "#                 # energy_start = 0\n",
    "#                 y_hat = layer(y_hat)\n",
    "#                 time_end_layer = time.time()\n",
    "#                 time_cost_layer = time_end_layer - time_start_layer\n",
    "\n",
    "#                 # print the result\n",
    "#                 print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "#                 print('*'*50)\n",
    "\n",
    "\n",
    "#                 # 将对应的layer_index的时间和能量加入到time_energy_data中\n",
    "#                 time_energy_data_i[layer_index,0] += time_cost_layer\n",
    "#                 # time_energy_data_i[layer_index,1] += energy_cost_layer\n",
    "\n",
    "#                 # print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "#                 # print(time_energy_data_i)\n",
    "            \n",
    "#             # 将time_energy_data_i加入到time_energy_data中\n",
    "#             time_energy_data[epoch,:,:] += time_energy_data_i\n",
    "\n",
    "#             # y_hat = net(X)\n",
    "\n",
    "#             # loss部分\n",
    "#             time_start_loss = time.time()\n",
    "#             l = loss(y_hat, y)\n",
    "#             time_end_loss = time.time()\n",
    "#             time_cost_loss = time_end_loss - time_start_loss\n",
    "#             # 对time_cost_loss数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[0] += time_cost_loss\n",
    "\n",
    "#             # backward部分\n",
    "#             time_start_backward = time.time()\n",
    "#             l.backward()\n",
    "#             time_end_backward = time.time()\n",
    "#             time_cost_backward = time_end_backward - time_start_backward\n",
    "#             # 对time_cost_backward数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[1] += time_cost_backward\n",
    "\n",
    "#             # optimizer部分\n",
    "#             time_start_optimizer = time.time()\n",
    "#             optimizer.step()\n",
    "#             time_end_optimizer = time.time()\n",
    "#             time_cost_optimizer = time_end_optimizer - time_start_optimizer\n",
    "#             # 对time_cost_optimizer数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[2] += time_cost_optimizer\n",
    "\n",
    "#             # 将time_energy_data_backward_optimizer_i加入到time_energy_data_backward_optimizer中\n",
    "#             time_energy_data_loss_backward_optimizer[epoch,:] += time_energy_data_loss_backward_optimizer_i\n",
    "            \n",
    "\n",
    "#             '''\n",
    "#             该部分是optimizer.zero_grad()部分，并且计算相对应的时间\n",
    "#             不计算梯度：torch.no_grad()上下文管理器确保不计算任何梯度。这实际上减少了计算和内存需求，因为它不会保留计算图中的中间状态。从这个角度看，使用torch.no_grad()会节省能源。\n",
    "#             理论上应该节省能源，但是在计算过程中，该部分消耗的时间是最长的\n",
    "#             为了进行测试，将李沐原本的torch.no_grad()进行替换，更换为了optimizer.zero_grad()\n",
    "#             '''\n",
    "#             # time about torch.no_grad()\n",
    "#             time_other_start = time.time()\n",
    "#             optimizer.zero_grad()\n",
    "#             # with torch.no_grad():\n",
    "#             #     metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "#             time_other_end = time.time()\n",
    "\n",
    "\n",
    "#             '''\n",
    "#             该部分是计算train_l和train_acc的部分，以及其运行过程所需时间的代码\n",
    "#             '''\n",
    "#             # time about avg the train_l and train_acc\n",
    "#             # time_avg_start = time.time()\n",
    "#             # train_l = metric[0] / metric[2]\n",
    "#             # train_acc = metric[1] / metric[2]\n",
    "#             # time_avg_end = time.time()\n",
    "#             # time_avg_cost_i = time_avg_end - time_avg_start\n",
    "#             # time_avg_cost += time_avg_cost_i\n",
    "\n",
    "#             # if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "#             #     animator.add(epoch + (i + 1) / num_batches,\n",
    "#             #                     (train_l, train_acc, None))\n",
    "#             # time_other_end = time.time()\n",
    "#             time_cost_other_i = time_other_end - time_other_start\n",
    "#             print('other time %f sec' % (time_cost_other_i))\n",
    "#             time_cost_other += time_cost_other_i\n",
    "\n",
    "#             time_end_round = time.time()\n",
    "#             time_cost_round = time_end_round - time_round\n",
    "#             print('round %d, time %f sec' % (i, time_cost_round))\n",
    "        \n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "#     time_test_acc_start = time.time()\n",
    "#     test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "#     time_test_acc_end = time.time()\n",
    "#     time_test_acc_cost = time_test_acc_end - time_test_acc_start\n",
    "#     # animator.add(epoch + 1, (None, None, test_acc))\n",
    "#     timer.stop() # 停止计时 \n",
    "    \n",
    "#     print('epoch %d, time %f sec' % (epoch, timer.sum()))\n",
    "#     # 输出一下每种层的训练时间\n",
    "#     for j in range(len(list_layer_name)):\n",
    "#         print(list_layer_name[j], 'time %f sec' % (time_energy_data[epoch,j,0]))\n",
    "    \n",
    "#     # 输出一下device的时间\n",
    "#     print('device time %f sec' % (time_to_device_cost))\n",
    "\n",
    "#     # 输出一下backward和optimizer的时间\n",
    "#     print('backward time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,1]))\n",
    "#     print('optimizer time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,2]))\n",
    "#     print('loss time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,0]))\n",
    "# print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "#         f'test acc {test_acc:.3f}')\n",
    "# print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "#         f'on {str(device)}')\n",
    "\n",
    "# print(time_energy_data)\n",
    "# print(time_energy_data_loss_backward_optimizer)\n",
    "# print('other time %f sec' % (time_cost_other))\n",
    "# print('time to device %f sec' % (time_to_device_cost))\n",
    "# # print('time avg %f sec' % (time_avg_cost))\n",
    "# print('time test acc %f sec' % (time_test_acc_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e649e",
   "metadata": {},
   "source": [
    "## 修改代码——Test1\n",
    "\n",
    "该段代码作为这个jupyter文件的最终运行部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516c82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_powermetrics(file_path):\n",
    "    \"\"\"\n",
    "    Run powermetrics and retrieve the output.\n",
    "\n",
    "    :param interval: Sampling interval in milliseconds.\n",
    "    :param count: Number of samples to retrieve.s\n",
    "    :return: The output from powermetrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the command as a list of arguments\n",
    "    cmd = [\"sudo\", \"powermetrics\",  \"-i\", \"1000\", \"--samplers\", \"cpu_power,gpu_power\", \"-a\", \"1\", \"-o\", file_path]\n",
    "    \n",
    "    process = subprocess.Popen(cmd)\n",
    "    return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89638b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_data_process(file_path):\n",
    "    \"\"\"\n",
    "    Read the output file of powermetric and extract the power value\n",
    "\n",
    "    :param file_path: The path of the output file of powermetric.\n",
    "    :return: The list of power values.\n",
    "    \"\"\"\n",
    "\n",
    "    list_power = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Combined Power' in line:\n",
    "                power_value = line.split(':')[1].strip()\n",
    "                print(power_value)\n",
    "\n",
    "                # Remove the unit\n",
    "                power_value = power_value.replace('mW', '')\n",
    "\n",
    "                # Convert to integer\n",
    "                power_value = int(power_value)\n",
    "                list_power.append(power_value)\n",
    "\n",
    "    print(list_power)\n",
    "    print(len(list_power))\n",
    "\n",
    "    # do the data process\n",
    "    '''\n",
    "    The data from list_power is the Conbined Power of each second.\n",
    "    The data is the Power of each second.\n",
    "    we need to calculate the energy consumption of the whole process.\n",
    "    and need to change the J to kWh.\n",
    "    '''\n",
    "    # calculate the energy consumption\n",
    "    energy_consumption = 0\n",
    "    for i in range(len(list_power)):\n",
    "       energy_consumption += list_power[i]\n",
    "    print(energy_consumption)\n",
    "\n",
    "    # change the mW to W\n",
    "    energy_consumption = energy_consumption / 1000\n",
    "\n",
    "    # calculate the energy consumption, the interval is 1 second, and the energy unit is J\n",
    "    energy_consumption = energy_consumption * 1\n",
    "    \n",
    "    # change the J to kWh\n",
    "    energy_consumption = energy_consumption / 3600000\n",
    "    print(energy_consumption)\n",
    "    \n",
    "    return energy_consumption, list_power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41bfa77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_file = 'energy.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad309dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on mps\n",
      "round 0\n",
      "time to device 0.220386 sec\n",
      "sum_time_cost_round_layer 1.784926 sec\n",
      "loss time 0.488891 sec\n",
      "backward time 1.938232 sec\n",
      "optimizer time 0.775720 sec\n",
      "round 0, time 5.212937 sec\n",
      "the calculation result for the round is:  5.2081544399261475\n",
      "round 1\n",
      "time to device 0.271692 sec\n",
      "sum_time_cost_round_layer 0.034106 sec\n",
      "loss time 0.018379 sec\n",
      "backward time 0.018756 sec\n",
      "optimizer time 0.002130 sec\n",
      "round 1, time 0.354382 sec\n",
      "the calculation result for the round is:  0.3450632095336914\n",
      "round 2\n",
      "time to device 2.547095 sec\n",
      "sum_time_cost_round_layer 0.030902 sec\n",
      "loss time 0.001932 sec\n",
      "backward time 0.007567 sec\n",
      "optimizer time 0.002954 sec\n",
      "round 2, time 2.591757 sec\n",
      "the calculation result for the round is:  2.5904505252838135\n",
      "round 3\n",
      "time to device 2.641136 sec\n",
      "sum_time_cost_round_layer 0.015797 sec\n",
      "loss time 0.001324 sec\n",
      "backward time 0.008775 sec\n",
      "optimizer time 0.008102 sec\n",
      "round 3, time 2.684207 sec\n",
      "the calculation result for the round is:  2.675133228302002\n",
      "round 4\n",
      "time to device 2.588789 sec\n",
      "sum_time_cost_round_layer 0.031261 sec\n",
      "loss time 0.002390 sec\n",
      "backward time 0.011109 sec\n",
      "optimizer time 0.004000 sec\n",
      "round 4, time 2.639620 sec\n",
      "the calculation result for the round is:  2.6375491619110107\n",
      "round 5\n",
      "time to device 2.445973 sec\n",
      "sum_time_cost_round_layer 0.052459 sec\n",
      "loss time 0.002307 sec\n",
      "backward time 0.014557 sec\n",
      "optimizer time 0.003904 sec\n",
      "round 5, time 2.521059 sec\n",
      "the calculation result for the round is:  2.5192010402679443\n",
      "round 6\n",
      "time to device 2.444975 sec\n",
      "sum_time_cost_round_layer 0.027264 sec\n",
      "loss time 0.001548 sec\n",
      "backward time 0.009641 sec\n",
      "optimizer time 0.003791 sec\n",
      "round 6, time 2.488302 sec\n",
      "the calculation result for the round is:  2.4872195720672607\n",
      "round 7\n",
      "time to device 2.472079 sec\n",
      "sum_time_cost_round_layer 0.034759 sec\n",
      "loss time 0.003817 sec\n",
      "backward time 0.045843 sec\n",
      "optimizer time 0.013157 sec\n",
      "round 7, time 2.571649 sec\n",
      "the calculation result for the round is:  2.569654941558838\n",
      "round 8\n",
      "time to device 2.534169 sec\n",
      "sum_time_cost_round_layer 0.068117 sec\n",
      "loss time 0.004516 sec\n",
      "backward time 0.019647 sec\n",
      "optimizer time 0.005410 sec\n",
      "round 8, time 2.639133 sec\n",
      "the calculation result for the round is:  2.6318585872650146\n",
      "round 9\n",
      "time to device 2.469761 sec\n",
      "sum_time_cost_round_layer 0.022887 sec\n",
      "loss time 0.002647 sec\n",
      "backward time 0.072791 sec\n",
      "optimizer time 0.024074 sec\n",
      "round 9, time 2.596945 sec\n",
      "the calculation result for the round is:  2.5921599864959717\n",
      "round 10\n",
      "time to device 2.484651 sec\n",
      "sum_time_cost_round_layer 0.015515 sec\n",
      "loss time 0.001501 sec\n",
      "backward time 0.012036 sec\n",
      "optimizer time 0.017639 sec\n",
      "round 10, time 2.543441 sec\n",
      "the calculation result for the round is:  2.5313422679901123\n",
      "round 11\n",
      "time to device 2.484354 sec\n",
      "sum_time_cost_round_layer 0.036835 sec\n",
      "loss time 0.009493 sec\n",
      "backward time 0.017222 sec\n",
      "optimizer time 0.005997 sec\n",
      "round 11, time 2.559808 sec\n",
      "the calculation result for the round is:  2.553901195526123\n",
      "round 12\n",
      "time to device 2.424529 sec\n",
      "sum_time_cost_round_layer 0.032288 sec\n",
      "loss time 0.001463 sec\n",
      "backward time 0.014494 sec\n",
      "optimizer time 0.001735 sec\n",
      "round 12, time 2.475431 sec\n",
      "the calculation result for the round is:  2.4745090007781982\n",
      "round 13\n",
      "time to device 2.477140 sec\n",
      "sum_time_cost_round_layer 0.025566 sec\n",
      "loss time 0.001240 sec\n",
      "backward time 0.041942 sec\n",
      "optimizer time 0.008787 sec\n",
      "round 13, time 2.556115 sec\n",
      "the calculation result for the round is:  2.5546751022338867\n",
      "round 14\n",
      "time to device 2.687041 sec\n",
      "sum_time_cost_round_layer 0.010181 sec\n",
      "loss time 0.001420 sec\n",
      "backward time 0.010611 sec\n",
      "optimizer time 0.006711 sec\n",
      "round 14, time 2.725064 sec\n",
      "the calculation result for the round is:  2.715963840484619\n",
      "round 15\n",
      "time to device 2.487955 sec\n",
      "sum_time_cost_round_layer 0.013166 sec\n",
      "loss time 0.001665 sec\n",
      "backward time 0.028667 sec\n",
      "optimizer time 0.004287 sec\n",
      "round 15, time 2.536765 sec\n",
      "the calculation result for the round is:  2.535740613937378\n",
      "round 16\n",
      "time to device 2.630252 sec\n",
      "sum_time_cost_round_layer 0.019596 sec\n",
      "loss time 0.015871 sec\n",
      "backward time 0.027239 sec\n",
      "optimizer time 0.011881 sec\n",
      "round 16, time 2.714143 sec\n",
      "the calculation result for the round is:  2.7048392295837402\n",
      "round 17\n",
      "time to device 2.533885 sec\n",
      "sum_time_cost_round_layer 0.037832 sec\n",
      "loss time 0.005007 sec\n",
      "backward time 0.024093 sec\n",
      "optimizer time 0.006964 sec\n",
      "round 17, time 2.626589 sec\n",
      "the calculation result for the round is:  2.607781171798706\n",
      "round 18\n",
      "time to device 2.652218 sec\n",
      "sum_time_cost_round_layer 0.022804 sec\n",
      "loss time 0.002626 sec\n",
      "backward time 0.014927 sec\n",
      "optimizer time 0.003895 sec\n",
      "round 18, time 2.744288 sec\n",
      "the calculation result for the round is:  2.69646954536438\n",
      "round 19\n",
      "time to device 2.617710 sec\n",
      "sum_time_cost_round_layer 0.019460 sec\n",
      "loss time 0.001204 sec\n",
      "backward time 0.040605 sec\n",
      "optimizer time 0.011908 sec\n",
      "round 19, time 2.698164 sec\n",
      "the calculation result for the round is:  2.690886974334717\n",
      "round 20\n",
      "time to device 2.514049 sec\n",
      "sum_time_cost_round_layer 0.010104 sec\n",
      "loss time 0.001380 sec\n",
      "backward time 0.014682 sec\n",
      "optimizer time 0.002137 sec\n",
      "round 20, time 2.550970 sec\n",
      "the calculation result for the round is:  2.5423526763916016\n",
      "round 21\n",
      "time to device 2.870370 sec\n",
      "sum_time_cost_round_layer 0.065765 sec\n",
      "loss time 0.006361 sec\n",
      "backward time 0.022603 sec\n",
      "optimizer time 0.004553 sec\n",
      "round 21, time 2.973976 sec\n",
      "the calculation result for the round is:  2.9696528911590576\n",
      "round 22\n",
      "time to device 2.485746 sec\n",
      "sum_time_cost_round_layer 0.003291 sec\n",
      "loss time 0.000556 sec\n",
      "backward time 0.005426 sec\n",
      "optimizer time 0.001436 sec\n",
      "round 22, time 2.496909 sec\n",
      "the calculation result for the round is:  2.496454954147339\n",
      "round 23\n",
      "time to device 2.784900 sec\n",
      "sum_time_cost_round_layer 0.029089 sec\n",
      "loss time 0.002270 sec\n",
      "backward time 0.012818 sec\n",
      "optimizer time 0.008544 sec\n",
      "round 23, time 2.844917 sec\n",
      "the calculation result for the round is:  2.837620735168457\n",
      "round 24\n",
      "time to device 2.645719 sec\n",
      "sum_time_cost_round_layer 0.030281 sec\n",
      "loss time 0.006964 sec\n",
      "backward time 0.022554 sec\n",
      "optimizer time 0.012597 sec\n",
      "round 24, time 2.730756 sec\n",
      "the calculation result for the round is:  2.7181148529052734\n",
      "round 25\n",
      "time to device 2.590694 sec\n",
      "sum_time_cost_round_layer 0.016604 sec\n",
      "loss time 0.000619 sec\n",
      "backward time 0.004554 sec\n",
      "optimizer time 0.001930 sec\n",
      "round 25, time 2.615401 sec\n",
      "the calculation result for the round is:  2.614401340484619\n",
      "round 26\n",
      "time to device 2.601859 sec\n",
      "sum_time_cost_round_layer 0.020524 sec\n",
      "loss time 0.000881 sec\n",
      "backward time 0.005562 sec\n",
      "optimizer time 0.002025 sec\n",
      "round 26, time 2.631955 sec\n",
      "the calculation result for the round is:  2.6308507919311523\n",
      "round 27\n",
      "time to device 2.551030 sec\n",
      "sum_time_cost_round_layer 0.015893 sec\n",
      "loss time 0.001429 sec\n",
      "backward time 0.049231 sec\n",
      "optimizer time 0.012954 sec\n",
      "round 27, time 2.639220 sec\n",
      "the calculation result for the round is:  2.6305365562438965\n",
      "round 28\n",
      "time to device 2.454404 sec\n",
      "sum_time_cost_round_layer 0.009234 sec\n",
      "loss time 0.001476 sec\n",
      "backward time 0.015018 sec\n",
      "optimizer time 0.001765 sec\n",
      "round 28, time 2.483120 sec\n",
      "the calculation result for the round is:  2.4818975925445557\n",
      "round 29\n",
      "time to device 2.471706 sec\n",
      "sum_time_cost_round_layer 0.016727 sec\n",
      "loss time 0.003032 sec\n",
      "backward time 0.030440 sec\n",
      "optimizer time 0.026165 sec\n",
      "round 29, time 2.551021 sec\n",
      "the calculation result for the round is:  2.548069953918457\n",
      "round 30\n",
      "time to device 2.479181 sec\n",
      "sum_time_cost_round_layer 0.024096 sec\n",
      "loss time 0.002141 sec\n",
      "backward time 0.012159 sec\n",
      "optimizer time 0.013016 sec\n",
      "round 30, time 2.542703 sec\n",
      "the calculation result for the round is:  2.5305933952331543\n",
      "round 31\n",
      "time to device 2.500499 sec\n",
      "sum_time_cost_round_layer 0.021388 sec\n",
      "loss time 0.001631 sec\n",
      "backward time 0.010125 sec\n",
      "optimizer time 0.004157 sec\n",
      "round 31, time 2.555092 sec\n",
      "the calculation result for the round is:  2.5378003120422363\n",
      "round 32\n",
      "time to device 2.473450 sec\n",
      "sum_time_cost_round_layer 0.039145 sec\n",
      "loss time 0.001713 sec\n",
      "backward time 0.015232 sec\n",
      "optimizer time 0.005139 sec\n",
      "round 32, time 2.536603 sec\n",
      "the calculation result for the round is:  2.5346791744232178\n",
      "round 33\n",
      "time to device 2.453759 sec\n",
      "sum_time_cost_round_layer 0.034171 sec\n",
      "loss time 0.001469 sec\n",
      "backward time 0.014528 sec\n",
      "optimizer time 0.005070 sec\n",
      "round 33, time 2.510705 sec\n",
      "the calculation result for the round is:  2.5089972019195557\n",
      "round 34\n",
      "time to device 2.465385 sec\n",
      "sum_time_cost_round_layer 0.011720 sec\n",
      "loss time 0.002174 sec\n",
      "backward time 0.012275 sec\n",
      "optimizer time 0.004043 sec\n",
      "round 34, time 2.497775 sec\n",
      "the calculation result for the round is:  2.4955973625183105\n",
      "round 35\n",
      "time to device 2.569327 sec\n",
      "sum_time_cost_round_layer 0.065684 sec\n",
      "loss time 0.003358 sec\n",
      "backward time 0.013392 sec\n",
      "optimizer time 0.004889 sec\n",
      "round 35, time 2.697454 sec\n",
      "the calculation result for the round is:  2.6566500663757324\n",
      "round 36\n",
      "time to device 2.427459 sec\n",
      "sum_time_cost_round_layer 0.013793 sec\n",
      "loss time 0.001337 sec\n",
      "backward time 0.023621 sec\n",
      "optimizer time 0.003352 sec\n",
      "round 36, time 2.470716 sec\n",
      "the calculation result for the round is:  2.4695608615875244\n",
      "round 37\n",
      "time to device 2.440418 sec\n",
      "sum_time_cost_round_layer 0.014115 sec\n",
      "loss time 0.001526 sec\n",
      "backward time 0.011632 sec\n",
      "optimizer time 0.003523 sec\n",
      "round 37, time 2.474420 sec\n",
      "the calculation result for the round is:  2.4712138175964355\n",
      "round 38\n",
      "time to device 2.429057 sec\n",
      "sum_time_cost_round_layer 0.015662 sec\n",
      "loss time 0.002136 sec\n",
      "backward time 0.020132 sec\n",
      "optimizer time 0.004415 sec\n",
      "round 38, time 2.472917 sec\n",
      "the calculation result for the round is:  2.471402645111084\n",
      "round 39\n",
      "time to device 2.409017 sec\n",
      "sum_time_cost_round_layer 0.045786 sec\n",
      "loss time 0.001846 sec\n",
      "backward time 0.015186 sec\n",
      "optimizer time 0.004144 sec\n",
      "round 39, time 2.477283 sec\n",
      "the calculation result for the round is:  2.4759795665740967\n",
      "round 40\n",
      "time to device 2.441712 sec\n",
      "sum_time_cost_round_layer 0.014717 sec\n",
      "loss time 0.001809 sec\n",
      "backward time 0.019921 sec\n",
      "optimizer time 0.003393 sec\n",
      "round 40, time 2.482605 sec\n",
      "the calculation result for the round is:  2.481553077697754\n",
      "round 41\n",
      "time to device 2.484077 sec\n",
      "sum_time_cost_round_layer 0.029368 sec\n",
      "loss time 0.002769 sec\n",
      "backward time 0.036781 sec\n",
      "optimizer time 0.010388 sec\n",
      "round 41, time 2.567424 sec\n",
      "the calculation result for the round is:  2.563382863998413\n",
      "round 42\n",
      "time to device 2.418997 sec\n",
      "sum_time_cost_round_layer 0.035929 sec\n",
      "loss time 0.002179 sec\n",
      "backward time 0.015180 sec\n",
      "optimizer time 0.003478 sec\n",
      "round 42, time 2.477260 sec\n",
      "the calculation result for the round is:  2.4757628440856934\n",
      "round 43\n",
      "time to device 2.451679 sec\n",
      "sum_time_cost_round_layer 0.050156 sec\n",
      "loss time 0.001564 sec\n",
      "backward time 0.017654 sec\n",
      "optimizer time 0.006166 sec\n",
      "round 43, time 2.528435 sec\n",
      "the calculation result for the round is:  2.5272183418273926\n",
      "round 44\n",
      "time to device 2.415261 sec\n",
      "sum_time_cost_round_layer 0.046579 sec\n",
      "loss time 0.002768 sec\n",
      "backward time 0.012831 sec\n",
      "optimizer time 0.003650 sec\n",
      "round 44, time 2.483410 sec\n",
      "the calculation result for the round is:  2.481088638305664\n",
      "round 45\n",
      "time to device 2.420014 sec\n",
      "sum_time_cost_round_layer 0.051933 sec\n",
      "loss time 0.001808 sec\n",
      "backward time 0.014723 sec\n",
      "optimizer time 0.003707 sec\n",
      "round 45, time 2.493361 sec\n",
      "the calculation result for the round is:  2.492185115814209\n",
      "round 46\n",
      "time to device 2.436357 sec\n",
      "sum_time_cost_round_layer 0.029585 sec\n",
      "loss time 0.001227 sec\n",
      "backward time 0.052328 sec\n",
      "optimizer time 0.014769 sec\n",
      "round 46, time 2.537008 sec\n",
      "the calculation result for the round is:  2.5342652797698975\n",
      "round 47\n",
      "time to device 2.473891 sec\n",
      "sum_time_cost_round_layer 0.015530 sec\n",
      "loss time 0.002658 sec\n",
      "backward time 0.027583 sec\n",
      "optimizer time 0.021193 sec\n",
      "round 47, time 2.548786 sec\n",
      "the calculation result for the round is:  2.5408554077148438\n",
      "round 48\n",
      "time to device 2.431114 sec\n",
      "sum_time_cost_round_layer 0.058908 sec\n",
      "loss time 0.001856 sec\n",
      "backward time 0.011170 sec\n",
      "optimizer time 0.003175 sec\n",
      "round 48, time 2.507959 sec\n",
      "the calculation result for the round is:  2.5062224864959717\n",
      "round 49\n",
      "time to device 2.451901 sec\n",
      "sum_time_cost_round_layer 0.027089 sec\n",
      "loss time 0.000590 sec\n",
      "backward time 0.011604 sec\n",
      "optimizer time 0.024304 sec\n",
      "round 49, time 2.517850 sec\n",
      "the calculation result for the round is:  2.515488386154175\n",
      "round 50\n",
      "time to device 2.455423 sec\n",
      "sum_time_cost_round_layer 0.014049 sec\n",
      "loss time 0.001342 sec\n",
      "backward time 0.012631 sec\n",
      "optimizer time 0.004083 sec\n",
      "round 50, time 2.488790 sec\n",
      "the calculation result for the round is:  2.4875288009643555\n",
      "round 51\n",
      "time to device 2.451711 sec\n",
      "sum_time_cost_round_layer 0.039490 sec\n",
      "loss time 0.002243 sec\n",
      "backward time 0.012991 sec\n",
      "optimizer time 0.003334 sec\n",
      "round 51, time 2.512173 sec\n",
      "the calculation result for the round is:  2.509768486022949\n",
      "round 52\n",
      "time to device 2.418196 sec\n",
      "sum_time_cost_round_layer 0.040184 sec\n",
      "loss time 0.001728 sec\n",
      "backward time 0.016811 sec\n",
      "optimizer time 0.004193 sec\n",
      "round 52, time 2.483047 sec\n",
      "the calculation result for the round is:  2.481112241744995\n",
      "round 53\n",
      "time to device 2.424133 sec\n",
      "sum_time_cost_round_layer 0.012275 sec\n",
      "loss time 0.001437 sec\n",
      "backward time 0.011933 sec\n",
      "optimizer time 0.003675 sec\n",
      "round 53, time 2.454441 sec\n",
      "the calculation result for the round is:  2.4534525871276855\n",
      "round 54\n",
      "time to device 2.431001 sec\n",
      "sum_time_cost_round_layer 0.041730 sec\n",
      "loss time 0.002199 sec\n",
      "backward time 0.014647 sec\n",
      "optimizer time 0.004289 sec\n",
      "round 54, time 2.495826 sec\n",
      "the calculation result for the round is:  2.4938652515411377\n",
      "round 55\n",
      "time to device 2.483526 sec\n",
      "sum_time_cost_round_layer 0.015356 sec\n",
      "loss time 0.001300 sec\n",
      "backward time 0.018614 sec\n",
      "optimizer time 0.003955 sec\n",
      "round 55, time 2.523693 sec\n",
      "the calculation result for the round is:  2.5227510929107666\n",
      "round 56\n",
      "time to device 2.488249 sec\n",
      "sum_time_cost_round_layer 0.036485 sec\n",
      "loss time 0.001598 sec\n",
      "backward time 0.012702 sec\n",
      "optimizer time 0.003898 sec\n",
      "round 56, time 2.544258 sec\n",
      "the calculation result for the round is:  2.5429325103759766\n",
      "round 57\n",
      "time to device 2.455911 sec\n",
      "sum_time_cost_round_layer 0.016622 sec\n",
      "loss time 0.003476 sec\n",
      "backward time 0.016325 sec\n",
      "optimizer time 0.005895 sec\n",
      "round 57, time 2.500007 sec\n",
      "the calculation result for the round is:  2.4982285499572754\n",
      "round 58\n",
      "time to device 2.428396 sec\n",
      "sum_time_cost_round_layer 0.044357 sec\n",
      "loss time 0.001347 sec\n",
      "backward time 0.011302 sec\n",
      "optimizer time 0.003684 sec\n",
      "round 58, time 2.491531 sec\n",
      "the calculation result for the round is:  2.489086151123047\n",
      "round 59\n",
      "time to device 2.429991 sec\n",
      "sum_time_cost_round_layer 0.049224 sec\n",
      "loss time 0.002265 sec\n",
      "backward time 0.014534 sec\n",
      "optimizer time 0.004480 sec\n",
      "round 59, time 2.501629 sec\n",
      "the calculation result for the round is:  2.5004937648773193\n",
      "round 60\n",
      "time to device 2.431879 sec\n",
      "sum_time_cost_round_layer 0.028640 sec\n",
      "loss time 0.001254 sec\n",
      "backward time 0.012752 sec\n",
      "optimizer time 0.005880 sec\n",
      "round 60, time 2.481329 sec\n",
      "the calculation result for the round is:  2.4804046154022217\n",
      "round 61\n",
      "time to device 2.416001 sec\n",
      "sum_time_cost_round_layer 0.010571 sec\n",
      "loss time 0.001051 sec\n",
      "backward time 0.009866 sec\n",
      "optimizer time 0.003444 sec\n",
      "round 61, time 2.441717 sec\n",
      "the calculation result for the round is:  2.440932512283325\n",
      "round 62\n",
      "time to device 2.400034 sec\n",
      "sum_time_cost_round_layer 0.013648 sec\n",
      "loss time 0.001604 sec\n",
      "backward time 0.011273 sec\n",
      "optimizer time 0.003772 sec\n",
      "round 62, time 2.432568 sec\n",
      "the calculation result for the round is:  2.430330991744995\n",
      "round 63\n",
      "time to device 2.433617 sec\n",
      "sum_time_cost_round_layer 0.043826 sec\n",
      "loss time 0.001940 sec\n",
      "backward time 0.014251 sec\n",
      "optimizer time 0.004612 sec\n",
      "round 63, time 2.500024 sec\n",
      "the calculation result for the round is:  2.498246192932129\n",
      "round 64\n",
      "time to device 2.431526 sec\n",
      "sum_time_cost_round_layer 0.049128 sec\n",
      "loss time 0.001790 sec\n",
      "backward time 0.012695 sec\n",
      "optimizer time 0.003974 sec\n",
      "round 64, time 2.499994 sec\n",
      "the calculation result for the round is:  2.499112844467163\n",
      "round 65\n",
      "time to device 2.469402 sec\n",
      "sum_time_cost_round_layer 0.042202 sec\n",
      "loss time 0.001338 sec\n",
      "backward time 0.013408 sec\n",
      "optimizer time 0.003918 sec\n",
      "round 65, time 2.531242 sec\n",
      "the calculation result for the round is:  2.530268430709839\n",
      "round 66\n",
      "time to device 2.440338 sec\n",
      "sum_time_cost_round_layer 0.035242 sec\n",
      "loss time 0.001856 sec\n",
      "backward time 0.012961 sec\n",
      "optimizer time 0.004567 sec\n",
      "round 66, time 2.496482 sec\n",
      "the calculation result for the round is:  2.4949638843536377\n",
      "round 67\n",
      "time to device 2.427256 sec\n",
      "sum_time_cost_round_layer 0.052199 sec\n",
      "loss time 0.001851 sec\n",
      "backward time 0.014387 sec\n",
      "optimizer time 0.005393 sec\n",
      "round 67, time 2.501981 sec\n",
      "the calculation result for the round is:  2.5010857582092285\n",
      "round 68\n",
      "time to device 2.423068 sec\n",
      "sum_time_cost_round_layer 0.053222 sec\n",
      "loss time 0.002368 sec\n",
      "backward time 0.012918 sec\n",
      "optimizer time 0.003835 sec\n",
      "round 68, time 2.496951 sec\n",
      "the calculation result for the round is:  2.4954116344451904\n",
      "round 69\n",
      "time to device 2.422070 sec\n",
      "sum_time_cost_round_layer 0.051152 sec\n",
      "loss time 0.001456 sec\n",
      "backward time 0.012611 sec\n",
      "optimizer time 0.003929 sec\n",
      "round 69, time 2.491927 sec\n",
      "the calculation result for the round is:  2.491217851638794\n",
      "round 70\n",
      "time to device 2.474524 sec\n",
      "sum_time_cost_round_layer 0.027438 sec\n",
      "loss time 0.001568 sec\n",
      "backward time 0.019370 sec\n",
      "optimizer time 0.002871 sec\n",
      "round 70, time 2.526631 sec\n",
      "the calculation result for the round is:  2.525771141052246\n",
      "round 71\n",
      "time to device 2.464192 sec\n",
      "sum_time_cost_round_layer 0.039291 sec\n",
      "loss time 0.001199 sec\n",
      "backward time 0.012192 sec\n",
      "optimizer time 0.003886 sec\n",
      "round 71, time 2.524571 sec\n",
      "the calculation result for the round is:  2.520759344100952\n",
      "round 72\n",
      "time to device 2.461049 sec\n",
      "sum_time_cost_round_layer 0.032828 sec\n",
      "loss time 0.001342 sec\n",
      "backward time 0.014572 sec\n",
      "optimizer time 0.005047 sec\n",
      "round 72, time 2.515969 sec\n",
      "the calculation result for the round is:  2.514838457107544\n",
      "round 73\n",
      "time to device 2.425533 sec\n",
      "sum_time_cost_round_layer 0.015460 sec\n",
      "loss time 0.001410 sec\n",
      "backward time 0.013042 sec\n",
      "optimizer time 0.003511 sec\n",
      "round 73, time 2.460186 sec\n",
      "the calculation result for the round is:  2.458955764770508\n",
      "round 74\n",
      "time to device 2.414475 sec\n",
      "sum_time_cost_round_layer 0.012781 sec\n",
      "loss time 0.001442 sec\n",
      "backward time 0.011453 sec\n",
      "optimizer time 0.004026 sec\n",
      "round 74, time 2.444912 sec\n",
      "the calculation result for the round is:  2.4441769123077393\n",
      "round 75\n",
      "time to device 2.388345 sec\n",
      "sum_time_cost_round_layer 0.013688 sec\n",
      "loss time 0.001102 sec\n",
      "backward time 0.014105 sec\n",
      "optimizer time 0.003069 sec\n",
      "round 75, time 2.422161 sec\n",
      "the calculation result for the round is:  2.4203085899353027\n",
      "round 76\n",
      "time to device 2.422152 sec\n",
      "sum_time_cost_round_layer 0.014727 sec\n",
      "loss time 0.001521 sec\n",
      "backward time 0.013207 sec\n",
      "optimizer time 0.003183 sec\n",
      "round 76, time 2.455915 sec\n",
      "the calculation result for the round is:  2.4547903537750244\n",
      "round 77\n",
      "time to device 2.524473 sec\n",
      "sum_time_cost_round_layer 0.028899 sec\n",
      "loss time 0.002182 sec\n",
      "backward time 0.011649 sec\n",
      "optimizer time 0.019261 sec\n",
      "round 77, time 2.601290 sec\n",
      "the calculation result for the round is:  2.5864641666412354\n",
      "round 78\n",
      "time to device 2.397837 sec\n",
      "sum_time_cost_round_layer 0.021810 sec\n",
      "loss time 0.001019 sec\n",
      "backward time 0.014898 sec\n",
      "optimizer time 0.003132 sec\n",
      "round 78, time 2.439630 sec\n",
      "the calculation result for the round is:  2.4386963844299316\n",
      "round 79\n",
      "time to device 2.446325 sec\n",
      "sum_time_cost_round_layer 0.031702 sec\n",
      "loss time 0.001743 sec\n",
      "backward time 0.010730 sec\n",
      "optimizer time 0.003186 sec\n",
      "round 79, time 2.495229 sec\n",
      "the calculation result for the round is:  2.4936861991882324\n",
      "round 80\n",
      "time to device 2.541034 sec\n",
      "sum_time_cost_round_layer 0.024795 sec\n",
      "loss time 0.004389 sec\n",
      "backward time 0.042924 sec\n",
      "optimizer time 0.020738 sec\n",
      "round 80, time 2.648005 sec\n",
      "the calculation result for the round is:  2.6338796615600586\n",
      "round 81\n",
      "time to device 2.456271 sec\n",
      "sum_time_cost_round_layer 0.004422 sec\n",
      "loss time 0.000693 sec\n",
      "backward time 0.004933 sec\n",
      "optimizer time 0.001991 sec\n",
      "round 81, time 2.469578 sec\n",
      "the calculation result for the round is:  2.4683094024658203\n",
      "round 82\n",
      "time to device 2.498526 sec\n",
      "sum_time_cost_round_layer 0.004319 sec\n",
      "loss time 0.000609 sec\n",
      "backward time 0.005535 sec\n",
      "optimizer time 0.002221 sec\n",
      "round 82, time 2.511736 sec\n",
      "the calculation result for the round is:  2.5112099647521973\n",
      "round 83\n",
      "time to device 2.539283 sec\n",
      "sum_time_cost_round_layer 0.024685 sec\n",
      "loss time 0.000891 sec\n",
      "backward time 0.006118 sec\n",
      "optimizer time 0.002821 sec\n",
      "round 83, time 2.574401 sec\n",
      "the calculation result for the round is:  2.5737979412078857\n",
      "round 84\n",
      "time to device 2.597046 sec\n",
      "sum_time_cost_round_layer 0.015830 sec\n",
      "loss time 0.007164 sec\n",
      "backward time 0.014423 sec\n",
      "optimizer time 0.003812 sec\n",
      "round 84, time 2.664134 sec\n",
      "the calculation result for the round is:  2.638274908065796\n",
      "round 85\n",
      "time to device 2.523649 sec\n",
      "sum_time_cost_round_layer 0.015634 sec\n",
      "loss time 0.001484 sec\n",
      "backward time 0.033664 sec\n",
      "optimizer time 0.012094 sec\n",
      "round 85, time 2.595661 sec\n",
      "the calculation result for the round is:  2.5865252017974854\n",
      "round 86\n",
      "time to device 2.588618 sec\n",
      "sum_time_cost_round_layer 0.028531 sec\n",
      "loss time 0.002277 sec\n",
      "backward time 0.013110 sec\n",
      "optimizer time 0.014984 sec\n",
      "round 86, time 2.657199 sec\n",
      "the calculation result for the round is:  2.647519588470459\n",
      "round 87\n",
      "time to device 2.404910 sec\n",
      "sum_time_cost_round_layer 0.047962 sec\n",
      "loss time 0.001365 sec\n",
      "backward time 0.014640 sec\n",
      "optimizer time 0.004637 sec\n",
      "round 87, time 2.474321 sec\n",
      "the calculation result for the round is:  2.473513603210449\n",
      "round 88\n",
      "time to device 2.459622 sec\n",
      "sum_time_cost_round_layer 0.022849 sec\n",
      "loss time 0.001532 sec\n",
      "backward time 0.014732 sec\n",
      "optimizer time 0.005106 sec\n",
      "round 88, time 2.504966 sec\n",
      "the calculation result for the round is:  2.5038416385650635\n",
      "round 89\n",
      "time to device 2.424031 sec\n",
      "sum_time_cost_round_layer 0.017627 sec\n",
      "loss time 0.001767 sec\n",
      "backward time 0.012646 sec\n",
      "optimizer time 0.003669 sec\n",
      "round 89, time 2.460903 sec\n",
      "the calculation result for the round is:  2.4597396850585938\n",
      "round 90\n",
      "time to device 2.495074 sec\n",
      "sum_time_cost_round_layer 0.022347 sec\n",
      "loss time 0.001822 sec\n",
      "backward time 0.008314 sec\n",
      "optimizer time 0.016125 sec\n",
      "round 90, time 2.548388 sec\n",
      "the calculation result for the round is:  2.5436813831329346\n",
      "round 91\n",
      "time to device 2.419250 sec\n",
      "sum_time_cost_round_layer 0.023247 sec\n",
      "loss time 0.001159 sec\n",
      "backward time 0.008986 sec\n",
      "optimizer time 0.001397 sec\n",
      "round 91, time 2.454905 sec\n",
      "the calculation result for the round is:  2.454038619995117\n",
      "round 92\n",
      "time to device 2.536081 sec\n",
      "sum_time_cost_round_layer 0.044720 sec\n",
      "loss time 0.002030 sec\n",
      "backward time 0.012193 sec\n",
      "optimizer time 0.002876 sec\n",
      "round 92, time 2.599106 sec\n",
      "the calculation result for the round is:  2.597900867462158\n",
      "round 93\n",
      "time to device 2.463360 sec\n",
      "sum_time_cost_round_layer 0.039736 sec\n",
      "loss time 0.001319 sec\n",
      "backward time 0.012533 sec\n",
      "optimizer time 0.003361 sec\n",
      "round 93, time 2.521908 sec\n",
      "the calculation result for the round is:  2.5203089714050293\n",
      "round 94\n",
      "time to device 2.489773 sec\n",
      "sum_time_cost_round_layer 0.027864 sec\n",
      "loss time 0.002507 sec\n",
      "backward time 0.014235 sec\n",
      "optimizer time 0.012412 sec\n",
      "round 94, time 2.559523 sec\n",
      "the calculation result for the round is:  2.546790599822998\n",
      "round 95\n",
      "time to device 2.458866 sec\n",
      "sum_time_cost_round_layer 0.008218 sec\n",
      "loss time 0.001428 sec\n",
      "backward time 0.012138 sec\n",
      "optimizer time 0.001510 sec\n",
      "round 95, time 2.482913 sec\n",
      "the calculation result for the round is:  2.4821603298187256\n",
      "round 96\n",
      "time to device 2.436931 sec\n",
      "sum_time_cost_round_layer 0.011232 sec\n",
      "loss time 0.001608 sec\n",
      "backward time 0.013678 sec\n",
      "optimizer time 0.003151 sec\n",
      "round 96, time 2.467370 sec\n",
      "the calculation result for the round is:  2.466599702835083\n",
      "round 97\n",
      "time to device 2.435882 sec\n",
      "sum_time_cost_round_layer 0.014739 sec\n",
      "loss time 0.001696 sec\n",
      "backward time 0.012446 sec\n",
      "optimizer time 0.003523 sec\n",
      "round 97, time 2.470197 sec\n",
      "the calculation result for the round is:  2.4682857990264893\n",
      "round 98\n",
      "time to device 2.423218 sec\n",
      "sum_time_cost_round_layer 0.011631 sec\n",
      "loss time 0.001255 sec\n",
      "backward time 0.012384 sec\n",
      "optimizer time 0.005685 sec\n",
      "round 98, time 2.455383 sec\n",
      "the calculation result for the round is:  2.4541728496551514\n",
      "round 99\n",
      "time to device 2.443335 sec\n",
      "sum_time_cost_round_layer 0.012703 sec\n",
      "loss time 0.001441 sec\n",
      "backward time 0.010164 sec\n",
      "optimizer time 0.001915 sec\n",
      "round 99, time 2.470876 sec\n",
      "the calculation result for the round is:  2.4695584774017334\n",
      "round 100\n",
      "time to device 2.431370 sec\n",
      "sum_time_cost_round_layer 0.030983 sec\n",
      "loss time 0.001558 sec\n",
      "backward time 0.014483 sec\n",
      "optimizer time 0.006188 sec\n",
      "round 100, time 2.485930 sec\n",
      "the calculation result for the round is:  2.4845821857452393\n",
      "round 101\n",
      "time to device 2.456925 sec\n",
      "sum_time_cost_round_layer 0.014528 sec\n",
      "loss time 0.003052 sec\n",
      "backward time 0.015069 sec\n",
      "optimizer time 0.005407 sec\n",
      "round 101, time 2.499949 sec\n",
      "the calculation result for the round is:  2.494981527328491\n",
      "round 102\n",
      "time to device 2.444177 sec\n",
      "sum_time_cost_round_layer 0.039538 sec\n",
      "loss time 0.003590 sec\n",
      "backward time 0.006733 sec\n",
      "optimizer time 0.002556 sec\n",
      "round 102, time 2.497897 sec\n",
      "the calculation result for the round is:  2.496593713760376\n",
      "round 103\n",
      "time to device 2.441075 sec\n",
      "sum_time_cost_round_layer 0.014063 sec\n",
      "loss time 0.001952 sec\n",
      "backward time 0.012083 sec\n",
      "optimizer time 0.004395 sec\n",
      "round 103, time 2.475583 sec\n",
      "the calculation result for the round is:  2.4735682010650635\n",
      "round 104\n",
      "time to device 2.469150 sec\n",
      "sum_time_cost_round_layer 0.043859 sec\n",
      "loss time 0.009999 sec\n",
      "backward time 0.024805 sec\n",
      "optimizer time 0.012712 sec\n",
      "round 104, time 2.566244 sec\n",
      "the calculation result for the round is:  2.5605247020721436\n",
      "round 105\n",
      "time to device 2.454428 sec\n",
      "sum_time_cost_round_layer 0.022856 sec\n",
      "loss time 0.001509 sec\n",
      "backward time 0.019171 sec\n",
      "optimizer time 0.004697 sec\n",
      "round 105, time 2.512991 sec\n",
      "the calculation result for the round is:  2.5026605129241943\n",
      "round 106\n",
      "time to device 2.454588 sec\n",
      "sum_time_cost_round_layer 0.009691 sec\n",
      "loss time 0.001595 sec\n",
      "backward time 0.013536 sec\n",
      "optimizer time 0.003671 sec\n",
      "round 106, time 2.484763 sec\n",
      "the calculation result for the round is:  2.4830806255340576\n",
      "round 107\n",
      "time to device 2.659556 sec\n",
      "sum_time_cost_round_layer 0.018411 sec\n",
      "loss time 0.001291 sec\n",
      "backward time 0.009694 sec\n",
      "optimizer time 0.003322 sec\n",
      "round 107, time 2.702721 sec\n",
      "the calculation result for the round is:  2.6922740936279297\n",
      "round 108\n",
      "time to device 2.586595 sec\n",
      "sum_time_cost_round_layer 0.037114 sec\n",
      "loss time 0.001656 sec\n",
      "backward time 0.009857 sec\n",
      "optimizer time 0.003845 sec\n",
      "round 108, time 2.640120 sec\n",
      "the calculation result for the round is:  2.639066696166992\n",
      "round 109\n",
      "time to device 2.694156 sec\n",
      "sum_time_cost_round_layer 0.025071 sec\n",
      "loss time 0.002322 sec\n",
      "backward time 0.012717 sec\n",
      "optimizer time 0.005322 sec\n",
      "round 109, time 2.756797 sec\n",
      "the calculation result for the round is:  2.7395877838134766\n",
      "round 110\n",
      "time to device 2.620959 sec\n",
      "sum_time_cost_round_layer 0.024269 sec\n",
      "loss time 0.002561 sec\n",
      "backward time 0.014659 sec\n",
      "optimizer time 0.005819 sec\n",
      "round 110, time 2.679009 sec\n",
      "the calculation result for the round is:  2.668266773223877\n",
      "round 111\n",
      "time to device 2.428384 sec\n",
      "sum_time_cost_round_layer 0.052755 sec\n",
      "loss time 0.001846 sec\n",
      "backward time 0.012578 sec\n",
      "optimizer time 0.003682 sec\n",
      "round 111, time 2.500197 sec\n",
      "the calculation result for the round is:  2.4992454051971436\n",
      "round 112\n",
      "time to device 2.428350 sec\n",
      "sum_time_cost_round_layer 0.020809 sec\n",
      "loss time 0.001437 sec\n",
      "backward time 0.011795 sec\n",
      "optimizer time 0.003668 sec\n",
      "round 112, time 2.467138 sec\n",
      "the calculation result for the round is:  2.4660589694976807\n",
      "round 113\n",
      "time to device 2.415900 sec\n",
      "sum_time_cost_round_layer 0.011157 sec\n",
      "loss time 0.001113 sec\n",
      "backward time 0.011875 sec\n",
      "optimizer time 0.003508 sec\n",
      "round 113, time 2.444298 sec\n",
      "the calculation result for the round is:  2.4435532093048096\n",
      "round 114\n",
      "time to device 2.548498 sec\n",
      "sum_time_cost_round_layer 0.022821 sec\n",
      "loss time 0.001606 sec\n",
      "backward time 0.031833 sec\n",
      "optimizer time 0.007470 sec\n",
      "round 114, time 2.624289 sec\n",
      "the calculation result for the round is:  2.6122283935546875\n",
      "round 115\n",
      "time to device 2.423941 sec\n",
      "sum_time_cost_round_layer 0.026690 sec\n",
      "loss time 0.001413 sec\n",
      "backward time 0.010947 sec\n",
      "optimizer time 0.003281 sec\n",
      "round 115, time 2.467141 sec\n",
      "the calculation result for the round is:  2.4662725925445557\n",
      "round 116\n",
      "time to device 2.445136 sec\n",
      "sum_time_cost_round_layer 0.011518 sec\n",
      "loss time 0.003423 sec\n",
      "backward time 0.014629 sec\n",
      "optimizer time 0.003692 sec\n",
      "round 116, time 2.479042 sec\n",
      "the calculation result for the round is:  2.478398323059082\n",
      "round 117\n",
      "time to device 2.531331 sec\n",
      "sum_time_cost_round_layer 0.858281 sec\n",
      "loss time 0.134837 sec\n",
      "backward time 1.191525 sec\n",
      "optimizer time 0.003188 sec\n",
      "round 117, time 4.730121 sec\n",
      "the calculation result for the round is:  4.7191619873046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtjgp/miniconda3/envs/d2l/lib/python3.8/site-packages/d2l/torch.py:3507: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, time 321.143429 sec\n",
      "Conv2d time 2.797619 sec\n",
      "ReLU time 0.870073 sec\n",
      "MaxPool2d time 0.673418 sec\n",
      "Linear time 0.461716 sec\n",
      "Dropout time 1.019217 sec\n",
      "Flatten time 0.012279 sec\n",
      "time to device 288.908734 sec\n",
      "[[[2.79761887 0.        ]\n",
      "  [0.87007284 0.        ]\n",
      "  [0.67341828 0.        ]\n",
      "  [0.46171594 0.        ]\n",
      "  [1.01921678 0.        ]\n",
      "  [0.0122788  0.        ]]]\n",
      "loss time 0.897935 sec\n",
      "backward time 5.052314 sec\n",
      "optimizer time 1.489554 sec\n",
      "time test acc 15.784601 sec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "这段是将d2l中的train_ch6()函数拷贝过来，然后修改了一下，目的是为了能够显示在进行每一层的前向传播的时候，能够显示出来\n",
    "修改了其中的metric部分，对该部分进行了注释，而是调用了简单的optimizer.no_grad()\n",
    "得调整一下整体的代码，需要对循环内部的东西全部重写一下，然后再进行测试\n",
    "'''\n",
    "lr, num_epochs = 0.01, 1 # learning rate and number of epochs\n",
    "def init_weights(m): # 初始化权重\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "net.apply(init_weights)\n",
    "\n",
    "list_layer_name = ['Conv2d','ReLU', 'MaxPool2d','Linear','Dropout','Flatten'] # 该模型中包括的所有的层的名字\n",
    "\n",
    "# create a numpy array to store the time and energy, the shape is (num_epochs, len(list_layer_name), 2)\n",
    "# the 2 means time and energy, respectively\n",
    "# 目标是将每次循环过程中，在运行每一个层的时候的所需时间全部记录下来（第一步测试）\n",
    "# 如果测试成功，则对后续的能耗进行测试\n",
    "time_energy_data = np.zeros((num_epochs, len(list_layer_name), 2)) # for epoch = 1, the shape is (6,2)，第一列是时间，第二列是能耗\n",
    "\n",
    "# create another numpy array to store the time and energy for loss, backward and optimizer\n",
    "# the shape is (num_epochs, 3)\n",
    "# # 增加这个的目的是为了记录loss，backward和optimizer的时间，看看整体计算的过程中是否有问题\n",
    "# time_energy_data_loss_backward_optimizer = np.zeros((num_epochs, 3))\n",
    "'''将loss, backward and optimizer的时间改为单独计算，先进行测试，看看结果如何'''\n",
    "time_cost_loss = 0\n",
    "time_cost_backward = 0\n",
    "time_cost_optimizer = 0\n",
    "\n",
    "# initialize a time_to_device_cost to store the time cost of the device\n",
    "time_to_device_cost = 0\n",
    "\n",
    "# 记录其他部分的时间，先加上，看看结果\n",
    "# time_cost_other = 0\n",
    "\n",
    "# 记录计算平均loss和平均准确度的时间，目前暂时不需要\n",
    "# time_avg_cost = 0\n",
    "\n",
    "# 记录计算test_acc的时间\n",
    "time_test_acc_cost = 0\n",
    "\n",
    "# 设定运行的设备\n",
    "device = 'mps'\n",
    "print('training on', 'mps')\n",
    "net.to('mps') # 将模型放到对应的设备上\n",
    "\n",
    "# 初始化optimizer和loss\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 对动画部分先进行注释，暂时先不对该部分进行考虑\n",
    "# animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "#                         legend=['train loss', 'train acc', 'test acc'])\n",
    "\n",
    "# 初始化计时器\n",
    "timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    timer.start() # 开始计时\n",
    "    # run powermetrics\n",
    "    powermetrics_process = run_powermetrics(energy_file)\n",
    "    # 训练损失之和，训练准确率之和，样本数\n",
    "    # metric = d2l.Accumulator(3)\n",
    "    net.train() # 设置为训练模式\n",
    "\n",
    "    # 将time_energy_data的第一个维度设置为epoch\n",
    "    time_energy_data[epoch,:,:] = epoch\n",
    "    # 将time_energy_data_backward_optimizer的第一个维度设置为epoch\n",
    "    # time_energy_data_loss_backward_optimizer[epoch,:] = epoch\n",
    "    # 增加一个time_energy_data_backward_optimizer_i, 用于记录每一次backward和optimizer的时间和能量\n",
    "    # time_energy_data_loss_backward_optimizer_i = np.zeros((3))\n",
    "\n",
    "    for i, (X, y) in enumerate(train_iter): # 每个循环结束的时候有个100s的睡眠时间，用于较长时间显示一下每一轮的数据\n",
    "        '''每一轮的运行结束之后，都会有一个100s的睡眠时间，用于显示一下每一轮的数据\n",
    "        数据包括：\n",
    "        1. 运行该轮的时间\n",
    "        2. 在该轮运行中，每一层的运行时间\n",
    "        3. 在该轮运行中，loss，backward和optimizer的运行时间\n",
    "        4. 在该轮运行中，将数据放到对应的设备上的时间'''\n",
    "        if i < 500:\n",
    "            # 显示是第几轮\n",
    "            print('round %d' % (i))\n",
    "            time_round = time.time() # 计算每一轮的时间\n",
    "            optimizer.zero_grad() # 将optimizer的梯度清零\n",
    "\n",
    "            # 增加一个time_energy_data_i, 用于记录每一次前向传播的时间和能量\n",
    "            time_energy_data_i = np.zeros((len(list_layer_name),2))\n",
    "            sum_time_cost_round = 0 # 用于记录每一轮的时间\n",
    "\n",
    "            # 计算将数据放到对应的设备上的时间\n",
    "            time_to_device_cost_i = 0\n",
    "            time_to_device = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            time_to_device_end = time.time()\n",
    "            time_to_device_cost_i = time_to_device_end - time_to_device\n",
    "            print('time to device %f sec' % (time_to_device_cost_i))\n",
    "            time_to_device_cost += time_to_device_cost_i\n",
    "\n",
    "            # initialize a time_cost_other_i to store the time cost of the other part\n",
    "            time_cost_other_i = 0\n",
    "            # initialize a time_avg_cost_i to store the time cost of the avg part\n",
    "            # time_avg_cost_i = 0\n",
    "\n",
    "            # 将原本的y_hat = net(X)改为下面的形式，目的是为了让模型逐层运行，并且在这个过程中，记录起运行的时间以及消耗的能量\n",
    "            y_hat = X\n",
    "            for layer in net:\n",
    "                # 每次循环开始的时候进行初始化，防止后面计算过程中出现错误\n",
    "                time_cost_layer = 0\n",
    "                # print(layer.__class__.__name__,'output shape:\\t',y_hat.shape)\n",
    "                layer_name = layer.__class__.__name__\n",
    "                # print(layer_name)\n",
    "                # check if the layer_name is in the list_layer_name，if yes, then act the code below, else, continue\n",
    "                # if layer_name in list_layer_name:\n",
    "                # find out the layer name is in where of the list\n",
    "                layer_index = list_layer_name.index(layer_name)\n",
    "                \n",
    "                # calculate the energy and time of each layer\n",
    "                # 这里的时间和能量都是在每一层的前向传播的时候，进行计算的\n",
    "                time_start_layer = time.time()\n",
    "                '''energy部分后续加入，先进行测试时间计算'''\n",
    "                # energy_start = 0\n",
    "                y_hat = layer(y_hat)\n",
    "                time_end_layer = time.time()\n",
    "                time_cost_layer = time_end_layer - time_start_layer\n",
    "                # print the result\n",
    "                # print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "                # print('*'*50)\n",
    "                # 将对应的layer_index的时间和能量加入到time_energy_data中\n",
    "                time_energy_data_i[layer_index,0] += time_cost_layer\n",
    "                # time_energy_data_i[layer_index,1] += energy_cost_layer\n",
    "                # print(time_energy_data_i)\n",
    "\n",
    "                # 加入一个sleep，用于测试\n",
    "                # time.sleep(2)\n",
    "\n",
    "            # 将time_energy_data_i加入到time_energy_data中\n",
    "            time_energy_data[epoch,:,:] += time_energy_data_i\n",
    "            # 求time_energy_data_i的和，用于计算每一轮的时间\n",
    "            sum_time_cost_round_layer = np.sum(time_energy_data_i[:,0])\n",
    "            print('sum_time_cost_round_layer %f sec' % (sum_time_cost_round_layer))\n",
    "            # 显示一下time_energy_data\n",
    "            # print(time_energy_data[epoch,:,:])\n",
    "            # print(\"-\"*50)\n",
    "            # time.sleep(3)\n",
    "\n",
    "            # loss部分\n",
    "            time_cost_loss_i = 0 # 初始化loss的时间\n",
    "            time_start_loss = time.time()\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            time_end_loss = time.time()\n",
    "            time_cost_loss_i = time_end_loss - time_start_loss\n",
    "            print('loss time %f sec' % (time_cost_loss_i))\n",
    "            time_cost_loss += time_cost_loss_i\n",
    "            # 对time_cost_loss数据进行累加\n",
    "            # time_energy_data_loss_backward_optimizer_i[0] += time_cost_loss\n",
    "\n",
    "            # backward部分\n",
    "            time_cost_backward_i = 0 # 初始化backward的时间\n",
    "            time_start_backward = time.time()\n",
    "            loss.backward()\n",
    "            time_end_backward = time.time()\n",
    "            time_cost_backward_i = time_end_backward - time_start_backward\n",
    "            print('backward time %f sec' % (time_cost_backward_i))\n",
    "            time_cost_backward += time_cost_backward_i\n",
    "            # 对time_cost_backward数据进行累加\n",
    "            # time_energy_data_loss_backward_optimizer_i[1] += time_cost_backward\n",
    "\n",
    "            # optimizer部分\n",
    "            time_cost_optimizer_i = 0 # 初始化optimizer的时间\n",
    "            time_start_optimizer = time.time()\n",
    "            optimizer.step()\n",
    "            time_end_optimizer = time.time()\n",
    "            time_cost_optimizer_i = time_end_optimizer - time_start_optimizer\n",
    "            print('optimizer time %f sec' % (time_cost_optimizer_i))\n",
    "            time_cost_optimizer += time_cost_optimizer_i\n",
    "            # 对time_cost_optimizer数据进行累加\n",
    "            # time_energy_data_loss_backward_optimizer_i[2] += time_cost_optimizer\n",
    "\n",
    "            # 将time_energy_data_backward_optimizer_i加入到time_energy_data_backward_optimizer中\n",
    "            # time_energy_data_loss_backward_optimizer[epoch,:] += time_energy_data_loss_backward_optimizer_i\n",
    "            \n",
    "            '''\n",
    "            该部分是optimizer.zero_grad()部分，并且计算相对应的时间\n",
    "            不计算梯度：torch.no_grad()上下文管理器确保不计算任何梯度。这实际上减少了计算和内存需求，因为它不会保留计算图中的中间状态。从这个角度看，使用torch.no_grad()会节省能源。\n",
    "            理论上应该节省能源，但是在计算过程中，该部分消耗的时间是最长的\n",
    "            为了进行测试，将李沐原本的torch.no_grad()进行替换，更换为了optimizer.zero_grad()\n",
    "            更新10.17：删除掉该部分的optimizer.zero_grad()，因为在循环开始的部分已经申明了\n",
    "            '''\n",
    "            # other time about torch.no_grad()\n",
    "            # time_cost_other_i = 0 # 初始化other的时间\n",
    "            # time about torch.no_grad()\n",
    "            # time_other_start = time.time()\n",
    "            # with torch.no_grad():\n",
    "            #     metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            # if i % 100 == 0:\n",
    "            #     loss, current = l.item(), (i + 1) * len(X)\n",
    "            #     print(f\"loss: {loss:>7f}  [{i:>5d}/{num_batches:>5d}]\")\n",
    "            '''该部分是计算train_l和train_acc的部分，以及其运行过程所需时间的代码'''\n",
    "            # time about avg the train_l and train_acc\n",
    "            # time_avg_start = time.time()\n",
    "            # train_l = metric[0] / metric[2]\n",
    "            # train_acc = metric[1] / metric[2]\n",
    "            # time_avg_end = time.time()\n",
    "            # time_avg_cost_i = time_avg_end - time_avg_start\n",
    "            # time_avg_cost += time_avg_cost_i\n",
    "            # if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "            #     animator.add(epoch + (i + 1) / num_batches,\n",
    "            #                     (train_l, train_acc, None))\n",
    "            # time_other_end = time.time()\n",
    "            # time_cost_other_i = time_other_end - time_other_start\n",
    "            # print('other time %f sec' % (time_cost_other_i))\n",
    "            # time_cost_other += time_cost_other_i\n",
    "\n",
    "            time_end_round = time.time()\n",
    "            time_cost_round = time_end_round - time_round\n",
    "            print('round %d, time %f sec' % (i, time_cost_round))\n",
    "            sum_time_cost_round = time_to_device_cost_i + time_cost_loss_i + time_cost_backward_i + time_cost_optimizer_i\n",
    "            sum_time_cost_round += sum_time_cost_round_layer\n",
    "            print('the calculation result for the round is: ', sum_time_cost_round)\n",
    "\n",
    "            # 设定一个time.sleep, 用于测试\n",
    "            # time.sleep(10)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    time_test_acc_cost_epoch = 0\n",
    "    time_test_acc_start = time.time()\n",
    "    # test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "    test_round = test_loop(test_iter, net, loss_fn, device)\n",
    "    time_test_acc_end = time.time()\n",
    "    time_test_acc_cost_epoch = time_test_acc_end - time_test_acc_start\n",
    "    time_test_acc_cost += time_test_acc_cost_epoch\n",
    "    # animator.add(epoch + 1, (None, None, test_acc))\n",
    "\n",
    "    powermetrics_process.terminate()\n",
    "    powermetrics_process.wait()\n",
    "    \n",
    "    timer.stop() # 停止计时 \n",
    "    \n",
    "    print('epoch %d, time %f sec' % (epoch, timer.sum()))\n",
    "    # 输出一下每种层的训练时间\n",
    "    for j in range(len(list_layer_name)):\n",
    "        print(list_layer_name[j], 'time %f sec' % (time_energy_data[epoch,j,0]))\n",
    "    \n",
    "    # # 输出一下device的时间\n",
    "    # print('device time %f sec' % (time_to_device_cost))\n",
    "\n",
    "    # 输出一下backward和optimizer的时间\n",
    "    # print('backward time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,1]))\n",
    "    # print('optimizer time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,2]))\n",
    "    # print('loss time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,0]))\n",
    "# print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "#         f'test acc {test_acc:.3f}')\n",
    "# print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "#         f'on {str(device)}')\n",
    "\n",
    "# print('other time %f sec' % (time_cost_other))\n",
    "print('time to device %f sec' % (time_to_device_cost))\n",
    "print(time_energy_data)\n",
    "# print(time_energy_data_loss_backward_optimizer)\n",
    "print('loss time %f sec' % (time_cost_loss))\n",
    "print('backward time %f sec' % (time_cost_backward))\n",
    "print('optimizer time %f sec' % (time_cost_optimizer))\n",
    "# print('time avg %f sec' % (time_avg_cost))\n",
    "print('time test acc %f sec' % (time_test_acc_cost))\n",
    "# 显示总体运行时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8117a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum of time_energy_data is 5.834321 sec\n",
      "the sum of total is 317.967460 sec\n"
     ]
    }
   ],
   "source": [
    "time_energy_data_sum = np.sum(time_energy_data[:,:,0])\n",
    "print('the sum of time_energy_data is %f sec' % (time_energy_data_sum))\n",
    "\n",
    "sum_total = time_to_device_cost + time_energy_data_sum + time_cost_loss + time_cost_backward + time_cost_optimizer + time_test_acc_cost\n",
    "print('the sum of total is %f sec' % (sum_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "817e0c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11588 mW\n",
      "13809 mW\n",
      "8857 mW\n",
      "17308 mW\n",
      "19979 mW\n",
      "17072 mW\n",
      "11718 mW\n",
      "11806 mW\n",
      "11635 mW\n",
      "13852 mW\n",
      "10972 mW\n",
      "12123 mW\n",
      "11097 mW\n",
      "11100 mW\n",
      "11338 mW\n",
      "9650 mW\n",
      "11098 mW\n",
      "11632 mW\n",
      "10863 mW\n",
      "11722 mW\n",
      "10077 mW\n",
      "11542 mW\n",
      "11554 mW\n",
      "12245 mW\n",
      "10874 mW\n",
      "11804 mW\n",
      "17279 mW\n",
      "18659 mW\n",
      "14140 mW\n",
      "10627 mW\n",
      "11636 mW\n",
      "12245 mW\n",
      "10943 mW\n",
      "16001 mW\n",
      "10854 mW\n",
      "11267 mW\n",
      "14140 mW\n",
      "13193 mW\n",
      "13495 mW\n",
      "11513 mW\n",
      "10950 mW\n",
      "11273 mW\n",
      "14179 mW\n",
      "17406 mW\n",
      "13395 mW\n",
      "12551 mW\n",
      "14701 mW\n",
      "15272 mW\n",
      "10800 mW\n",
      "13633 mW\n",
      "12752 mW\n",
      "12519 mW\n",
      "12252 mW\n",
      "12419 mW\n",
      "15182 mW\n",
      "13725 mW\n",
      "15899 mW\n",
      "13780 mW\n",
      "11495 mW\n",
      "10878 mW\n",
      "14393 mW\n",
      "12261 mW\n",
      "11436 mW\n",
      "15595 mW\n",
      "16945 mW\n",
      "12930 mW\n",
      "13455 mW\n",
      "12236 mW\n",
      "12865 mW\n",
      "13234 mW\n",
      "11956 mW\n",
      "11476 mW\n",
      "12970 mW\n",
      "13762 mW\n",
      "11092 mW\n",
      "11753 mW\n",
      "12086 mW\n",
      "11062 mW\n",
      "12783 mW\n",
      "11908 mW\n",
      "10945 mW\n",
      "11784 mW\n",
      "11754 mW\n",
      "12000 mW\n",
      "12735 mW\n",
      "10403 mW\n",
      "11458 mW\n",
      "10672 mW\n",
      "11070 mW\n",
      "11079 mW\n",
      "11435 mW\n",
      "13666 mW\n",
      "13251 mW\n",
      "12420 mW\n",
      "11289 mW\n",
      "10393 mW\n",
      "11466 mW\n",
      "10859 mW\n",
      "11552 mW\n",
      "11866 mW\n",
      "10573 mW\n",
      "11306 mW\n",
      "10786 mW\n",
      "11096 mW\n",
      "11093 mW\n",
      "11266 mW\n",
      "12116 mW\n",
      "12304 mW\n",
      "12091 mW\n",
      "11151 mW\n",
      "10889 mW\n",
      "11349 mW\n",
      "10692 mW\n",
      "10457 mW\n",
      "11194 mW\n",
      "10251 mW\n",
      "10584 mW\n",
      "10731 mW\n",
      "10474 mW\n",
      "11844 mW\n",
      "12183 mW\n",
      "10583 mW\n",
      "16218 mW\n",
      "12709 mW\n",
      "11839 mW\n",
      "10581 mW\n",
      "10625 mW\n",
      "11034 mW\n",
      "12009 mW\n",
      "11265 mW\n",
      "11194 mW\n",
      "10490 mW\n",
      "11272 mW\n",
      "10728 mW\n",
      "13638 mW\n",
      "11661 mW\n",
      "10467 mW\n",
      "10901 mW\n",
      "10836 mW\n",
      "11736 mW\n",
      "11323 mW\n",
      "11252 mW\n",
      "10980 mW\n",
      "11565 mW\n",
      "11193 mW\n",
      "10618 mW\n",
      "10796 mW\n",
      "10721 mW\n",
      "11517 mW\n",
      "10688 mW\n",
      "10866 mW\n",
      "11702 mW\n",
      "11179 mW\n",
      "11794 mW\n",
      "10495 mW\n",
      "10777 mW\n",
      "10751 mW\n",
      "11048 mW\n",
      "10550 mW\n",
      "10817 mW\n",
      "11287 mW\n",
      "10822 mW\n",
      "10642 mW\n",
      "10427 mW\n",
      "10937 mW\n",
      "10661 mW\n",
      "10831 mW\n",
      "10966 mW\n",
      "10332 mW\n",
      "11613 mW\n",
      "10549 mW\n",
      "10880 mW\n",
      "10488 mW\n",
      "10566 mW\n",
      "9750 mW\n",
      "11160 mW\n",
      "10516 mW\n",
      "10430 mW\n",
      "11931 mW\n",
      "11091 mW\n",
      "11833 mW\n",
      "13211 mW\n",
      "11155 mW\n",
      "11385 mW\n",
      "10500 mW\n",
      "11550 mW\n",
      "10683 mW\n",
      "11321 mW\n",
      "10691 mW\n",
      "11023 mW\n",
      "10588 mW\n",
      "10770 mW\n",
      "10937 mW\n",
      "10759 mW\n",
      "11685 mW\n",
      "11951 mW\n",
      "11988 mW\n",
      "10845 mW\n",
      "11098 mW\n",
      "11372 mW\n",
      "10521 mW\n",
      "10772 mW\n",
      "10996 mW\n",
      "13708 mW\n",
      "15634 mW\n",
      "11457 mW\n",
      "13289 mW\n",
      "13450 mW\n",
      "11962 mW\n",
      "12953 mW\n",
      "11344 mW\n",
      "13171 mW\n",
      "13564 mW\n",
      "15144 mW\n",
      "13694 mW\n",
      "14269 mW\n",
      "11129 mW\n",
      "13658 mW\n",
      "14901 mW\n",
      "10670 mW\n",
      "11290 mW\n",
      "10861 mW\n",
      "10682 mW\n",
      "10700 mW\n",
      "9864 mW\n",
      "11757 mW\n",
      "11248 mW\n",
      "11156 mW\n",
      "11514 mW\n",
      "12853 mW\n",
      "11822 mW\n",
      "10936 mW\n",
      "11802 mW\n",
      "12082 mW\n",
      "10975 mW\n",
      "12714 mW\n",
      "11085 mW\n",
      "12298 mW\n",
      "12885 mW\n",
      "10773 mW\n",
      "12688 mW\n",
      "13219 mW\n",
      "12374 mW\n",
      "10942 mW\n",
      "10585 mW\n",
      "11056 mW\n",
      "11158 mW\n",
      "11185 mW\n",
      "11130 mW\n",
      "11344 mW\n",
      "10793 mW\n",
      "11737 mW\n",
      "11820 mW\n",
      "11215 mW\n",
      "11734 mW\n",
      "10564 mW\n",
      "12752 mW\n",
      "11296 mW\n",
      "11338 mW\n",
      "11294 mW\n",
      "11045 mW\n",
      "10694 mW\n",
      "12501 mW\n",
      "11667 mW\n",
      "11121 mW\n",
      "11408 mW\n",
      "10573 mW\n",
      "12486 mW\n",
      "13540 mW\n",
      "12150 mW\n",
      "15845 mW\n",
      "13259 mW\n",
      "12451 mW\n",
      "12157 mW\n",
      "10845 mW\n",
      "14383 mW\n",
      "15841 mW\n",
      "15744 mW\n",
      "11971 mW\n",
      "16969 mW\n",
      "11099 mW\n",
      "10973 mW\n",
      "10259 mW\n",
      "10724 mW\n",
      "11956 mW\n",
      "10277 mW\n",
      "11146 mW\n",
      "11412 mW\n",
      "14804 mW\n",
      "12376 mW\n",
      "10829 mW\n",
      "11126 mW\n",
      "11475 mW\n",
      "11387 mW\n",
      "10641 mW\n",
      "11792 mW\n",
      "14068 mW\n",
      "9375 mW\n",
      "6419 mW\n",
      "10400 mW\n",
      "11373 mW\n",
      "11511 mW\n",
      "12902 mW\n",
      "11472 mW\n",
      "12101 mW\n",
      "11043 mW\n",
      "10803 mW\n",
      "11325 mW\n",
      "11044 mW\n",
      "11170 mW\n",
      "10761 mW\n",
      "11519 mW\n",
      "10930 mW\n",
      "11050 mW\n",
      "11154 mW\n",
      "[11588, 13809, 8857, 17308, 19979, 17072, 11718, 11806, 11635, 13852, 10972, 12123, 11097, 11100, 11338, 9650, 11098, 11632, 10863, 11722, 10077, 11542, 11554, 12245, 10874, 11804, 17279, 18659, 14140, 10627, 11636, 12245, 10943, 16001, 10854, 11267, 14140, 13193, 13495, 11513, 10950, 11273, 14179, 17406, 13395, 12551, 14701, 15272, 10800, 13633, 12752, 12519, 12252, 12419, 15182, 13725, 15899, 13780, 11495, 10878, 14393, 12261, 11436, 15595, 16945, 12930, 13455, 12236, 12865, 13234, 11956, 11476, 12970, 13762, 11092, 11753, 12086, 11062, 12783, 11908, 10945, 11784, 11754, 12000, 12735, 10403, 11458, 10672, 11070, 11079, 11435, 13666, 13251, 12420, 11289, 10393, 11466, 10859, 11552, 11866, 10573, 11306, 10786, 11096, 11093, 11266, 12116, 12304, 12091, 11151, 10889, 11349, 10692, 10457, 11194, 10251, 10584, 10731, 10474, 11844, 12183, 10583, 16218, 12709, 11839, 10581, 10625, 11034, 12009, 11265, 11194, 10490, 11272, 10728, 13638, 11661, 10467, 10901, 10836, 11736, 11323, 11252, 10980, 11565, 11193, 10618, 10796, 10721, 11517, 10688, 10866, 11702, 11179, 11794, 10495, 10777, 10751, 11048, 10550, 10817, 11287, 10822, 10642, 10427, 10937, 10661, 10831, 10966, 10332, 11613, 10549, 10880, 10488, 10566, 9750, 11160, 10516, 10430, 11931, 11091, 11833, 13211, 11155, 11385, 10500, 11550, 10683, 11321, 10691, 11023, 10588, 10770, 10937, 10759, 11685, 11951, 11988, 10845, 11098, 11372, 10521, 10772, 10996, 13708, 15634, 11457, 13289, 13450, 11962, 12953, 11344, 13171, 13564, 15144, 13694, 14269, 11129, 13658, 14901, 10670, 11290, 10861, 10682, 10700, 9864, 11757, 11248, 11156, 11514, 12853, 11822, 10936, 11802, 12082, 10975, 12714, 11085, 12298, 12885, 10773, 12688, 13219, 12374, 10942, 10585, 11056, 11158, 11185, 11130, 11344, 10793, 11737, 11820, 11215, 11734, 10564, 12752, 11296, 11338, 11294, 11045, 10694, 12501, 11667, 11121, 11408, 10573, 12486, 13540, 12150, 15845, 13259, 12451, 12157, 10845, 14383, 15841, 15744, 11971, 16969, 11099, 10973, 10259, 10724, 11956, 10277, 11146, 11412, 14804, 12376, 10829, 11126, 11475, 11387, 10641, 11792, 14068, 9375, 6419, 10400, 11373, 11511, 12902, 11472, 12101, 11043, 10803, 11325, 11044, 11170, 10761, 11519, 10930, 11050, 11154]\n",
      "315\n",
      "3737888\n",
      "0.0010383022222222222\n",
      "the total training time is 315 sec\n"
     ]
    }
   ],
   "source": [
    "energy_consumption, power_list_model = txt_data_process(energy_file)\n",
    "total_training_time = len(power_list_model)\n",
    "print('the total training time is %d sec' % (total_training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94764ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3737.888"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# power_list_model\n",
    "energy_consumption_J = energy_consumption * 3600000\n",
    "energy_consumption_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cbd6b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.866311111111111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_energy_cost_persec = energy_consumption_J / total_training_time\n",
    "avg_energy_cost_persec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d317a463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.79761887, 0.        ],\n",
       "        [0.87007284, 0.        ],\n",
       "        [0.67341828, 0.        ],\n",
       "        [0.46171594, 0.        ],\n",
       "        [1.01921678, 0.        ],\n",
       "        [0.0122788 , 0.        ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change time_energy_data to np array\n",
    "time_energy_data_np = np.array(time_energy_data)\n",
    "time_energy_data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a734c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abfbd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.79761887e+00 3.31974158e+01]\n",
      "  [8.70072842e-01 1.03245550e+01]\n",
      "  [6.73418283e-01 7.99099086e+00]\n",
      "  [4.61715937e-01 5.47886495e+00]\n",
      "  [1.01921678e+00 1.20943434e+01]\n",
      "  [1.22787952e-02 1.45704004e-01]]]\n"
     ]
    }
   ],
   "source": [
    "# calculate the energy consumption of each type of layer\n",
    "for i in range(len(time_energy_data_np[epoch,:,0])):\n",
    "    time_energy_data_np[epoch, i, 1] = time_energy_data_np[epoch, i, 0] * avg_energy_cost_persec\n",
    "\n",
    "print(time_energy_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b1c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the energy consumption of loss and backward and optimizer\n",
    "loss_energy_consumption = time_cost_loss * avg_energy_cost_persec\n",
    "backward_energy_consumption = time_cost_backward * avg_energy_cost_persec\n",
    "optimizer_energy_consumption = time_cost_optimizer * avg_energy_cost_persec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1876aca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "# x axis is the list_layer_name and the \n",
    "x = list_layer_name\n",
    "# print(x)\n",
    "# y axis\n",
    "y = time_energy_data_np[epoch,:,1]\n",
    "# print(y)\n",
    "# plot the bar chart using different colors\n",
    "plt.figure(figsize=(20,10))\n",
    "# plt.bar(x, y, label='energy consumption of each layer')\n",
    "plt.bar(x, y, label='energy consumption of each layer', color=['b','g','r','c','m','y'])\n",
    "plt.xlabel('layer name')\n",
    "plt.ylabel('energy consumption (J)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52c308cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total energy consumption of forward is 69.231874 J\n"
     ]
    }
   ],
   "source": [
    "# calculate the total sumation of the energy consumption\n",
    "total_energy_consumption_forward = np.sum(time_energy_data_np[epoch,:,1])\n",
    "print('the total energy consumption of forward is %f J' % (total_energy_consumption_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "074342c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.655177871873644\n",
      "59.95233309495714\n",
      "17.675510330878364\n",
      "69.23187402784559\n"
     ]
    }
   ],
   "source": [
    "print(loss_energy_consumption)\n",
    "print(backward_energy_consumption)\n",
    "print(optimizer_energy_consumption)\n",
    "print(total_energy_consumption_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c99d47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results of forward, loss, backward and optimizer\n",
    "x = ['forward', 'loss', 'backward', 'optimizer']\n",
    "y = [total_energy_consumption_forward, loss_energy_consumption, backward_energy_consumption, optimizer_energy_consumption]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x, y, label='energy consumption of each part', color=['b','g','r','c'])\n",
    "plt.xlabel('part name')\n",
    "plt.ylabel('energy consumption (J)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c8094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
