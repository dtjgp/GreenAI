{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to gather the information of the energy consumption of the whole training process of different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ptflops import get_model_complexity_info\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import pynvml\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current path is: /root/autodl-tmp/GreenAI/3080\n",
      "The data path is: /root/autodl-tmp/GreenAI/3080/ModelsData\n"
     ]
    }
   ],
   "source": [
    "'''find the Model path'''\n",
    "# find the current path\n",
    "from pathlib import Path\n",
    "\n",
    "# find the current path\n",
    "current_path = Path.cwd()\n",
    "print('The current path is:', current_path)\n",
    "\n",
    "# find the data path\n",
    "data_path = Path(current_path / 'ModelsData')\n",
    "print('The data path is:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = ['alexnet', \n",
    "               'vgg11', 'vgg13', 'vgg16', \n",
    "               'resnet18', 'resnet34', 'resnet50',\n",
    "               'googlenet_origin', 'googlenet_mod1', 'googlenet_mod2', 'googlenet_mod3',\n",
    "               'googlenet_mod4', 'googlenet_mod5', 'googlenet_mod6', 'googlenet_mod7', \n",
    "               'googlenet_mod8', 'googlenet_mod9',\n",
    "               'mobilenetv1_path', 'mobilenetv2_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/alexnet'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/vgg11'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/vgg13'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/vgg16'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/resnet18'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/resnet34'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/resnet50'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_origin'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod1'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod2'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod3'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod4'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod5'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod6'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod7'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod8'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod9'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/mobilenetv1_path'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/mobilenetv2_path')]\n"
     ]
    }
   ],
   "source": [
    "DataList = [Path(f\"{data_path}/{i}\") for i in models_name]\n",
    "print(DataList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(img_channel, num_labels):\n",
    "    net = nn.Sequential(\n",
    "        # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "        # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "        # 另外，输出通道的数目远大于LeNet\n",
    "        nn.Conv2d(img_channel, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "        nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "        # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "        # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "        nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.AdaptiveAvgPool2d((6, 6)),   # 使用全局平均池化对每个通道中所有元素求平均并直接将结果传递到全连接层\n",
    "        nn.Flatten(),\n",
    "        # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "        nn.Linear(256 * 6 * 6, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "        nn.Linear(4096, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG11 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg11(input_channels, output_channels):\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg11_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG13 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg13_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg13(input_channels, output_channels):\n",
    "    # VGG-13 architecture\n",
    "    conv_arch = [(2, 64), (2, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg13_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg16(input_channels, output_channels):\n",
    "    conv_arch = [(2, 64), (2, 128), (3, 256), (3, 512), (3, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg16_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual18(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.relu1(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return self.relu2(Y)\n",
    "    \n",
    "    \n",
    "def resnet18(img_channel, num_labels):\n",
    "    # blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                    first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual18(input_channels, num_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual18(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                        nn.AdaptiveAvgPool2d((1,1)),\n",
    "                        nn.Flatten(), nn.Linear(512, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet34 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual34(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "    \n",
    "def resnet34(img_channel, num_labels):\n",
    "    # blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                    first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual34(input_channels, num_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual34(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 3, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 4))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 6))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 3))\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                        nn.AdaptiveAvgPool2d((1,1)),\n",
    "                        nn.Flatten(), nn.Linear(512, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual50(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels , kernel_size=1, stride=strides, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels )\n",
    "        self.conv2 = nn.Conv2d(num_channels , num_channels , kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels )\n",
    "        self.conv3 = nn.Conv2d(num_channels , num_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "        if use_1x1conv or strides != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides, bias=False),\n",
    "                nn.BatchNorm2d(num_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        if self.downsample:\n",
    "            X = self.downsample(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "def resnet50(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(\n",
    "        nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    )\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual50(input_channels, num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual50(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 3, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 4))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 6))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 3))\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        b1, b2, b3, b4, b5,\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(), nn.Linear(512, num_labels)\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogleNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet Model(orinigal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod1(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod1, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod1(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod1(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod1(256-64, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod1(480-128, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod1(512-192, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod1(512-160, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod1(512-128, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod1(528-112, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod1(832-256, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod1(832-256, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod2(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod2, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        # p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod2(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod2(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod2(256-128, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod2(480-192, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod2(512-208, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod2(512-224, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod2(512-256, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod2(528-288, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod2(832-320, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod2(832-320, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod3(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod3, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        # self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        # self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod3(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod3(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod3(256-32, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod3(480-96, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod3(512-48, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod3(512-64, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod3(512-64, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod3(528-64, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod3(832-128, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod3(832-128, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-128, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod4(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod4, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3), dim=1)\n",
    "    \n",
    "def Googlenet_mod4(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod4(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod4(256-32, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod4(480-64, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod4(512-64, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod4(512-64, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod4(512-64, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod4(528-64, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod4(832-128, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod4(832-128, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-128, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod5(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod5, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        # self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        # self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat([p2], dim=1)\n",
    "    \n",
    "def Googlenet_mod5(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod5(192, (96, 128)),\n",
    "                   Inception_mod5(128, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod5(192, (96, 208)),\n",
    "                   Inception_mod5(208, (112, 224)),\n",
    "                   Inception_mod5(224, (128, 256)),\n",
    "                   Inception_mod5(256, (144, 288)),\n",
    "                   Inception_mod5(288, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod5(320, (160, 320)),\n",
    "                   Inception_mod5(320, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version6 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod6(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod6, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3), dim=1)\n",
    "    \n",
    "def Googlenet_mod6(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod6(192, (96, 128)),\n",
    "                   Inception_mod6(128*2, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod6(192*2, (96, 208)),\n",
    "                   Inception_mod6(208*2, (112, 224)),\n",
    "                   Inception_mod6(224*2, (128, 256)),\n",
    "                   Inception_mod6(256*2, (144, 288)),\n",
    "                   Inception_mod6(288*2, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod6(320*2, (160, 320)),\n",
    "                   Inception_mod6(320*2, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*2, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod7(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod7, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod7(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod7(192, (96, 128)),\n",
    "                   Inception_mod7(128*3, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod7(192*3, (96, 208)),\n",
    "                   Inception_mod7(208*3, (112, 224)),\n",
    "                   Inception_mod7(224*3, (128, 256)),\n",
    "                   Inception_mod7(256*3, (144, 288)),\n",
    "                   Inception_mod7(288*3, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod7(320*3, (160, 320)),\n",
    "                   Inception_mod7(320*3, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*3, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod8(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod8, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p1_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_2(F.relu(self.p1_1(x))))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod8(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod8(192, (96, 128)),\n",
    "                   Inception_mod8(128*4, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod8(192*4, (96, 208)),\n",
    "                   Inception_mod8(208*4, (112, 224)),\n",
    "                   Inception_mod8(224*4, (128, 256)),\n",
    "                   Inception_mod8(256*4, (144, 288)),\n",
    "                   Inception_mod8(288*4, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod8(320*4, (160, 320)),\n",
    "                   Inception_mod8(320*4, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*4, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version9 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Inception_mod8(nn.Module):\n",
    "#     # c1--c4是每条路径的输出通道数\n",
    "#     def __init__(self, in_channels, c2, **kwargs):\n",
    "#         super(Inception_mod8, self).__init__(**kwargs)\n",
    "#         # 线路1，单1x1卷积层\n",
    "#         self.p1_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p1_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路2，1x1卷积层后接3x3卷积层\n",
    "#         self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路3，1x1卷积层后接5x5卷积层\n",
    "#         self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "#         self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         p1 = F.relu(self.p1_2(F.relu(self.p1_1(x))))\n",
    "#         p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "#         p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "#         p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "#         # 在通道维度上连结输出\n",
    "#         return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod9(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b3 = nn.Sequential(Inception_mod8(192, (96, 128)),\n",
    "    #                Inception_mod8(128*4, (128, 192)),\n",
    "    #                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b4 = nn.Sequential(Inception_mod8(192*4, (96, 208)),\n",
    "    #                Inception_mod8(208*4, (112, 224)),\n",
    "    #                Inception_mod8(224*4, (128, 256)),\n",
    "    #                Inception_mod8(256*4, (144, 288)),\n",
    "    #                Inception_mod8(288*4, (160, 320)),\n",
    "    #                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b5 = nn.Sequential(Inception_mod8(320*4, (160, 320)),\n",
    "    #                Inception_mod8(320*4, (192, 384)),\n",
    "    #                nn.AdaptiveAvgPool2d((1,1)),\n",
    "    #                nn.Flatten())\n",
    "    \n",
    "    b5 = nn.Sequential(\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b5, nn.Linear(192, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MobileNetV1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            DepthwiseSeparableConv(32, 64, 1),\n",
    "            DepthwiseSeparableConv(64, 128, 2),\n",
    "            DepthwiseSeparableConv(128, 128, 1),\n",
    "            DepthwiseSeparableConv(128, 256, 2),\n",
    "            DepthwiseSeparableConv(256, 256, 1),\n",
    "            DepthwiseSeparableConv(256, 512, 2),\n",
    "\n",
    "            # Typically, 5 Depthwise Separable Convolutions are repeated here, each with stride 1\n",
    "            *[DepthwiseSeparableConv(512, 512, 1) for _ in range(5)],\n",
    "\n",
    "            DepthwiseSeparableConv(512, 1024, 2),\n",
    "            DepthwiseSeparableConv(1024, 1024, 1),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, output_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_residual = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # expand\n",
    "            nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # depthwise\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # project\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.layers(x)\n",
    "        else:\n",
    "            return self.layers(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.inverted_residual_blocks = nn.Sequential(\n",
    "            InvertedResidual(32, 16, 1, 1),\n",
    "            InvertedResidual(16, 24, 2, 6),\n",
    "            InvertedResidual(24, 24, 1, 6),\n",
    "            InvertedResidual(24, 32, 2, 6),\n",
    "            InvertedResidual(32, 32, 1, 6),\n",
    "            InvertedResidual(32, 32, 1, 6),\n",
    "            InvertedResidual(32, 64, 2, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 96, 1, 6),\n",
    "            InvertedResidual(96, 96, 1, 6),\n",
    "            InvertedResidual(96, 96, 1, 6),\n",
    "            InvertedResidual(96, 160, 2, 6),\n",
    "            InvertedResidual(160, 160, 1, 6),\n",
    "            InvertedResidual(160, 160, 1, 6),\n",
    "            InvertedResidual(160, 320, 1, 6)\n",
    "        )\n",
    "\n",
    "        self.last_layers = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, output_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = self.inverted_residual_blocks(x)\n",
    "        x = self.last_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "print('The device is:', device)\n",
    "\n",
    "# check if mps on macbook is availabel\n",
    "# print(torch.backends.mps.is_available())  # 检查 MPS 是否可用\n",
    "# print(torch.backends.mps.is_built())      # 检查 MPS 是否已编译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to store all the parameters and the number of MACs, be careful of the different datasets\n",
    "# to avoid the error of the number of input channels and any other mistake, try to use different dictionaries to store each dataset\n",
    "# create different empty dictionary\n",
    "macs_f = {}\n",
    "paras_f = {}\n",
    "macs_c100 = {}\n",
    "paras_c100 = {}\n",
    "macs_c10 = {}\n",
    "paras_c10 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usea function to call the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for all the models to run\n",
    "# image channel for fashion mnist \n",
    "channel_f = 1\n",
    "# image channel for cifar100 and cifar10\n",
    "channel_c = 3\n",
    "\n",
    "# number of labels for fashion mnist\n",
    "num_labels_f = 10\n",
    "# number of labels for cifar100 \n",
    "num_labels_c100 = 100\n",
    "# number of labels for cifar10\n",
    "num_labels_c10 = 10\n",
    "\n",
    "def get_model_info(model, img_channel, num_labels):\n",
    "    model_ini = model.__name__\n",
    "    print(f'The model name is {model_ini}')\n",
    "\n",
    "    net = model(img_channel, num_labels)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model name is alexnet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model name is alexnet\n",
      "The model name is alexnet\n",
      "The model name is resnet18\n",
      "The model name is resnet18\n",
      "The model name is resnet18\n",
      "The model name is resnet34\n",
      "The model name is resnet34\n",
      "The model name is resnet34\n",
      "The model name is resnet50\n",
      "The model name is resnet50\n",
      "The model name is resnet50\n",
      "The model name is vgg11\n",
      "The model name is vgg11\n",
      "The model name is vgg11\n",
      "The model name is vgg13\n",
      "The model name is vgg13\n",
      "The model name is vgg13\n",
      "The model name is vgg16\n",
      "The model name is vgg16\n",
      "The model name is vgg16\n",
      "The model name is Googlenet\n",
      "The model name is Googlenet\n",
      "The model name is Googlenet\n",
      "The model name is Googlenet_mod1\n",
      "The model name is Googlenet_mod1\n",
      "The model name is Googlenet_mod1\n",
      "The model name is Googlenet_mod2\n",
      "The model name is Googlenet_mod2\n",
      "The model name is Googlenet_mod2\n",
      "The model name is Googlenet_mod3\n",
      "The model name is Googlenet_mod3\n",
      "The model name is Googlenet_mod3\n",
      "The model name is Googlenet_mod4\n",
      "The model name is Googlenet_mod4\n",
      "The model name is Googlenet_mod4\n",
      "The model name is Googlenet_mod5\n",
      "The model name is Googlenet_mod5\n",
      "The model name is Googlenet_mod5\n",
      "The model name is Googlenet_mod6\n",
      "The model name is Googlenet_mod6\n",
      "The model name is Googlenet_mod6\n",
      "The model name is Googlenet_mod7\n",
      "The model name is Googlenet_mod7\n",
      "The model name is Googlenet_mod7\n",
      "The model name is Googlenet_mod8\n",
      "The model name is Googlenet_mod8\n",
      "The model name is Googlenet_mod8\n",
      "The model name is Googlenet_mod9\n",
      "The model name is Googlenet_mod9\n",
      "The model name is Googlenet_mod9\n",
      "The model name is MobileNet\n",
      "The model name is MobileNet\n",
      "The model name is MobileNet\n",
      "The model name is MobileNetV2\n",
      "The model name is MobileNetV2\n",
      "The model name is MobileNetV2\n"
     ]
    }
   ],
   "source": [
    "# alexnet\n",
    "alexnet_f= get_model_info(alexnet, channel_f, num_labels_f)\n",
    "alexnet_c100 = get_model_info(alexnet, channel_c, num_labels_c100)\n",
    "alexnet_c10 = get_model_info(alexnet, channel_c, num_labels_c10)\n",
    "\n",
    "# ResNet\n",
    "# resnet18\n",
    "resnet18_f = get_model_info(resnet18, channel_f, num_labels_f)\n",
    "resnet18_c100 = get_model_info(resnet18, channel_c, num_labels_c100)\n",
    "resnet18_c10 = get_model_info(resnet18, channel_c, num_labels_c10)\n",
    "# resnet34\n",
    "resnet34_f = get_model_info(resnet34, channel_f, num_labels_f)\n",
    "resnet34_c100 = get_model_info(resnet34, channel_c, num_labels_c100)\n",
    "resnet34_c10 = get_model_info(resnet34, channel_c, num_labels_c10)\n",
    "# resnet50\n",
    "resnet50_f = get_model_info(resnet50, channel_f, num_labels_f)\n",
    "resnet50_c100 = get_model_info(resnet50, channel_c, num_labels_c100)\n",
    "resnet50_c10 = get_model_info(resnet50, channel_c, num_labels_c10)\n",
    "\n",
    "# VGG\n",
    "# vgg11\n",
    "vgg11_f = get_model_info(vgg11, channel_f, num_labels_f)\n",
    "vgg11_c100 = get_model_info(vgg11, channel_c, num_labels_c100)\n",
    "vgg11_c10 = get_model_info(vgg11, channel_c, num_labels_c10)\n",
    "# vgg13\n",
    "vgg13_f = get_model_info(vgg13, channel_f, num_labels_f)\n",
    "vgg13_c100 = get_model_info(vgg13, channel_c, num_labels_c100)\n",
    "vgg13_c10 = get_model_info(vgg13, channel_c, num_labels_c10)\n",
    "# vgg16 \n",
    "vgg16_f = get_model_info(vgg16, channel_f, num_labels_f)\n",
    "vgg16_c100 = get_model_info(vgg16, channel_c, num_labels_c100)\n",
    "vgg16_c10 = get_model_info(vgg16, channel_c, num_labels_c10)\n",
    "\n",
    "# GoogLeNet\n",
    "# googlenet_original\n",
    "googlenet_org_f = get_model_info(Googlenet, channel_f, num_labels_f)\n",
    "googlenet_org_c100 = get_model_info(Googlenet, channel_c, num_labels_c100)\n",
    "googlenet_org_c10 = get_model_info(Googlenet, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod1\n",
    "googlenet_mod1_f = get_model_info(Googlenet_mod1, channel_f, num_labels_f)\n",
    "googlenet_mod1_c100 = get_model_info(Googlenet_mod1, channel_c, num_labels_c100)\n",
    "googlenet_mod1_c10 = get_model_info(Googlenet_mod1, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod2\n",
    "googlenet_mod2_f = get_model_info(Googlenet_mod2, channel_f, num_labels_f)\n",
    "googlenet_mod2_c100 = get_model_info(Googlenet_mod2, channel_c, num_labels_c100)\n",
    "googlenet_mod2_c10 = get_model_info(Googlenet_mod2, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod3\n",
    "googlenet_mod3_f = get_model_info(Googlenet_mod3, channel_f, num_labels_f)\n",
    "googlenet_mod3_c100 = get_model_info(Googlenet_mod3, channel_c, num_labels_c100)\n",
    "googlenet_mod3_c10 = get_model_info(Googlenet_mod3, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod4\n",
    "googlenet_mod4_f = get_model_info(Googlenet_mod4, channel_f, num_labels_f)\n",
    "googlenet_mod4_c100 = get_model_info(Googlenet_mod4, channel_c, num_labels_c100)\n",
    "googlenet_mod4_c10 = get_model_info(Googlenet_mod4, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod5\n",
    "googlenet_mod5_f = get_model_info(Googlenet_mod5, channel_f, num_labels_f)\n",
    "googlenet_mod5_c100 = get_model_info(Googlenet_mod5, channel_c, num_labels_c100)\n",
    "googlenet_mod5_c10 = get_model_info(Googlenet_mod5, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod6\n",
    "googlenet_mod6_f = get_model_info(Googlenet_mod6, channel_f, num_labels_f)\n",
    "googlenet_mod6_c100 = get_model_info(Googlenet_mod6, channel_c, num_labels_c100)\n",
    "googlenet_mod6_c10 = get_model_info(Googlenet_mod6, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod7\n",
    "googlenet_mod7_f = get_model_info(Googlenet_mod7, channel_f, num_labels_f)\n",
    "googlenet_mod7_c100 = get_model_info(Googlenet_mod7, channel_c, num_labels_c100)\n",
    "googlenet_mod7_c10 = get_model_info(Googlenet_mod7, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod8\n",
    "googlenet_mod8_f = get_model_info(Googlenet_mod8, channel_f, num_labels_f)\n",
    "googlenet_mod8_c100 = get_model_info(Googlenet_mod8, channel_c, num_labels_c100)\n",
    "googlenet_mod8_c10 = get_model_info(Googlenet_mod8, channel_c, num_labels_c10)\n",
    "\n",
    "# googlenet_mod9\n",
    "googlenet_mod9_f = get_model_info(Googlenet_mod9, channel_f, num_labels_f)\n",
    "googlenet_mod9_c100 = get_model_info(Googlenet_mod9, channel_c, num_labels_c100)\n",
    "googlenet_mod9_c10 = get_model_info(Googlenet_mod9, channel_c, num_labels_c10)\n",
    "\n",
    "# MobileNet\n",
    "# mobilenetv1\n",
    "mobilenetv1_f = get_model_info(MobileNet, channel_f, num_labels_f)\n",
    "mobilenetv1_c100 = get_model_info(MobileNet, channel_c, num_labels_c100)\n",
    "mobilenetv1_c10 = get_model_info(MobileNet, channel_c, num_labels_c10)\n",
    "\n",
    "# mobilenetv2\n",
    "mobilenetv2_f = get_model_info(MobileNetV2, channel_f, num_labels_f)\n",
    "mobilenetv2_c100 = get_model_info(MobileNetV2, channel_c, num_labels_c100)\n",
    "mobilenetv2_c10 = get_model_info(MobileNetV2, channel_c, num_labels_c10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'vgg11', 'vgg13', 'vgg16', 'resnet18', 'resnet34', 'resnet50', 'googlenet_origin', 'googlenet_mod1', 'googlenet_mod2', 'googlenet_mod3', 'googlenet_mod4', 'googlenet_mod5', 'googlenet_mod6', 'googlenet_mod7', 'googlenet_mod8', 'googlenet_mod9', 'mobilenetv1_path', 'mobilenetv2_path']\n"
     ]
    }
   ],
   "source": [
    "print(models_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model list according to models_name order\n",
    "models_f_list = [alexnet_f, \n",
    "                vgg11_f, vgg13_f, vgg16_f,\n",
    "                resnet18_f, resnet34_f, resnet50_f,\n",
    "                googlenet_org_f, googlenet_mod1_f, googlenet_mod2_f, googlenet_mod3_f, \n",
    "                googlenet_mod4_f, googlenet_mod5_f, googlenet_mod6_f, googlenet_mod7_f,\n",
    "                googlenet_mod8_f, googlenet_mod9_f,\n",
    "                mobilenetv1_f, mobilenetv2_f]\n",
    "\n",
    "models_c100_list = [alexnet_c100,\n",
    "                vgg11_c100, vgg13_c100, vgg16_c100,\n",
    "                resnet18_c100, resnet34_c100, resnet50_c100,\n",
    "                googlenet_org_c100, googlenet_mod1_c100, googlenet_mod2_c100, googlenet_mod3_c100, \n",
    "                googlenet_mod4_c100, googlenet_mod5_c100, googlenet_mod6_c100, googlenet_mod7_c100,\n",
    "                googlenet_mod8_c100, googlenet_mod9_c100,\n",
    "                mobilenetv1_c100, mobilenetv2_c100]\n",
    "\n",
    "models_c10_list = [alexnet_c10,\n",
    "                vgg11_c10, vgg13_c10, vgg16_c10,\n",
    "                resnet18_c10, resnet34_c10, resnet50_c10,\n",
    "                googlenet_org_c10, googlenet_mod1_c10, googlenet_mod2_c10, googlenet_mod3_c10, \n",
    "                googlenet_mod4_c10, googlenet_mod5_c10, googlenet_mod6_c10, googlenet_mod7_c10,\n",
    "                googlenet_mod8_c10, googlenet_mod9_c10,\n",
    "                mobilenetv1_c10, mobilenetv2_c10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the output size of each layers after the picture is passed through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv2d_0', 'ReLU_1', 'MaxPool2d_2', 'Conv2d_3', 'ReLU_4', 'MaxPool2d_5', 'Conv2d_6', 'ReLU_7', 'Conv2d_8', 'ReLU_9', 'Conv2d_10', 'ReLU_11', 'MaxPool2d_12', 'AdaptiveAvgPool2d_13', 'Flatten_14', 'Linear_15', 'ReLU_16', 'Dropout_17', 'Linear_18', 'ReLU_19', 'Dropout_20', 'Linear_21']\n"
     ]
    }
   ],
   "source": [
    "X_f = torch.randn(size=(1, 1, 224, 224), dtype=torch.float32) # fashion mnist\n",
    "\n",
    "# for layer in alexnet_f:\n",
    "#     X_f=layer(X_f)\n",
    "#     print(layer.__class__.__name__,'output shape:\\t',X_f.shape)\n",
    "layerlist_alexnet = []\n",
    "layer_count = 0\n",
    "for layer in alexnet_f:\n",
    "    name = layer.__class__.__name__\n",
    "    name = name + '_' + str(layer_count)\n",
    "    layer_count += 1\n",
    "    layerlist_alexnet.append(name)\n",
    "\n",
    "\n",
    "print(layerlist_alexnet)\n",
    "            \n",
    "    \n",
    "\n",
    "# layerlist_resnet18 = []\n",
    "# for layer in resnet18_f:\n",
    "#     name = layer.__class__.__name__\n",
    "#     if name == 'Sequential':\n",
    "#         for l in layer:\n",
    "#             inner_name = l.__class__.__name__\n",
    "#             if inner_name == 'Residual18':\n",
    "#                 layerlist_resnet18.append('residual_'+l.conv1.__class__.__name__+'_1')\n",
    "#                 layerlist_resnet18.append('residual_'+l.bn1.__class__.__name__+'_1')\n",
    "#                 layerlist_resnet18.append('residual_'+l.relu1.__class__.__name__+'_1')\n",
    "#                 layerlist_resnet18.append('residual_'+l.conv2.__class__.__name__+'_2')   \n",
    "#                 layerlist_resnet18.append('residual_'+l.bn2.__class__.__name__+'_2')\n",
    "#                 if l.conv3 is not None:  # 确保 conv3 存在\n",
    "#                     layerlist_resnet18.append('residual_' + l.conv3.__class__.__name__+'_3')      \n",
    "#                 layerlist_resnet18.append('residual_'+l.relu2.__class__.__name__+'_2')\n",
    "#             else:\n",
    "#                 layerlist_resnet18.append(inner_name)\n",
    "#     else:\n",
    "#         layerlist_resnet18.append(name)\n",
    "\n",
    "# print(layerlist_resnet18)\n",
    "\n",
    "                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the datas:  \n",
    "    1. FashionMNIST\n",
    "    2. CIFAR100\n",
    "    3. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# fashion mnist\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集, 然后将其加载到内存中\n",
    "\n",
    "    Defined in :numref:`sec_fashion_mnist`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "def load_data_cifar100(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    \n",
    "def load_data_cifar10(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [128]\n",
    "epochs = [5]\n",
    "rounds = 1\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using pynvml to get the GPU power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvml_sampling_thread(handle, filename, stop_event, sampling_interval):\n",
    "    \"\"\"\n",
    "    在单独的线程中定期调用 NVML, 获取功耗数据并存储到 data_queue 中。\n",
    "    参数：\n",
    "    - handle: nvmlDeviceGetHandleByIndex(0) 得到的 GPU 句柄\n",
    "    - data_queue: 用于存放 (timestamp, power_in_watts) 数据的队列\n",
    "    - stop_event: 当此事件被设置时，线程应结束循环\n",
    "    - sampling_interval: 采样间隔（秒）\n",
    "    \"\"\"\n",
    "    with open(filename/'energy_consumption_file.csv', 'a') as f:  # 追加模式\n",
    "        # 写入列名\n",
    "        f.write(\"timestamp,power_in_watts\\n\")\n",
    "        while not stop_event.is_set():\n",
    "            try:\n",
    "                # 采集功率和时间戳\n",
    "                current_time = time.time()\n",
    "                current_power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # 转换 mW -> W\n",
    "                # 写入文件\n",
    "                f.write(f\"{current_time},{current_power}\\n\")\n",
    "                # 等待下一次采样\n",
    "                time.sleep(sampling_interval)\n",
    "            except pynvml.NVMLError as e:\n",
    "                print(f\"NVML Error: {e}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the interval of the power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_power_over_interval(samples, start_time, end_time):\n",
    "    # 假定 samples是按时间升序排序的 (t, p)\n",
    "    # 若未排序，请先排序:\n",
    "    # samples = sorted(samples, key=lambda x: x[0])\n",
    "    \n",
    "    def interpolate(samples, target_time):\n",
    "        # 在 samples 中找到 target_time 左右最近的两个点，并进行线性插值\n",
    "        # 若 target_time 恰好等于某个样本点时间，直接返回该点功率\n",
    "        # 若无法找到两侧点（如 target_time在样本时间轴外），根据情况返回None或边界点\n",
    "        n = len(samples)\n",
    "        if n == 0:\n",
    "            return None\n",
    "        # 若 target_time 小于第一个样本点时间，无法向左插值，这里直接返回第一个点的功率值(或None)\n",
    "        if target_time <= samples[0][0]:\n",
    "            # 简化处理：返回最早样本点的功率（或None）\n",
    "            return samples[0][1]\n",
    "        # 若 target_time 大于最后一个样本点时间，无法向右插值，返回最后一个点的功率（或None）\n",
    "        if target_time >= samples[-1][0]:\n",
    "            return samples[-1][1]\n",
    "\n",
    "        # 否则，在中间插值\n",
    "        # 使用二分查找快速定位\n",
    "        import bisect\n",
    "        times = [t for t, _ in samples]\n",
    "        pos = bisect.bisect_left(times, target_time)\n",
    "        # pos是使times保持有序插入target_time的位置\n",
    "        # 因为target_time不在已有样本点中，pos不会越界且pos>0且pos<n\n",
    "        t1, p1 = samples[pos-1]\n",
    "        t2, p2 = samples[pos]\n",
    "        # 线性插值： p = p1 + (p2 - p1)*((target_time - t1)/(t2 - t1))\n",
    "        ratio = (target_time - t1) / (t2 - t1)\n",
    "        p = p1 + (p2 - p1)*ratio\n",
    "        return p\n",
    "\n",
    "    # 从原始 samples 中筛选出位于[start_time, end_time]内的点\n",
    "    filtered = [(t, p) for t, p in samples if start_time <= t <= end_time]\n",
    "\n",
    "    # 如果不足2个点，则尝试使用插值\n",
    "    if len(filtered) < 2:\n",
    "        # 无论如何都需要在边界处插值出两个点(起码start和end)\n",
    "        start_power = interpolate(samples, start_time)\n",
    "        end_power = interpolate(samples, end_time)\n",
    "\n",
    "        # 如果从样本中无法插值出任何有意义的点（比如samples为空或无法插值），返回0.0\n",
    "        if start_power is None or end_power is None:\n",
    "            return 0.0\n",
    "\n",
    "        # 将插值的边界点加入到 filtered\n",
    "        # 注意：如果filtered中有一个点在区间内，我们也需要确保边界有两点以上\n",
    "        # 例如filtered只有一个点在中间，则需要在start和end插值点全部加入。\n",
    "        # 若filtered为空，则只用start/end两点插值点求积分\n",
    "        new_filtered = [(start_time, start_power)] + filtered + [(end_time, end_power)]\n",
    "        # 确保按时间排序\n",
    "        new_filtered.sort(key=lambda x: x[0])\n",
    "        filtered = new_filtered\n",
    "\n",
    "    # 正常积分计算\n",
    "    if len(filtered) < 2:\n",
    "        # 经过插值仍不够，返回0\n",
    "        return 0.0\n",
    "\n",
    "    total_energy = 0.0\n",
    "    for i in range(len(filtered)-1):\n",
    "        t1, p1 = filtered[i]\n",
    "        t2, p2 = filtered[i+1]\n",
    "        dt = t2 - t1\n",
    "        avg_p = (p1 + p2)/2.0\n",
    "        total_energy += avg_p * dt\n",
    "\n",
    "    return total_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(net, train_iter, test_iter, num_epochs, lr, device, filename, sampling_interval):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    # print(f'The name of the layers are: {alexlayer}')\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # save all epochs time data using list\n",
    "    to_device_intervals_total = []\n",
    "    forward_intervals_total = []\n",
    "    loss_intervals_total = []\n",
    "    backward_intervals_total = []\n",
    "    optimize_intervals_total = []\n",
    "    test_intervals_total = []\n",
    "\n",
    "    # create a dictionary to store each layer time period data in each batch\n",
    "    layer_time_alexnet = {}\n",
    "\n",
    "    # create a list to store the epoch time data\n",
    "    epoch_intervals_total = []\n",
    "    \n",
    "    # 初始化NVML和采样线程\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    power_data_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "    sampler_thread = threading.Thread(target=nvml_sampling_thread, args=(handle, filename, stop_event, sampling_interval))\n",
    "    sampler_thread.start()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        layer_time_alexnet[str(epoch)] = {}\n",
    "        print('The epoch is:', epoch+1)\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "        to_device_intervals_epoch = []  # 用来记录本epoch每个batch的to_device时间段\n",
    "        forward_intervals_epoch = []  # 用来记录本epoch每个batch的forward时间段\n",
    "        loss_intervals_epoch = []  # 用来记录本epoch每个batch的loss时间段\n",
    "        backward_intervals_epoch = [] \n",
    "        optimize_intervals_epoch = []\n",
    "        test_intervals_epoch = []   \n",
    "        epoch_intervals_epoch = []  # 用来记录本epoch的时间段\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            print('The batch is:', i+1)\n",
    "            layer_time_alexnet[str(epoch)][str(i)] = {}\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # 记录to_device前后的时间戳\n",
    "            start_ttd_time = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            end_ttd_time = time.time()\n",
    "            to_device_intervals_epoch.append((start_ttd_time, end_ttd_time))\n",
    "\n",
    "            # forward\n",
    "            start_forward_time = time.time()\n",
    "            y_hat = X\n",
    "            layer_count = 0\n",
    "            for layer in net:\n",
    "                name = layer.__class__.__name__ # get the name of the layer\n",
    "                name = name + '_' + str(layer_count)\n",
    "                layer_count += 1\n",
    "                if name in layerlist_alexnet:\n",
    "                    start_layer_time = time.time()\n",
    "                    y_hat = layer(y_hat)\n",
    "                    torch.cuda.synchronize()\n",
    "                    end_layer_time = time.time()\n",
    "                    layer_time_alexnet[str(epoch)][str(i)][name] = (start_layer_time, end_layer_time)\n",
    "\n",
    "            # y_hat = net(X)\n",
    "            # torch.cuda.synchronize()\n",
    "            end_forward_time = time.time()\n",
    "            forward_intervals_epoch.append((start_forward_time, end_forward_time))\n",
    "\n",
    "            # loss\n",
    "            start_loss_time = time.time()\n",
    "            l = loss_fn(y_hat, y)\n",
    "            torch.cuda.synchronize()\n",
    "            end_loss_time = time.time()\n",
    "            loss_intervals_epoch.append((start_loss_time, end_loss_time))\n",
    "\n",
    "            # backward\n",
    "            start_backward_time = time.time()\n",
    "            l.backward()\n",
    "            torch.cuda.synchronize()\n",
    "            end_backward_time = time.time()\n",
    "            backward_intervals_epoch.append((start_backward_time, end_backward_time))\n",
    "\n",
    "            # optimize\n",
    "            start_optimize_time = time.time()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            end_optimize_time = time.time()\n",
    "            optimize_intervals_epoch.append((start_optimize_time, end_optimize_time))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "        start_test_time = time.time()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        end_test_time = time.time()\n",
    "        print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "        test_intervals_epoch.append((start_test_time, end_test_time))\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_intervals_epoch.append((epoch_start_time, epoch_end_time))\n",
    "\n",
    "        # data need to be saved\n",
    "        # add the intervals_epoch to intervals_total\n",
    "        to_device_intervals_total.append(to_device_intervals_epoch)\n",
    "        forward_intervals_total.append(forward_intervals_epoch)\n",
    "        loss_intervals_total.append(loss_intervals_epoch)\n",
    "        backward_intervals_total.append(backward_intervals_epoch)\n",
    "        optimize_intervals_total.append(optimize_intervals_epoch)\n",
    "        test_intervals_total.append(test_intervals_epoch)\n",
    "        epoch_intervals_total.append(epoch_intervals_epoch)\n",
    "\n",
    "\n",
    "    # 训练结束后关闭线程\n",
    "    stop_event.set()\n",
    "    sampler_thread.join()\n",
    "\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    return to_device_intervals_total, forward_intervals_total, loss_intervals_total, backward_intervals_total, optimize_intervals_total, test_intervals_total, epoch_intervals_total, layer_time_alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set a function to train the model with FashionMNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(main_folder, batch_size, num_epochs, round, lr, device, sample_interval, net):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    # epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    sr_number = int(sample_interval*1000)\n",
    "    epoch_batch_folder = f'E{num_epochs}_B{batch_size}_R{round}_SR{sr_number}_layer'\n",
    "\n",
    "    data_dir = 'fashion_mnist'\n",
    "    # data_dir = 'cifar100'\n",
    "    # data_dir = 'cifar10'\n",
    "\n",
    "    # the folder path is main_folder/epoch_batch_folder\n",
    "    folder_path = main_folder/epoch_batch_folder/data_dir\n",
    "    print(f'The folder path is: {folder_path}')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "    # show the shape of the data\n",
    "    list_of_i = []\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        if i < 3:\n",
    "            print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "        else:\n",
    "            pass\n",
    "        list_of_i.append(i)\n",
    "    print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "    to_device_intervals_total, forward_intervals_total, loss_intervals_total,\\\n",
    "          backward_intervals_total, optimize_intervals_total, test_intervals_total, epoch_intervals_total, layer_time_alexnet = train_func(net, train_iter, test_iter, num_epochs, lr, device, folder_path, sample_interval)\n",
    "\n",
    "    # transfer the data to the numpy array\n",
    "    to_device_data = np.array(to_device_intervals_total)\n",
    "    forward_time = np.array(forward_intervals_total)\n",
    "    loss_time = np.array(loss_intervals_total)\n",
    "    backward_time = np.array(backward_intervals_total)\n",
    "    optimize_time = np.array(optimize_intervals_total)\n",
    "    test_time = np.array(test_intervals_total)\n",
    "    epoch_time = np.array(epoch_intervals_total)\n",
    "\n",
    "    # save the layer_time_alexnet, the type is a dictionary, need to be saved as a csv file\n",
    "    # the first column is the epoch, the second column is the batch, the third column is the layer name, the fourth column is the start time, the fifth column is the end time\n",
    "    layer_time_alexnet_df = pd.DataFrame.from_dict(layer_time_alexnet)\n",
    "    \n",
    "\n",
    "\n",
    "    # print(layer_time_alexnet_df)\n",
    "\n",
    "    # save the data\n",
    "    np.save(folder_path/'to_device.npy', to_device_data, allow_pickle=True)\n",
    "    np.save(folder_path/'forward.npy', forward_time, allow_pickle=True)\n",
    "    np.save(folder_path/'loss.npy', loss_time, allow_pickle=True)\n",
    "    np.save(folder_path/'backward.npy', backward_time, allow_pickle=True)\n",
    "    np.save(folder_path/'optimize.npy', optimize_time, allow_pickle=True)\n",
    "    np.save(folder_path/'test.npy', test_time, allow_pickle=True)\n",
    "    np.save(folder_path/'epoch.npy', epoch_time, allow_pickle=True)\n",
    "    layer_time_alexnet_df.to_csv(folder_path/'layer_time.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is: /root/autodl-tmp/GreenAI/3080/ModelsData/alexnet\n",
      "文件存在。\n",
      "The epoch is set: 5, batch is set: 128, is in 1th running\n",
      "The folder path is: /root/autodl-tmp/GreenAI/3080/ModelsData/alexnet/E5_B128_R0_SR2_layer/fashion_mnist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the 0 batch of the train_iter is: torch.Size([128, 1, 224, 224])\n",
      "the shape of the 1 batch of the train_iter is: torch.Size([128, 1, 224, 224])\n",
      "the shape of the 2 batch of the train_iter is: torch.Size([128, 1, 224, 224])\n",
      "The number of batches is: (469,)\n",
      "training on cuda\n",
      "The epoch is: 1\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "The batch is: 236\n",
      "The batch is: 237\n",
      "The batch is: 238\n",
      "The batch is: 239\n",
      "The batch is: 240\n",
      "The batch is: 241\n",
      "The batch is: 242\n",
      "The batch is: 243\n",
      "The batch is: 244\n",
      "The batch is: 245\n",
      "The batch is: 246\n",
      "The batch is: 247\n",
      "The batch is: 248\n",
      "The batch is: 249\n",
      "The batch is: 250\n",
      "The batch is: 251\n",
      "The batch is: 252\n",
      "The batch is: 253\n",
      "The batch is: 254\n",
      "The batch is: 255\n",
      "The batch is: 256\n",
      "The batch is: 257\n",
      "The batch is: 258\n",
      "The batch is: 259\n",
      "The batch is: 260\n",
      "The batch is: 261\n",
      "The batch is: 262\n",
      "The batch is: 263\n",
      "The batch is: 264\n",
      "The batch is: 265\n",
      "The batch is: 266\n",
      "The batch is: 267\n",
      "The batch is: 268\n",
      "The batch is: 269\n",
      "The batch is: 270\n",
      "The batch is: 271\n",
      "The batch is: 272\n",
      "The batch is: 273\n",
      "The batch is: 274\n",
      "The batch is: 275\n",
      "The batch is: 276\n",
      "The batch is: 277\n",
      "The batch is: 278\n",
      "The batch is: 279\n",
      "The batch is: 280\n",
      "The batch is: 281\n",
      "The batch is: 282\n",
      "The batch is: 283\n",
      "The batch is: 284\n",
      "The batch is: 285\n",
      "The batch is: 286\n",
      "The batch is: 287\n",
      "The batch is: 288\n",
      "The batch is: 289\n",
      "The batch is: 290\n",
      "The batch is: 291\n",
      "The batch is: 292\n",
      "The batch is: 293\n",
      "The batch is: 294\n",
      "The batch is: 295\n",
      "The batch is: 296\n",
      "The batch is: 297\n",
      "The batch is: 298\n",
      "The batch is: 299\n",
      "The batch is: 300\n",
      "The batch is: 301\n",
      "The batch is: 302\n",
      "The batch is: 303\n",
      "The batch is: 304\n",
      "The batch is: 305\n",
      "The batch is: 306\n",
      "The batch is: 307\n",
      "The batch is: 308\n",
      "The batch is: 309\n",
      "The batch is: 310\n",
      "The batch is: 311\n",
      "The batch is: 312\n",
      "The batch is: 313\n",
      "The batch is: 314\n",
      "The batch is: 315\n",
      "The batch is: 316\n",
      "The batch is: 317\n",
      "The batch is: 318\n",
      "The batch is: 319\n",
      "The batch is: 320\n",
      "The batch is: 321\n",
      "The batch is: 322\n",
      "The batch is: 323\n",
      "The batch is: 324\n",
      "The batch is: 325\n",
      "The batch is: 326\n",
      "The batch is: 327\n",
      "The batch is: 328\n",
      "The batch is: 329\n",
      "The batch is: 330\n",
      "The batch is: 331\n",
      "The batch is: 332\n",
      "The batch is: 333\n",
      "The batch is: 334\n",
      "The batch is: 335\n",
      "The batch is: 336\n",
      "The batch is: 337\n",
      "The batch is: 338\n",
      "The batch is: 339\n",
      "The batch is: 340\n",
      "The batch is: 341\n",
      "The batch is: 342\n",
      "The batch is: 343\n",
      "The batch is: 344\n",
      "The batch is: 345\n",
      "The batch is: 346\n",
      "The batch is: 347\n",
      "The batch is: 348\n",
      "The batch is: 349\n",
      "The batch is: 350\n",
      "The batch is: 351\n",
      "The batch is: 352\n",
      "The batch is: 353\n",
      "The batch is: 354\n",
      "The batch is: 355\n",
      "The batch is: 356\n",
      "The batch is: 357\n",
      "The batch is: 358\n",
      "The batch is: 359\n",
      "The batch is: 360\n",
      "The batch is: 361\n",
      "The batch is: 362\n",
      "The batch is: 363\n",
      "The batch is: 364\n",
      "The batch is: 365\n",
      "The batch is: 366\n",
      "The batch is: 367\n",
      "The batch is: 368\n",
      "The batch is: 369\n",
      "The batch is: 370\n",
      "The batch is: 371\n",
      "The batch is: 372\n",
      "The batch is: 373\n",
      "The batch is: 374\n",
      "The batch is: 375\n",
      "The batch is: 376\n",
      "The batch is: 377\n",
      "The batch is: 378\n",
      "The batch is: 379\n",
      "The batch is: 380\n",
      "The batch is: 381\n",
      "The batch is: 382\n",
      "The batch is: 383\n",
      "The batch is: 384\n",
      "The batch is: 385\n",
      "The batch is: 386\n",
      "The batch is: 387\n",
      "The batch is: 388\n",
      "The batch is: 389\n",
      "The batch is: 390\n",
      "The batch is: 391\n",
      "The batch is: 392\n",
      "The batch is: 393\n",
      "The batch is: 394\n",
      "The batch is: 395\n",
      "The batch is: 396\n",
      "The batch is: 397\n",
      "The batch is: 398\n",
      "The batch is: 399\n",
      "The batch is: 400\n",
      "The batch is: 401\n",
      "The batch is: 402\n",
      "The batch is: 403\n",
      "The batch is: 404\n",
      "The batch is: 405\n",
      "The batch is: 406\n",
      "The batch is: 407\n",
      "The batch is: 408\n",
      "The batch is: 409\n",
      "The batch is: 410\n",
      "The batch is: 411\n",
      "The batch is: 412\n",
      "The batch is: 413\n",
      "The batch is: 414\n",
      "The batch is: 415\n",
      "The batch is: 416\n",
      "The batch is: 417\n",
      "The batch is: 418\n",
      "The batch is: 419\n",
      "The batch is: 420\n",
      "The batch is: 421\n",
      "The batch is: 422\n",
      "The batch is: 423\n",
      "The batch is: 424\n",
      "The batch is: 425\n",
      "The batch is: 426\n",
      "The batch is: 427\n",
      "The batch is: 428\n",
      "The batch is: 429\n",
      "The batch is: 430\n",
      "The batch is: 431\n",
      "The batch is: 432\n",
      "The batch is: 433\n",
      "The batch is: 434\n",
      "The batch is: 435\n",
      "The batch is: 436\n",
      "The batch is: 437\n",
      "The batch is: 438\n",
      "The batch is: 439\n",
      "The batch is: 440\n",
      "The batch is: 441\n",
      "The batch is: 442\n",
      "The batch is: 443\n",
      "The batch is: 444\n",
      "The batch is: 445\n",
      "The batch is: 446\n",
      "The batch is: 447\n",
      "The batch is: 448\n",
      "The batch is: 449\n",
      "The batch is: 450\n",
      "The batch is: 451\n",
      "The batch is: 452\n",
      "The batch is: 453\n",
      "The batch is: 454\n",
      "The batch is: 455\n",
      "The batch is: 456\n",
      "The batch is: 457\n",
      "The batch is: 458\n",
      "The batch is: 459\n",
      "The batch is: 460\n",
      "The batch is: 461\n",
      "The batch is: 462\n",
      "The batch is: 463\n",
      "The batch is: 464\n",
      "The batch is: 465\n",
      "The batch is: 466\n",
      "The batch is: 467\n",
      "The batch is: 468\n",
      "The batch is: 469\n",
      "train acc 0.499, test acc 0.750\n",
      "The epoch is: 2\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "The batch is: 236\n",
      "The batch is: 237\n",
      "The batch is: 238\n",
      "The batch is: 239\n",
      "The batch is: 240\n",
      "The batch is: 241\n",
      "The batch is: 242\n",
      "The batch is: 243\n",
      "The batch is: 244\n",
      "The batch is: 245\n",
      "The batch is: 246\n",
      "The batch is: 247\n",
      "The batch is: 248\n",
      "The batch is: 249\n",
      "The batch is: 250\n",
      "The batch is: 251\n",
      "The batch is: 252\n",
      "The batch is: 253\n",
      "The batch is: 254\n",
      "The batch is: 255\n",
      "The batch is: 256\n",
      "The batch is: 257\n",
      "The batch is: 258\n",
      "The batch is: 259\n",
      "The batch is: 260\n",
      "The batch is: 261\n",
      "The batch is: 262\n",
      "The batch is: 263\n",
      "The batch is: 264\n",
      "The batch is: 265\n",
      "The batch is: 266\n",
      "The batch is: 267\n",
      "The batch is: 268\n",
      "The batch is: 269\n",
      "The batch is: 270\n",
      "The batch is: 271\n",
      "The batch is: 272\n",
      "The batch is: 273\n",
      "The batch is: 274\n",
      "The batch is: 275\n",
      "The batch is: 276\n",
      "The batch is: 277\n",
      "The batch is: 278\n",
      "The batch is: 279\n",
      "The batch is: 280\n",
      "The batch is: 281\n",
      "The batch is: 282\n",
      "The batch is: 283\n",
      "The batch is: 284\n",
      "The batch is: 285\n",
      "The batch is: 286\n",
      "The batch is: 287\n",
      "The batch is: 288\n",
      "The batch is: 289\n",
      "The batch is: 290\n",
      "The batch is: 291\n",
      "The batch is: 292\n",
      "The batch is: 293\n",
      "The batch is: 294\n",
      "The batch is: 295\n",
      "The batch is: 296\n",
      "The batch is: 297\n",
      "The batch is: 298\n",
      "The batch is: 299\n",
      "The batch is: 300\n",
      "The batch is: 301\n",
      "The batch is: 302\n",
      "The batch is: 303\n",
      "The batch is: 304\n",
      "The batch is: 305\n",
      "The batch is: 306\n",
      "The batch is: 307\n",
      "The batch is: 308\n",
      "The batch is: 309\n",
      "The batch is: 310\n",
      "The batch is: 311\n",
      "The batch is: 312\n",
      "The batch is: 313\n",
      "The batch is: 314\n",
      "The batch is: 315\n",
      "The batch is: 316\n",
      "The batch is: 317\n",
      "The batch is: 318\n",
      "The batch is: 319\n",
      "The batch is: 320\n",
      "The batch is: 321\n",
      "The batch is: 322\n",
      "The batch is: 323\n",
      "The batch is: 324\n",
      "The batch is: 325\n",
      "The batch is: 326\n",
      "The batch is: 327\n",
      "The batch is: 328\n",
      "The batch is: 329\n",
      "The batch is: 330\n",
      "The batch is: 331\n",
      "The batch is: 332\n",
      "The batch is: 333\n",
      "The batch is: 334\n",
      "The batch is: 335\n",
      "The batch is: 336\n",
      "The batch is: 337\n",
      "The batch is: 338\n",
      "The batch is: 339\n",
      "The batch is: 340\n",
      "The batch is: 341\n",
      "The batch is: 342\n",
      "The batch is: 343\n",
      "The batch is: 344\n",
      "The batch is: 345\n",
      "The batch is: 346\n",
      "The batch is: 347\n",
      "The batch is: 348\n",
      "The batch is: 349\n",
      "The batch is: 350\n",
      "The batch is: 351\n",
      "The batch is: 352\n",
      "The batch is: 353\n",
      "The batch is: 354\n",
      "The batch is: 355\n",
      "The batch is: 356\n",
      "The batch is: 357\n",
      "The batch is: 358\n",
      "The batch is: 359\n",
      "The batch is: 360\n",
      "The batch is: 361\n",
      "The batch is: 362\n",
      "The batch is: 363\n",
      "The batch is: 364\n",
      "The batch is: 365\n",
      "The batch is: 366\n",
      "The batch is: 367\n",
      "The batch is: 368\n",
      "The batch is: 369\n",
      "The batch is: 370\n",
      "The batch is: 371\n",
      "The batch is: 372\n",
      "The batch is: 373\n",
      "The batch is: 374\n",
      "The batch is: 375\n",
      "The batch is: 376\n",
      "The batch is: 377\n",
      "The batch is: 378\n",
      "The batch is: 379\n",
      "The batch is: 380\n",
      "The batch is: 381\n",
      "The batch is: 382\n",
      "The batch is: 383\n",
      "The batch is: 384\n",
      "The batch is: 385\n",
      "The batch is: 386\n",
      "The batch is: 387\n",
      "The batch is: 388\n",
      "The batch is: 389\n",
      "The batch is: 390\n",
      "The batch is: 391\n",
      "The batch is: 392\n",
      "The batch is: 393\n",
      "The batch is: 394\n",
      "The batch is: 395\n",
      "The batch is: 396\n",
      "The batch is: 397\n",
      "The batch is: 398\n",
      "The batch is: 399\n",
      "The batch is: 400\n",
      "The batch is: 401\n",
      "The batch is: 402\n",
      "The batch is: 403\n",
      "The batch is: 404\n",
      "The batch is: 405\n",
      "The batch is: 406\n",
      "The batch is: 407\n",
      "The batch is: 408\n",
      "The batch is: 409\n",
      "The batch is: 410\n",
      "The batch is: 411\n",
      "The batch is: 412\n",
      "The batch is: 413\n",
      "The batch is: 414\n",
      "The batch is: 415\n",
      "The batch is: 416\n",
      "The batch is: 417\n",
      "The batch is: 418\n",
      "The batch is: 419\n",
      "The batch is: 420\n",
      "The batch is: 421\n",
      "The batch is: 422\n",
      "The batch is: 423\n",
      "The batch is: 424\n",
      "The batch is: 425\n",
      "The batch is: 426\n",
      "The batch is: 427\n",
      "The batch is: 428\n",
      "The batch is: 429\n",
      "The batch is: 430\n",
      "The batch is: 431\n",
      "The batch is: 432\n",
      "The batch is: 433\n",
      "The batch is: 434\n",
      "The batch is: 435\n",
      "The batch is: 436\n",
      "The batch is: 437\n",
      "The batch is: 438\n",
      "The batch is: 439\n",
      "The batch is: 440\n",
      "The batch is: 441\n",
      "The batch is: 442\n",
      "The batch is: 443\n",
      "The batch is: 444\n",
      "The batch is: 445\n",
      "The batch is: 446\n",
      "The batch is: 447\n",
      "The batch is: 448\n",
      "The batch is: 449\n",
      "The batch is: 450\n",
      "The batch is: 451\n",
      "The batch is: 452\n",
      "The batch is: 453\n",
      "The batch is: 454\n",
      "The batch is: 455\n",
      "The batch is: 456\n",
      "The batch is: 457\n",
      "The batch is: 458\n",
      "The batch is: 459\n",
      "The batch is: 460\n",
      "The batch is: 461\n",
      "The batch is: 462\n",
      "The batch is: 463\n",
      "The batch is: 464\n",
      "The batch is: 465\n",
      "The batch is: 466\n",
      "The batch is: 467\n",
      "The batch is: 468\n",
      "The batch is: 469\n",
      "train acc 0.764, test acc 0.802\n",
      "The epoch is: 3\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "The batch is: 236\n",
      "The batch is: 237\n",
      "The batch is: 238\n",
      "The batch is: 239\n",
      "The batch is: 240\n",
      "The batch is: 241\n",
      "The batch is: 242\n",
      "The batch is: 243\n",
      "The batch is: 244\n",
      "The batch is: 245\n",
      "The batch is: 246\n",
      "The batch is: 247\n",
      "The batch is: 248\n",
      "The batch is: 249\n",
      "The batch is: 250\n",
      "The batch is: 251\n",
      "The batch is: 252\n",
      "The batch is: 253\n",
      "The batch is: 254\n",
      "The batch is: 255\n",
      "The batch is: 256\n",
      "The batch is: 257\n",
      "The batch is: 258\n",
      "The batch is: 259\n",
      "The batch is: 260\n",
      "The batch is: 261\n",
      "The batch is: 262\n",
      "The batch is: 263\n",
      "The batch is: 264\n",
      "The batch is: 265\n",
      "The batch is: 266\n",
      "The batch is: 267\n",
      "The batch is: 268\n",
      "The batch is: 269\n",
      "The batch is: 270\n",
      "The batch is: 271\n",
      "The batch is: 272\n",
      "The batch is: 273\n",
      "The batch is: 274\n",
      "The batch is: 275\n",
      "The batch is: 276\n",
      "The batch is: 277\n",
      "The batch is: 278\n",
      "The batch is: 279\n",
      "The batch is: 280\n",
      "The batch is: 281\n",
      "The batch is: 282\n",
      "The batch is: 283\n",
      "The batch is: 284\n",
      "The batch is: 285\n",
      "The batch is: 286\n",
      "The batch is: 287\n",
      "The batch is: 288\n",
      "The batch is: 289\n",
      "The batch is: 290\n",
      "The batch is: 291\n",
      "The batch is: 292\n",
      "The batch is: 293\n",
      "The batch is: 294\n",
      "The batch is: 295\n",
      "The batch is: 296\n",
      "The batch is: 297\n",
      "The batch is: 298\n",
      "The batch is: 299\n",
      "The batch is: 300\n",
      "The batch is: 301\n",
      "The batch is: 302\n",
      "The batch is: 303\n",
      "The batch is: 304\n",
      "The batch is: 305\n",
      "The batch is: 306\n",
      "The batch is: 307\n",
      "The batch is: 308\n",
      "The batch is: 309\n",
      "The batch is: 310\n",
      "The batch is: 311\n",
      "The batch is: 312\n",
      "The batch is: 313\n",
      "The batch is: 314\n",
      "The batch is: 315\n",
      "The batch is: 316\n",
      "The batch is: 317\n",
      "The batch is: 318\n",
      "The batch is: 319\n",
      "The batch is: 320\n",
      "The batch is: 321\n",
      "The batch is: 322\n",
      "The batch is: 323\n",
      "The batch is: 324\n",
      "The batch is: 325\n",
      "The batch is: 326\n",
      "The batch is: 327\n",
      "The batch is: 328\n",
      "The batch is: 329\n",
      "The batch is: 330\n",
      "The batch is: 331\n",
      "The batch is: 332\n",
      "The batch is: 333\n",
      "The batch is: 334\n",
      "The batch is: 335\n",
      "The batch is: 336\n",
      "The batch is: 337\n",
      "The batch is: 338\n",
      "The batch is: 339\n",
      "The batch is: 340\n",
      "The batch is: 341\n",
      "The batch is: 342\n",
      "The batch is: 343\n",
      "The batch is: 344\n",
      "The batch is: 345\n",
      "The batch is: 346\n",
      "The batch is: 347\n",
      "The batch is: 348\n",
      "The batch is: 349\n",
      "The batch is: 350\n",
      "The batch is: 351\n",
      "The batch is: 352\n",
      "The batch is: 353\n",
      "The batch is: 354\n",
      "The batch is: 355\n",
      "The batch is: 356\n",
      "The batch is: 357\n",
      "The batch is: 358\n",
      "The batch is: 359\n",
      "The batch is: 360\n",
      "The batch is: 361\n",
      "The batch is: 362\n",
      "The batch is: 363\n",
      "The batch is: 364\n",
      "The batch is: 365\n",
      "The batch is: 366\n",
      "The batch is: 367\n",
      "The batch is: 368\n",
      "The batch is: 369\n",
      "The batch is: 370\n",
      "The batch is: 371\n",
      "The batch is: 372\n",
      "The batch is: 373\n",
      "The batch is: 374\n",
      "The batch is: 375\n",
      "The batch is: 376\n",
      "The batch is: 377\n",
      "The batch is: 378\n",
      "The batch is: 379\n",
      "The batch is: 380\n",
      "The batch is: 381\n",
      "The batch is: 382\n",
      "The batch is: 383\n",
      "The batch is: 384\n",
      "The batch is: 385\n",
      "The batch is: 386\n",
      "The batch is: 387\n",
      "The batch is: 388\n",
      "The batch is: 389\n",
      "The batch is: 390\n",
      "The batch is: 391\n",
      "The batch is: 392\n",
      "The batch is: 393\n",
      "The batch is: 394\n",
      "The batch is: 395\n",
      "The batch is: 396\n",
      "The batch is: 397\n",
      "The batch is: 398\n",
      "The batch is: 399\n",
      "The batch is: 400\n",
      "The batch is: 401\n",
      "The batch is: 402\n",
      "The batch is: 403\n",
      "The batch is: 404\n",
      "The batch is: 405\n",
      "The batch is: 406\n",
      "The batch is: 407\n",
      "The batch is: 408\n",
      "The batch is: 409\n",
      "The batch is: 410\n",
      "The batch is: 411\n",
      "The batch is: 412\n",
      "The batch is: 413\n",
      "The batch is: 414\n",
      "The batch is: 415\n",
      "The batch is: 416\n",
      "The batch is: 417\n",
      "The batch is: 418\n",
      "The batch is: 419\n",
      "The batch is: 420\n",
      "The batch is: 421\n",
      "The batch is: 422\n",
      "The batch is: 423\n",
      "The batch is: 424\n",
      "The batch is: 425\n",
      "The batch is: 426\n",
      "The batch is: 427\n",
      "The batch is: 428\n",
      "The batch is: 429\n",
      "The batch is: 430\n",
      "The batch is: 431\n",
      "The batch is: 432\n",
      "The batch is: 433\n",
      "The batch is: 434\n",
      "The batch is: 435\n",
      "The batch is: 436\n",
      "The batch is: 437\n",
      "The batch is: 438\n",
      "The batch is: 439\n",
      "The batch is: 440\n",
      "The batch is: 441\n",
      "The batch is: 442\n",
      "The batch is: 443\n",
      "The batch is: 444\n",
      "The batch is: 445\n",
      "The batch is: 446\n",
      "The batch is: 447\n",
      "The batch is: 448\n",
      "The batch is: 449\n",
      "The batch is: 450\n",
      "The batch is: 451\n",
      "The batch is: 452\n",
      "The batch is: 453\n",
      "The batch is: 454\n",
      "The batch is: 455\n",
      "The batch is: 456\n",
      "The batch is: 457\n",
      "The batch is: 458\n",
      "The batch is: 459\n",
      "The batch is: 460\n",
      "The batch is: 461\n",
      "The batch is: 462\n",
      "The batch is: 463\n",
      "The batch is: 464\n",
      "The batch is: 465\n",
      "The batch is: 466\n",
      "The batch is: 467\n",
      "The batch is: 468\n",
      "The batch is: 469\n",
      "train acc 0.808, test acc 0.820\n",
      "The epoch is: 4\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "The batch is: 236\n",
      "The batch is: 237\n",
      "The batch is: 238\n",
      "The batch is: 239\n",
      "The batch is: 240\n",
      "The batch is: 241\n",
      "The batch is: 242\n",
      "The batch is: 243\n",
      "The batch is: 244\n",
      "The batch is: 245\n",
      "The batch is: 246\n",
      "The batch is: 247\n",
      "The batch is: 248\n",
      "The batch is: 249\n",
      "The batch is: 250\n",
      "The batch is: 251\n",
      "The batch is: 252\n",
      "The batch is: 253\n",
      "The batch is: 254\n",
      "The batch is: 255\n",
      "The batch is: 256\n",
      "The batch is: 257\n",
      "The batch is: 258\n",
      "The batch is: 259\n",
      "The batch is: 260\n",
      "The batch is: 261\n",
      "The batch is: 262\n",
      "The batch is: 263\n",
      "The batch is: 264\n",
      "The batch is: 265\n",
      "The batch is: 266\n",
      "The batch is: 267\n",
      "The batch is: 268\n",
      "The batch is: 269\n",
      "The batch is: 270\n",
      "The batch is: 271\n",
      "The batch is: 272\n",
      "The batch is: 273\n",
      "The batch is: 274\n",
      "The batch is: 275\n",
      "The batch is: 276\n",
      "The batch is: 277\n",
      "The batch is: 278\n",
      "The batch is: 279\n",
      "The batch is: 280\n",
      "The batch is: 281\n",
      "The batch is: 282\n",
      "The batch is: 283\n",
      "The batch is: 284\n",
      "The batch is: 285\n",
      "The batch is: 286\n",
      "The batch is: 287\n",
      "The batch is: 288\n",
      "The batch is: 289\n",
      "The batch is: 290\n",
      "The batch is: 291\n",
      "The batch is: 292\n",
      "The batch is: 293\n",
      "The batch is: 294\n",
      "The batch is: 295\n",
      "The batch is: 296\n",
      "The batch is: 297\n",
      "The batch is: 298\n",
      "The batch is: 299\n",
      "The batch is: 300\n",
      "The batch is: 301\n",
      "The batch is: 302\n",
      "The batch is: 303\n",
      "The batch is: 304\n",
      "The batch is: 305\n",
      "The batch is: 306\n",
      "The batch is: 307\n",
      "The batch is: 308\n",
      "The batch is: 309\n",
      "The batch is: 310\n",
      "The batch is: 311\n",
      "The batch is: 312\n",
      "The batch is: 313\n",
      "The batch is: 314\n",
      "The batch is: 315\n",
      "The batch is: 316\n",
      "The batch is: 317\n",
      "The batch is: 318\n",
      "The batch is: 319\n",
      "The batch is: 320\n",
      "The batch is: 321\n",
      "The batch is: 322\n",
      "The batch is: 323\n",
      "The batch is: 324\n",
      "The batch is: 325\n",
      "The batch is: 326\n",
      "The batch is: 327\n",
      "The batch is: 328\n",
      "The batch is: 329\n",
      "The batch is: 330\n",
      "The batch is: 331\n",
      "The batch is: 332\n",
      "The batch is: 333\n",
      "The batch is: 334\n",
      "The batch is: 335\n",
      "The batch is: 336\n",
      "The batch is: 337\n",
      "The batch is: 338\n",
      "The batch is: 339\n",
      "The batch is: 340\n",
      "The batch is: 341\n",
      "The batch is: 342\n",
      "The batch is: 343\n",
      "The batch is: 344\n",
      "The batch is: 345\n",
      "The batch is: 346\n",
      "The batch is: 347\n",
      "The batch is: 348\n",
      "The batch is: 349\n",
      "The batch is: 350\n",
      "The batch is: 351\n",
      "The batch is: 352\n",
      "The batch is: 353\n",
      "The batch is: 354\n",
      "The batch is: 355\n",
      "The batch is: 356\n",
      "The batch is: 357\n",
      "The batch is: 358\n",
      "The batch is: 359\n",
      "The batch is: 360\n",
      "The batch is: 361\n",
      "The batch is: 362\n",
      "The batch is: 363\n",
      "The batch is: 364\n",
      "The batch is: 365\n",
      "The batch is: 366\n",
      "The batch is: 367\n",
      "The batch is: 368\n",
      "The batch is: 369\n",
      "The batch is: 370\n",
      "The batch is: 371\n",
      "The batch is: 372\n",
      "The batch is: 373\n",
      "The batch is: 374\n",
      "The batch is: 375\n",
      "The batch is: 376\n",
      "The batch is: 377\n",
      "The batch is: 378\n",
      "The batch is: 379\n",
      "The batch is: 380\n",
      "The batch is: 381\n",
      "The batch is: 382\n",
      "The batch is: 383\n",
      "The batch is: 384\n",
      "The batch is: 385\n",
      "The batch is: 386\n",
      "The batch is: 387\n",
      "The batch is: 388\n",
      "The batch is: 389\n",
      "The batch is: 390\n",
      "The batch is: 391\n",
      "The batch is: 392\n",
      "The batch is: 393\n",
      "The batch is: 394\n",
      "The batch is: 395\n",
      "The batch is: 396\n",
      "The batch is: 397\n",
      "The batch is: 398\n",
      "The batch is: 399\n",
      "The batch is: 400\n",
      "The batch is: 401\n",
      "The batch is: 402\n",
      "The batch is: 403\n",
      "The batch is: 404\n",
      "The batch is: 405\n",
      "The batch is: 406\n",
      "The batch is: 407\n",
      "The batch is: 408\n",
      "The batch is: 409\n",
      "The batch is: 410\n",
      "The batch is: 411\n",
      "The batch is: 412\n",
      "The batch is: 413\n",
      "The batch is: 414\n",
      "The batch is: 415\n",
      "The batch is: 416\n",
      "The batch is: 417\n",
      "The batch is: 418\n",
      "The batch is: 419\n",
      "The batch is: 420\n",
      "The batch is: 421\n",
      "The batch is: 422\n",
      "The batch is: 423\n",
      "The batch is: 424\n",
      "The batch is: 425\n",
      "The batch is: 426\n",
      "The batch is: 427\n",
      "The batch is: 428\n",
      "The batch is: 429\n",
      "The batch is: 430\n",
      "The batch is: 431\n",
      "The batch is: 432\n",
      "The batch is: 433\n",
      "The batch is: 434\n",
      "The batch is: 435\n",
      "The batch is: 436\n",
      "The batch is: 437\n",
      "The batch is: 438\n",
      "The batch is: 439\n",
      "The batch is: 440\n",
      "The batch is: 441\n",
      "The batch is: 442\n",
      "The batch is: 443\n",
      "The batch is: 444\n",
      "The batch is: 445\n",
      "The batch is: 446\n",
      "The batch is: 447\n",
      "The batch is: 448\n",
      "The batch is: 449\n",
      "The batch is: 450\n",
      "The batch is: 451\n",
      "The batch is: 452\n",
      "The batch is: 453\n",
      "The batch is: 454\n",
      "The batch is: 455\n",
      "The batch is: 456\n",
      "The batch is: 457\n",
      "The batch is: 458\n",
      "The batch is: 459\n",
      "The batch is: 460\n",
      "The batch is: 461\n",
      "The batch is: 462\n",
      "The batch is: 463\n",
      "The batch is: 464\n",
      "The batch is: 465\n",
      "The batch is: 466\n",
      "The batch is: 467\n",
      "The batch is: 468\n",
      "The batch is: 469\n",
      "train acc 0.832, test acc 0.853\n",
      "The epoch is: 5\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "The batch is: 236\n",
      "The batch is: 237\n",
      "The batch is: 238\n",
      "The batch is: 239\n",
      "The batch is: 240\n",
      "The batch is: 241\n",
      "The batch is: 242\n",
      "The batch is: 243\n",
      "The batch is: 244\n",
      "The batch is: 245\n",
      "The batch is: 246\n",
      "The batch is: 247\n",
      "The batch is: 248\n",
      "The batch is: 249\n",
      "The batch is: 250\n",
      "The batch is: 251\n",
      "The batch is: 252\n",
      "The batch is: 253\n",
      "The batch is: 254\n",
      "The batch is: 255\n",
      "The batch is: 256\n",
      "The batch is: 257\n",
      "The batch is: 258\n",
      "The batch is: 259\n",
      "The batch is: 260\n",
      "The batch is: 261\n",
      "The batch is: 262\n",
      "The batch is: 263\n",
      "The batch is: 264\n",
      "The batch is: 265\n",
      "The batch is: 266\n",
      "The batch is: 267\n",
      "The batch is: 268\n",
      "The batch is: 269\n",
      "The batch is: 270\n",
      "The batch is: 271\n",
      "The batch is: 272\n",
      "The batch is: 273\n",
      "The batch is: 274\n",
      "The batch is: 275\n",
      "The batch is: 276\n",
      "The batch is: 277\n",
      "The batch is: 278\n",
      "The batch is: 279\n",
      "The batch is: 280\n",
      "The batch is: 281\n",
      "The batch is: 282\n",
      "The batch is: 283\n",
      "The batch is: 284\n",
      "The batch is: 285\n",
      "The batch is: 286\n",
      "The batch is: 287\n",
      "The batch is: 288\n",
      "The batch is: 289\n",
      "The batch is: 290\n",
      "The batch is: 291\n",
      "The batch is: 292\n",
      "The batch is: 293\n",
      "The batch is: 294\n",
      "The batch is: 295\n",
      "The batch is: 296\n",
      "The batch is: 297\n",
      "The batch is: 298\n",
      "The batch is: 299\n",
      "The batch is: 300\n",
      "The batch is: 301\n",
      "The batch is: 302\n",
      "The batch is: 303\n",
      "The batch is: 304\n",
      "The batch is: 305\n",
      "The batch is: 306\n",
      "The batch is: 307\n",
      "The batch is: 308\n",
      "The batch is: 309\n",
      "The batch is: 310\n",
      "The batch is: 311\n",
      "The batch is: 312\n",
      "The batch is: 313\n",
      "The batch is: 314\n",
      "The batch is: 315\n",
      "The batch is: 316\n",
      "The batch is: 317\n",
      "The batch is: 318\n",
      "The batch is: 319\n",
      "The batch is: 320\n",
      "The batch is: 321\n",
      "The batch is: 322\n",
      "The batch is: 323\n",
      "The batch is: 324\n",
      "The batch is: 325\n",
      "The batch is: 326\n",
      "The batch is: 327\n",
      "The batch is: 328\n",
      "The batch is: 329\n",
      "The batch is: 330\n",
      "The batch is: 331\n",
      "The batch is: 332\n",
      "The batch is: 333\n",
      "The batch is: 334\n",
      "The batch is: 335\n",
      "The batch is: 336\n",
      "The batch is: 337\n",
      "The batch is: 338\n",
      "The batch is: 339\n",
      "The batch is: 340\n",
      "The batch is: 341\n",
      "The batch is: 342\n",
      "The batch is: 343\n",
      "The batch is: 344\n",
      "The batch is: 345\n",
      "The batch is: 346\n",
      "The batch is: 347\n",
      "The batch is: 348\n",
      "The batch is: 349\n",
      "The batch is: 350\n",
      "The batch is: 351\n",
      "The batch is: 352\n",
      "The batch is: 353\n",
      "The batch is: 354\n",
      "The batch is: 355\n",
      "The batch is: 356\n",
      "The batch is: 357\n",
      "The batch is: 358\n",
      "The batch is: 359\n",
      "The batch is: 360\n",
      "The batch is: 361\n",
      "The batch is: 362\n",
      "The batch is: 363\n",
      "The batch is: 364\n",
      "The batch is: 365\n",
      "The batch is: 366\n",
      "The batch is: 367\n",
      "The batch is: 368\n",
      "The batch is: 369\n",
      "The batch is: 370\n",
      "The batch is: 371\n",
      "The batch is: 372\n",
      "The batch is: 373\n",
      "The batch is: 374\n",
      "The batch is: 375\n",
      "The batch is: 376\n",
      "The batch is: 377\n",
      "The batch is: 378\n",
      "The batch is: 379\n",
      "The batch is: 380\n",
      "The batch is: 381\n",
      "The batch is: 382\n",
      "The batch is: 383\n",
      "The batch is: 384\n",
      "The batch is: 385\n",
      "The batch is: 386\n",
      "The batch is: 387\n",
      "The batch is: 388\n",
      "The batch is: 389\n",
      "The batch is: 390\n",
      "The batch is: 391\n",
      "The batch is: 392\n",
      "The batch is: 393\n",
      "The batch is: 394\n",
      "The batch is: 395\n",
      "The batch is: 396\n",
      "The batch is: 397\n",
      "The batch is: 398\n",
      "The batch is: 399\n",
      "The batch is: 400\n",
      "The batch is: 401\n",
      "The batch is: 402\n",
      "The batch is: 403\n",
      "The batch is: 404\n",
      "The batch is: 405\n",
      "The batch is: 406\n",
      "The batch is: 407\n",
      "The batch is: 408\n",
      "The batch is: 409\n",
      "The batch is: 410\n",
      "The batch is: 411\n",
      "The batch is: 412\n",
      "The batch is: 413\n",
      "The batch is: 414\n",
      "The batch is: 415\n",
      "The batch is: 416\n",
      "The batch is: 417\n",
      "The batch is: 418\n",
      "The batch is: 419\n",
      "The batch is: 420\n",
      "The batch is: 421\n",
      "The batch is: 422\n",
      "The batch is: 423\n",
      "The batch is: 424\n",
      "The batch is: 425\n",
      "The batch is: 426\n",
      "The batch is: 427\n",
      "The batch is: 428\n",
      "The batch is: 429\n",
      "The batch is: 430\n",
      "The batch is: 431\n",
      "The batch is: 432\n",
      "The batch is: 433\n",
      "The batch is: 434\n",
      "The batch is: 435\n",
      "The batch is: 436\n",
      "The batch is: 437\n",
      "The batch is: 438\n",
      "The batch is: 439\n",
      "The batch is: 440\n",
      "The batch is: 441\n",
      "The batch is: 442\n",
      "The batch is: 443\n",
      "The batch is: 444\n",
      "The batch is: 445\n",
      "The batch is: 446\n",
      "The batch is: 447\n",
      "The batch is: 448\n",
      "The batch is: 449\n",
      "The batch is: 450\n",
      "The batch is: 451\n",
      "The batch is: 452\n",
      "The batch is: 453\n",
      "The batch is: 454\n",
      "The batch is: 455\n",
      "The batch is: 456\n",
      "The batch is: 457\n",
      "The batch is: 458\n",
      "The batch is: 459\n",
      "The batch is: 460\n",
      "The batch is: 461\n",
      "The batch is: 462\n",
      "The batch is: 463\n",
      "The batch is: 464\n",
      "The batch is: 465\n",
      "The batch is: 466\n",
      "The batch is: 467\n",
      "The batch is: 468\n",
      "The batch is: 469\n",
      "train acc 0.846, test acc 0.853\n"
     ]
    }
   ],
   "source": [
    "# sampling_interval = 0.002 # 2ms\n",
    "sampling_interval = 0.002 # 1ms\n",
    "# create the folder to store the data\n",
    "main_folder = DataList[0]\n",
    "\n",
    "print('The folder is:', main_folder)\n",
    "    # find out that if the folder exists in the data path\n",
    "    # 判断文件是否存在\n",
    "if main_folder.exists():\n",
    "    print(\"文件存在。\")\n",
    "else:\n",
    "    os.makedirs(main_folder)\n",
    "    print(\"文件不存在，已创建。\")\n",
    "    print(\"文件创建于：\", main_folder)\n",
    "for epoch in epochs:\n",
    "    for batch in batch_size:\n",
    "        for round in range(rounds):\n",
    "            train_model(main_folder, batch, epoch, round, lr, device, sampling_interval, alexnet_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreenAI",
   "language": "python",
   "name": "greenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
