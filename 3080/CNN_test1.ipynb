{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ptflops import get_model_complexity_info\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "from pynvml import *\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current path is: /home/GreenAI/3080\n",
      "The parent path is: /home/GreenAI\n",
      "The data path is: /home/GreenAI/3080/Test_data\n"
     ]
    }
   ],
   "source": [
    "'''find the Model path'''\n",
    "# find the current path\n",
    "current_path = os.getcwd()\n",
    "print('The current path is:', current_path)\n",
    "\n",
    "# find the parent path\n",
    "parent_path = Path(current_path).parent\n",
    "print('The parent path is:', parent_path)\n",
    "\n",
    "# find the data path\n",
    "data_path = parent_path / '3080/Test_data'\n",
    "print('The data path is:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef alexnet(img_channel, num_labels):\\n    net = nn.Sequential(\\n        # 这里使用一个11*11的更大窗口来捕捉对象。\\n        # 同时，步幅为4，以减少输出的高度和宽度。\\n        # 另外，输出通道的数目远大于LeNet\\n        nn.Conv2d(img_channel, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(),\\n        nn.MaxPool2d(kernel_size=3, stride=2),\\n        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\\n        nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(),\\n        nn.MaxPool2d(kernel_size=3, stride=2),\\n        # 使用三个连续的卷积层和较小的卷积窗口。\\n        # 除了最后的卷积层，输出通道的数量进一步增加。\\n        # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\\n        nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(),\\n        nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\\n        nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(),\\n        nn.MaxPool2d(kernel_size=3, stride=2),\\n        nn.AdaptiveAvgPool2d((6, 6)),   # 使用全局平均池化对每个通道中所有元素求平均并直接将结果传递到全连接层\\n        nn.Flatten(),\\n        # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\\n        nn.Linear(256 * 6 * 6, 4096), nn.ReLU(),\\n        nn.Dropout(p=0.5),\\n        nn.Linear(4096, 4096), nn.ReLU(),\\n        nn.Dropout(p=0.5),\\n        # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\\n        nn.Linear(4096, num_labels))\\n    return net\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def alexnet(img_channel, num_labels):\n",
    "    net = nn.Sequential(\n",
    "        # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "        # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "        # 另外，输出通道的数目远大于LeNet\n",
    "        nn.Conv2d(img_channel, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "        nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "        # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "        # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "        nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.AdaptiveAvgPool2d((6, 6)),   # 使用全局平均池化对每个通道中所有元素求平均并直接将结果传递到全连接层\n",
    "        nn.Flatten(),\n",
    "        # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "        nn.Linear(256 * 6 * 6, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "        nn.Linear(4096, num_labels))\n",
    "    return net\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "    \n",
    "def resnet(img_channel, num_labels):\n",
    "    # blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                    first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(input_channels, num_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                        nn.AdaptiveAvgPool2d((1,1)),\n",
    "                        nn.Flatten(), nn.Linear(512, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer name is: ['AdaptiveAvgPool2d', 'Flatten', 'Linear', 'Sequential']\n",
      "The number of layers is: 4\n"
     ]
    }
   ],
   "source": [
    "# print the model structure\n",
    "# net = alexnet(1, 10)    \n",
    "net = resnet(1, 10)\n",
    "# print(net)\n",
    "# print each layer\n",
    "layer_name = []\n",
    "for layer in net:\n",
    "    name = layer.__class__.__name__\n",
    "    layer_name.append(name)\n",
    "# find the unique layer name, and fix the order\n",
    "layer_name = sorted(list(set(layer_name)))\n",
    "print('The layer name is:', layer_name)\n",
    "# the number of layers, which contains ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
    "num_layers = len(layer_name) \n",
    "print('The number of layers is:', num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print the model structure\n",
    "# AlexNet = alexnet(1, 10)    \n",
    "# # print(net)\n",
    "# # print each layer\n",
    "# alexlayer = []\n",
    "# DNNlayer = []\n",
    "# num = 0\n",
    "# for layer in AlexNet:\n",
    "#     num += 1\n",
    "#     name = layer.__class__.__name__\n",
    "#     layer_name = name[:4] + str(num)\n",
    "#     DNNlayer.append(name)\n",
    "#     alexlayer.append(layer_name)\n",
    "# # find the unique layer name, and fix the order\n",
    "# DNNlayer_org = sorted(list(set(DNNlayer)))\n",
    "# print('The layer name is:', alexlayer)\n",
    "# print('The layer name after orged is:', DNNlayer_org)\n",
    "# # the number of layers, which contains ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
    "# alexlayer_num = len(alexlayer) \n",
    "# print('The number of layers is:', alexlayer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Sequential\n",
      "  0: Conv2d\n",
      "  1: BatchNorm2d\n",
      "  2: ReLU\n",
      "  3: MaxPool2d\n",
      "1: Sequential\n",
      "  0: Residual\n",
      "  1: Residual\n",
      "2: Sequential\n",
      "  0: Residual\n",
      "  1: Residual\n",
      "3: Sequential\n",
      "  0: Residual\n",
      "  1: Residual\n",
      "4: Sequential\n",
      "  0: Residual\n",
      "  1: Residual\n",
      "5: AdaptiveAvgPool2d\n",
      "6: Flatten\n",
      "7: Linear\n",
      "The layer name is: ['S0_C0', 'S0_B1', 'S0_R2', 'S0_M3', 'S1_R0', 'S1_R1', 'S2_R0', 'S2_R1', 'S3_R0', 'S3_R1', 'S4_R0', 'S4_R1', 'A5', 'F6', 'L7']\n",
      "The length of layer name is: 15\n",
      "The number of blocks is: 5\n",
      "The number of inception blocks is: 8\n"
     ]
    }
   ],
   "source": [
    "net = resnet(1, 10)    \n",
    "LayerName = []\n",
    "block_num = 0\n",
    "resblock_num = 0\n",
    "\n",
    "for num, layer in net.named_children():  # 使用 named_children 来获取层名和层\n",
    "    layername = layer.__class__.__name__\n",
    "    print(f\"{num}: {layername}\")  # 打印层名和层类名\n",
    "    if layer.__class__.__name__ == 'Sequential':\n",
    "        block_num += 1\n",
    "        for sublayernum, sublayer in layer.named_children():  # 再次使用 named_children\n",
    "            sublayername = sublayer.__class__.__name__\n",
    "            if sublayername == 'Residual':\n",
    "                resblock_num += 1\n",
    "            print(f\"  {sublayernum}: {sublayername}\")\n",
    "            layer_label = f'{layername[0]}{num}_{sublayername[0]}{sublayernum}'\n",
    "            LayerName.append(layer_label)  # 收集子块的类型\n",
    "    else:\n",
    "        layer_label = f'{layername[0]}{num}'\n",
    "        LayerName.append(layer_label)\n",
    "            \n",
    "print('The layer name is:', LayerName)\n",
    "print(f'The length of layer name is: {len(LayerName)}')\n",
    "print('The number of blocks is:', block_num)\n",
    "print('The number of inception blocks is:', resblock_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build different alexnet model for different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# CIFAR100中的图像通道数为3，类别数为100\\nalexnet_c = alexnet(3, 100)\\n# CIFAR10中的图像通道数为3，类别数为10\\nalexnet_c10 = alexnet(3, 10)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于不同的数据集，要设置不同的img_channel和num_labels\n",
    "# Fashion-MNIST中的图像通道数为1，类别数为10\n",
    "# alexnet_f = alexnet(1, 10)\n",
    "resnet_f = resnet(1, 10)\n",
    "'''\n",
    "# CIFAR100中的图像通道数为3，类别数为100\n",
    "alexnet_c = alexnet(3, 100)\n",
    "# CIFAR10中的图像通道数为3，类别数为10\n",
    "alexnet_c10 = alexnet(3, 10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Residual is treated as a zero-op.\n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Sequential(\n",
      "  11.18 M, 100.000% Params, 1.74 GMac, 99.820% MACs, \n",
      "  (0): Sequential(\n",
      "    3.33 k, 0.030% Params, 43.35 MMac, 2.482% MACs, \n",
      "    (0): Conv2d(3.2 k, 0.029% Params, 40.14 MMac, 2.298% MACs, 1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(128, 0.001% Params, 1.61 MMac, 0.092% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(0, 0.000% Params, 802.82 KMac, 0.046% MACs, )\n",
      "    (3): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.046% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    148.22 k, 1.326% Params, 464.83 MMac, 26.611% MACs, \n",
      "    (0): Residual(\n",
      "      74.11 k, 0.663% Params, 232.42 MMac, 13.306% MACs, \n",
      "      (conv1): Conv2d(36.93 k, 0.330% Params, 115.81 MMac, 6.630% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(36.93 k, 0.330% Params, 115.81 MMac, 6.630% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      74.11 k, 0.663% Params, 232.42 MMac, 13.306% MACs, \n",
      "      (conv1): Conv2d(36.93 k, 0.330% Params, 115.81 MMac, 6.630% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(36.93 k, 0.330% Params, 115.81 MMac, 6.630% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, 0.001% Params, 401.41 KMac, 0.023% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    525.95 k, 4.705% Params, 412.35 MMac, 23.606% MACs, \n",
      "    (0): Residual(\n",
      "      230.27 k, 2.060% Params, 180.53 MMac, 10.335% MACs, \n",
      "      (conv1): Conv2d(73.86 k, 0.661% Params, 57.9 MMac, 3.315% MACs, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(147.58 k, 1.320% Params, 115.71 MMac, 6.624% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(8.32 k, 0.074% Params, 6.52 MMac, 0.373% MACs, 64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.011% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.011% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      295.68 k, 2.645% Params, 231.81 MMac, 13.271% MACs, \n",
      "      (conv1): Conv2d(147.58 k, 1.320% Params, 115.71 MMac, 6.624% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(147.58 k, 1.320% Params, 115.71 MMac, 6.624% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.011% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, 0.002% Params, 200.7 KMac, 0.011% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    2.1 M, 18.791% Params, 411.69 MMac, 23.569% MACs, \n",
      "    (0): Residual(\n",
      "      919.3 k, 8.224% Params, 180.18 MMac, 10.315% MACs, \n",
      "      (conv1): Conv2d(295.17 k, 2.641% Params, 57.85 MMac, 3.312% MACs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(590.08 k, 5.279% Params, 115.66 MMac, 6.621% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(33.02 k, 0.295% Params, 6.47 MMac, 0.371% MACs, 128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      1.18 M, 10.567% Params, 231.51 MMac, 13.254% MACs, \n",
      "      (conv1): Conv2d(590.08 k, 5.279% Params, 115.66 MMac, 6.621% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(590.08 k, 5.279% Params, 115.66 MMac, 6.621% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, 0.005% Params, 100.35 KMac, 0.006% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    8.4 M, 75.103% Params, 411.37 MMac, 23.550% MACs, \n",
      "    (0): Residual(\n",
      "      3.67 M, 32.863% Params, 180.01 MMac, 10.305% MACs, \n",
      "      (conv1): Conv2d(1.18 M, 10.558% Params, 57.83 MMac, 3.311% MACs, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (conv2): Conv2d(2.36 M, 21.110% Params, 115.63 MMac, 6.620% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv3): Conv2d(131.58 k, 1.177% Params, 6.45 MMac, 0.369% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Residual(\n",
      "      4.72 M, 42.239% Params, 231.36 MMac, 13.245% MACs, \n",
      "      (conv1): Conv2d(2.36 M, 21.110% Params, 115.63 MMac, 6.620% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(2.36 M, 21.110% Params, 115.63 MMac, 6.620% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.009% Params, 50.18 KMac, 0.003% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): AdaptiveAvgPool2d(0, 0.000% Params, 25.09 KMac, 0.001% MACs, output_size=(1, 1))\n",
      "  (6): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (7): Linear(5.13 k, 0.046% Params, 5.13 KMac, 0.000% MACs, in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       1.75 GMac\n",
      "Number of parameters:           11.18 M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('*'*50)\\n\\n# cifar100\\nwith torch.cuda.device(0):\\n    macs_c, params_c = get_model_complexity_info(alexnet_c, (3, 224, 224), as_strings=True,\\n                                            print_per_layer_stat=True, verbose=True)\\n    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_c))\\n    print('{:<30}  {:<8}'.format('Number of parameters: ', params_c))\\n    \\n# cifar10\\nwith torch.cuda.device(0):\\n    macs_c10, params_c10 = get_model_complexity_info(alexnet_c10, (3, 224, 224), as_strings=True,\\n                                            print_per_layer_stat=True, verbose=True)\\n    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_c10))\\n    print('{:<30}  {:<8}'.format('Number of parameters: ', params_c10))\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # fashion mnist\n",
    "# with torch.cuda.device(0):\n",
    "#     macs_f, params_f = get_model_complexity_info(alexnet_f, (1, 224, 224), as_strings=True,\n",
    "#                                             print_per_layer_stat=True, verbose=True)\n",
    "#     print('{:<30}  {:<8}'.format('Computational complexity: ', macs_f))\n",
    "#     print('{:<30}  {:<8}'.format('Number of parameters: ', params_f))\n",
    "\n",
    "# fashion mnist\n",
    "with torch.cuda.device(0):\n",
    "    macs_f, params_f = get_model_complexity_info(resnet_f, (1, 224, 224), as_strings=True,\n",
    "                                            print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_f))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params_f))\n",
    "'''\n",
    "print('*'*50)\n",
    "\n",
    "# cifar100\n",
    "with torch.cuda.device(0):\n",
    "    macs_c, params_c = get_model_complexity_info(alexnet_c, (3, 224, 224), as_strings=True,\n",
    "                                            print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_c))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params_c))\n",
    "    \n",
    "# cifar10\n",
    "with torch.cuda.device(0):\n",
    "    macs_c10, params_c10 = get_model_complexity_info(alexnet_c10, (3, 224, 224), as_strings=True,\n",
    "                                            print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_c10))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params_c10))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('*'*50)\\n\\nfor layer in alexnet_c:\\n    X_c=layer(X_c)\\n    print(layer.__class__.__name__,'output shape:\\t',X_c.shape)\\n    \\nprint('*'*50)\\n\\nfor layer in alexnet_c10:\\n    x_c10=layer(x_c10)\\n    print(layer.__class__.__name__,'output shape:\\t',x_c10.shape)\\n\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_f = torch.randn(size=(1, 1, 224, 224), dtype=torch.float32) # fashion mnist\n",
    "'''\n",
    "X_c = torch.randn(size=(1, 3, 224, 224), dtype=torch.float32) # cifar100\n",
    "x_c10 = torch.randn(size=(1, 3, 224, 224), dtype=torch.float32) # cifar10\n",
    "'''\n",
    "\n",
    "# for layer in alexnet_f:\n",
    "#     X_f=layer(X_f)\n",
    "#     print(layer.__class__.__name__,'output shape:\\t',X_f.shape)\n",
    "\n",
    "\n",
    "for layer in resnet_f:\n",
    "    X_f=layer(X_f)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X_f.shape)\n",
    "\n",
    "'''\n",
    "print('*'*50)\n",
    "\n",
    "for layer in alexnet_c:\n",
    "    X_c=layer(X_c)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X_c.shape)\n",
    "    \n",
    "print('*'*50)\n",
    "\n",
    "for layer in alexnet_c10:\n",
    "    x_c10=layer(x_c10)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',x_c10.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_data_cifar100(batch_size, resize=None):\\n    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\\n\\n    Defined in :numref:`sec_utils`\"\"\"\\n    trans = [transforms.ToTensor()]\\n    if resize:\\n        trans.insert(0, transforms.Resize(resize))\\n    trans = transforms.Compose(trans)\\n    # import the cifar100 dataset\\n    cifar_train = torchvision.datasets.CIFAR100(\\n        root=\"../data\", train=True, transform=trans, download=True)\\n    cifar_test = torchvision.datasets.CIFAR100(\\n        root=\"../data\", train=False, transform=trans, download=True)\\n    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\\n                                        num_workers=get_dataloader_workers()),\\n            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\\n                                        num_workers=get_dataloader_workers()))\\n    \\ndef load_data_cifar10(batch_size, resize=None):\\n    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\\n\\n    Defined in :numref:`sec_utils`\"\"\"\\n    trans = [transforms.ToTensor()]\\n    if resize:\\n        trans.insert(0, transforms.Resize(resize))\\n    trans = transforms.Compose(trans)\\n    # import the cifar100 dataset\\n    cifar_train = torchvision.datasets.CIFAR10(\\n        root=\"../data\", train=True, transform=trans, download=True)\\n    cifar_test = torchvision.datasets.CIFAR10(\\n        root=\"../data\", train=False, transform=trans, download=True)\\n    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\\n                                        num_workers=get_dataloader_workers()),\\n            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\\n                                        num_workers=get_dataloader_workers()))\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "# fashion mnist\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集，然后将其加载到内存中\n",
    "\n",
    "    Defined in :numref:`sec_fashion_mnist`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "'''\n",
    "def load_data_cifar100(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    \n",
    "def load_data_cifar10(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [256]\n",
    "# epochs = [10, 20, 30, 40, 50, 80, 100]\n",
    "epochs = [1]\n",
    "rounds = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_func(net, train_iter, test_iter, alexlayer, num_epochs, lr, device):\\n    def init_weights(m): # 初始化权重\\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\\n            nn.init.xavier_uniform_(m.weight)\\n    net.apply(init_weights)\\n    print(\\'training on\\', device)\\n    net.to(device)\\n    # create a ndarray to store each layer\\'s total running time of each epoch\\n    Layers_time = np.zeros((len(alexlayer), num_epochs)) # each row is a layer, each column is an epoch\\n    print(f\\'The name of the layers are: {alexlayer}\\')\\n    Train_part_time = np.zeros((6, num_epochs)) # store the time to device, forward and backward time, and test time of each epoch\\n    Train_time = np.zeros(num_epochs) # store the total training time of each epoch\\n    Train_acc = np.zeros(num_epochs) # store the training accuracy of each epoch\\n    Test_acc = np.zeros(num_epochs) # store the test accuracy of each epoch\\n    Epoch_time = np.zeros(num_epochs) # store the total time of each epoch\\n    Epoch_energy = np.zeros((num_epochs,1), dtype=\\'object\\') # store the total energy of each epoch\\n    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\\n    loss_fn = nn.CrossEntropyLoss()\\n    timer = d2l.Timer()\\n    train_timer = d2l.Timer()\\n    ttd_timer = d2l.Timer()\\n    forward_timer = d2l.Timer()\\n    loss_timer = d2l.Timer()\\n    backward_timer = d2l.Timer()\\n    opt_timer = d2l.Timer()\\n    layer_timer = d2l.Timer()\\n    test_timer = d2l.Timer()\\n    # start training\\n    for epoch in range(num_epochs):\\n        print(\\'The epoch is:\\', epoch+1)\\n        timer.start()\\n        net.train()\\n        train_epoch, ttd_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch= 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\\n        layer_epoch = np.zeros((len(alexlayer), 1)) # store the total running time of each layer in one epoch\\n        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples   \\n        # start the nvidia-smi command\\n        with open(\\'gpu_power_usage.csv\\', \\'w\\') as file:\\n            # Start the nvidia-smi command\\n            nvidia_smi_process = subprocess.Popen(\\n                [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv\", \"--loop-ms=10\"],\\n                stdout=file,  # Redirect the output directly to the file\\n                stderr=subprocess.PIPE,\\n                text=True)\\n        train_timer.start()\\n        for i, (X, y) in enumerate(train_iter):\\n            lnamenum = 0\\n            print(\\'The batch is:\\', i+1)\\n            optimizer.zero_grad()\\n            # to device\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            ttd_timer.start()\\n            # sleep for 2 seconds\\n            time.sleep(2)\\n            X, y = X.to(device), y.to(device)\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            # sleep for 2 seconds\\n            time.sleep(2)\\n            ttd_epoch += ttd_timer.stop()\\n            # forward\\n            forward_timer.start()\\n            y_hat = X\\n            for layer in net:\\n                name = layer.__class__.__name__ # 获取层的名字\\n                lnamenum += 1\\n                lname = name[:4] + str(lnamenum)\\n                layer_index = alexlayer.index(lname)\\n                layer_timer.start()\\n                y_hat = layer(y_hat)\\n                torch.cuda.synchronize()  # 等待数据传输完成\\n                layer_epoch[layer_index] += layer_timer.stop()\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            forward_epoch += forward_timer.stop()\\n            # loss\\n            loss_timer.start()\\n            l = loss_fn(y_hat, y)\\n            # backward\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            loss_epoch += loss_timer.stop()\\n            backward_timer.start()\\n            l.backward()\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            backward_epoch += backward_timer.stop()\\n            # optimize\\n            opt_timer.start()   \\n            optimizer.step()\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            opt_epoch += opt_timer.stop()\\n            with torch.no_grad():\\n                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\\n            train_acc = metric[1] / metric[2]\\n        train_epoch = train_timer.stop()\\n        test_timer.start()\\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\\n        testtime_epoch = test_timer.stop()\\n        print(f\\'train acc {train_acc:.3f}, test acc {test_acc:.3f}\\')\\n        print(\\'epoch %d, time %f sec\\' % (epoch+1, timer.sum()))\\n        # store the time and acc data\\n        Epoch_time[epoch] = timer.stop()\\n        print(f\\'The total time of the {epoch} is:\\', Epoch_time[epoch])\\n        Layers_time[:, epoch] = layer_epoch.flatten()\\n        Train_part_time[:, epoch] = ttd_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch\\n        Train_time[epoch] = train_epoch\\n        Train_acc[epoch] = train_acc\\n        Test_acc[epoch] = test_acc\\n        # stop the nvidia-smi command\\n        nvidia_smi_process.terminate()\\n        # calculate the energy consumption of each epoch\\n        GPU_df = pd.read_csv(\\'gpu_power_usage.csv\\')\\n        for row in range(len(GPU_df)):\\n            GPU_df.iloc[row,0] = GPU_df.iloc[row,0].replace(\\' W\\',\\'\\')\\n        Consumption_df = GPU_df.astype(float)  \\n        EnergyDatai = Consumption_df.iloc[:,0].values # 将数据转换为numpy数组\\n        # store the energy data\\n        Epoch_energy[epoch,0] = EnergyDatai\\n    return Layers_time, Train_part_time, Train_time, Train_acc, Test_acc, Epoch_time, Epoch_energy\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_func(net, train_iter, test_iter, alexlayer, num_epochs, lr, device):\n",
    "    def init_weights(m): # 初始化权重\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    # create a ndarray to store each layer's total running time of each epoch\n",
    "    Layers_time = np.zeros((len(alexlayer), num_epochs)) # each row is a layer, each column is an epoch\n",
    "    print(f'The name of the layers are: {alexlayer}')\n",
    "    Train_part_time = np.zeros((6, num_epochs)) # store the time to device, forward and backward time, and test time of each epoch\n",
    "    Train_time = np.zeros(num_epochs) # store the total training time of each epoch\n",
    "    Train_acc = np.zeros(num_epochs) # store the training accuracy of each epoch\n",
    "    Test_acc = np.zeros(num_epochs) # store the test accuracy of each epoch\n",
    "    Epoch_time = np.zeros(num_epochs) # store the total time of each epoch\n",
    "    Epoch_energy = np.zeros((num_epochs,1), dtype='object') # store the total energy of each epoch\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    timer = d2l.Timer()\n",
    "    train_timer = d2l.Timer()\n",
    "    ttd_timer = d2l.Timer()\n",
    "    forward_timer = d2l.Timer()\n",
    "    loss_timer = d2l.Timer()\n",
    "    backward_timer = d2l.Timer()\n",
    "    opt_timer = d2l.Timer()\n",
    "    layer_timer = d2l.Timer()\n",
    "    test_timer = d2l.Timer()\n",
    "    # start training\n",
    "    for epoch in range(num_epochs):\n",
    "        print('The epoch is:', epoch+1)\n",
    "        timer.start()\n",
    "        net.train()\n",
    "        train_epoch, ttd_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch= 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        layer_epoch = np.zeros((len(alexlayer), 1)) # store the total running time of each layer in one epoch\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples   \n",
    "        # start the nvidia-smi command\n",
    "        with open('gpu_power_usage.csv', 'w') as file:\n",
    "            # Start the nvidia-smi command\n",
    "            nvidia_smi_process = subprocess.Popen(\n",
    "                [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv\", \"--loop-ms=10\"],\n",
    "                stdout=file,  # Redirect the output directly to the file\n",
    "                stderr=subprocess.PIPE,\n",
    "                text=True)\n",
    "        train_timer.start()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            lnamenum = 0\n",
    "            print('The batch is:', i+1)\n",
    "            optimizer.zero_grad()\n",
    "            # to device\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            ttd_timer.start()\n",
    "            # sleep for 2 seconds\n",
    "            time.sleep(2)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            # sleep for 2 seconds\n",
    "            time.sleep(2)\n",
    "            ttd_epoch += ttd_timer.stop()\n",
    "            # forward\n",
    "            forward_timer.start()\n",
    "            y_hat = X\n",
    "            for layer in net:\n",
    "                name = layer.__class__.__name__ # 获取层的名字\n",
    "                lnamenum += 1\n",
    "                lname = name[:4] + str(lnamenum)\n",
    "                layer_index = alexlayer.index(lname)\n",
    "                layer_timer.start()\n",
    "                y_hat = layer(y_hat)\n",
    "                torch.cuda.synchronize()  # 等待数据传输完成\n",
    "                layer_epoch[layer_index] += layer_timer.stop()\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            forward_epoch += forward_timer.stop()\n",
    "            # loss\n",
    "            loss_timer.start()\n",
    "            l = loss_fn(y_hat, y)\n",
    "            # backward\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            loss_epoch += loss_timer.stop()\n",
    "            backward_timer.start()\n",
    "            l.backward()\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            backward_epoch += backward_timer.stop()\n",
    "            # optimize\n",
    "            opt_timer.start()   \n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            opt_epoch += opt_timer.stop()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_acc = metric[1] / metric[2]\n",
    "        train_epoch = train_timer.stop()\n",
    "        test_timer.start()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        testtime_epoch = test_timer.stop()\n",
    "        print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "        print('epoch %d, time %f sec' % (epoch+1, timer.sum()))\n",
    "        # store the time and acc data\n",
    "        Epoch_time[epoch] = timer.stop()\n",
    "        print(f'The total time of the {epoch} is:', Epoch_time[epoch])\n",
    "        Layers_time[:, epoch] = layer_epoch.flatten()\n",
    "        Train_part_time[:, epoch] = ttd_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch\n",
    "        Train_time[epoch] = train_epoch\n",
    "        Train_acc[epoch] = train_acc\n",
    "        Test_acc[epoch] = test_acc\n",
    "        # stop the nvidia-smi command\n",
    "        nvidia_smi_process.terminate()\n",
    "        # calculate the energy consumption of each epoch\n",
    "        GPU_df = pd.read_csv('gpu_power_usage.csv')\n",
    "        for row in range(len(GPU_df)):\n",
    "            GPU_df.iloc[row,0] = GPU_df.iloc[row,0].replace(' W','')\n",
    "        Consumption_df = GPU_df.astype(float)  \n",
    "        EnergyDatai = Consumption_df.iloc[:,0].values # 将数据转换为numpy数组\n",
    "        # store the energy data\n",
    "        Epoch_energy[epoch,0] = EnergyDatai\n",
    "    return Layers_time, Train_part_time, Train_time, Train_acc, Test_acc, Epoch_time, Epoch_energy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvml_sampling_thread(handle, data_queue, stop_event, sampling_interval=0.01):\n",
    "    \"\"\"\n",
    "    在单独的线程中定期调用 NVML, 获取功耗数据并存储到 data_queue 中。\n",
    "    参数：\n",
    "    - handle: nvmlDeviceGetHandleByIndex(0) 得到的 GPU 句柄\n",
    "    - data_queue: 用于存放 (timestamp, power_in_watts) 数据的队列\n",
    "    - stop_event: 当此事件被设置时，线程应结束循环\n",
    "    - sampling_interval: 采样间隔（秒）\n",
    "    \"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        current_time = time.time()\n",
    "        current_power = nvmlDeviceGetPowerUsage(handle) / 1000.0  # mW -> W\n",
    "        data_queue.put((current_time, current_power))\n",
    "        time.sleep(sampling_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_func(net, train_iter, test_iter, alexlayer, num_epochs, lr, device):\\n    def init_weights(m): # 初始化权重\\n        if type(m) == nn.Linear or type(m) == nn.Conv2d:\\n            nn.init.xavier_uniform_(m.weight)\\n    net.apply(init_weights)\\n    print(\\'training on\\', device)\\n    net.to(device)\\n    # create a ndarray to store each layer\\'s total running time of each epoch\\n    # Layers_time = np.zeros((len(alexlayer), num_epochs)) # each row is a layer, each column is an epoch\\n    print(f\\'The name of the layers are: {alexlayer}\\')\\n    # Train_part_time = np.zeros((6, num_epochs)) # store the time to device, forward and backward time, and test time of each epoch\\n    # Train_time = np.zeros(num_epochs) # store the total training time of each epoch\\n    # Epoch_time = np.zeros(num_epochs) # store the total time of each epoch\\n    # Epoch_energy = np.zeros((num_epochs,1), dtype=\\'object\\') # store the total energy of each epoch\\n    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\\n    loss_fn = nn.CrossEntropyLoss()\\n    timer = d2l.Timer()\\n    # train_timer = d2l.Timer()\\n    ttd_timer = d2l.Timer()\\n    # forward_timer = d2l.Timer()\\n    # loss_timer = d2l.Timer()\\n    # backward_timer = d2l.Timer()\\n    # opt_timer = d2l.Timer()\\n    # layer_timer = d2l.Timer()\\n    # test_timer = d2l.Timer()\\n    nvmlInit()\\n    handle = nvmlDeviceGetHandleByIndex(0)\\n    # 创建一个队列来存储功率数据\\n    power_data_queue = queue.Queue()\\n\\n    # 创建一个线程停止事件\\n    stop_event = threading.Event()\\n\\n    # 启动采样线程，每10ms获取一次功耗数据\\n    sampling_interval = 0.005\\n\\n    sampler_thread1 = threading.Thread(target=nvml_sampling_thread, args=(handle, power_data_queue, stop_event, sampling_interval))\\n    start_time = time.time()\\n    ttd_total = []\\n    # start training\\n    for epoch in range(num_epochs):\\n        ttd_epoch = 0.0\\n        print(\\'The epoch is:\\', epoch+1)\\n        timer.start()\\n        net.train()\\n        # train_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch= 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\\n        # layer_epoch = np.zeros((len(alexlayer), 1)) # store the total running time of each layer in one epoch\\n        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\\n        ##########################################################################################################   \\n        # start the nvidia-smi command\\n        # with open(\\'gpu_power_usage.csv\\', \\'w\\') as file:\\n        #     # Start the nvidia-smi command\\n        #     nvidia_smi_process = subprocess.Popen(\\n        #         [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv\", \"--loop-ms=10\"],\\n        #         stdout=file,  # Redirect the output directly to the file\\n        #         stderr=subprocess.PIPE,\\n        #         text=True)\\n        ##########################################################################################################\\n        # train_timer.start()\\n        for i, (X, y) in enumerate(train_iter):\\n            # lnamenum = 0\\n            print(\\'The batch is:\\', i+1)\\n            optimizer.zero_grad()\\n            # to device\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            ttd_timer.start()\\n            sampler_thread1.start()\\n            # start_power = nvmlDeviceGetPowerUsage(handle) / 1000.0\\n            X, y = X.to(device), y.to(device)\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            duration = ttd_timer.stop()\\n            # 训练结束后，通知线程停止并等待其结束\\n            stop_event.set()\\n            sampler_thread1.join()\\n            # end_power = nvmlDeviceGetPowerUsage(handle) / 1000.0\\n            # avg_power = (start_power + end_power) / 2.0\\n            # energy_joules = avg_power * duration\\n            # print(f\"Data transfer took {duration:.6f}s, Estimated Energy: {energy_joules:.6f}J\")\\n            # forward\\n            # forward_timer.start()\\n            y_hat = net(X)\\n            # for layer in net:\\n            #     name = layer.__class__.__name__ # 获取层的名字\\n            #     lnamenum += 1\\n            #     lname = name[:4] + str(lnamenum)\\n            #     layer_index = alexlayer.index(lname)\\n            #     layer_timer.start()\\n            #     y_hat = layer(y_hat)\\n            #     torch.cuda.synchronize()  # 等待数据传输完成\\n            #     layer_epoch[layer_index] += layer_timer.stop()\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            # forward_epoch += forward_timer.stop()\\n            # loss\\n            # loss_timer.start()\\n            l = loss_fn(y_hat, y)\\n            # backward\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            # loss_epoch += loss_timer.stop()\\n            # backward_timer.start()\\n            l.backward()\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            # backward_epoch += backward_timer.stop()\\n            # optimize\\n            # opt_timer.start()   \\n            optimizer.step()\\n            torch.cuda.synchronize()  # 等待数据传输完成\\n            # opt_epoch += opt_timer.stop()\\n            with torch.no_grad():\\n                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\\n            train_acc = metric[1] / metric[2]\\n        # train_epoch = train_timer.stop()\\n        # test_timer.start()\\n        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\\n        # testtime_epoch = test_timer.stop()\\n        print(f\\'train acc {train_acc:.3f}, test acc {test_acc:.3f}\\')\\n        print(\\'epoch %d, time %f sec\\' % (epoch+1, timer.sum()))\\n        # store the time and acc data\\n        # Epoch_time[epoch] = timer.stop()\\n        # print(f\\'The total time of the {epoch} is:\\', Epoch_time[epoch])\\n        # Layers_time[:, epoch] = layer_epoch.flatten()\\n        # Train_part_time[:, epoch] = ttd_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch\\n        # Train_time[epoch] = train_epoch\\n        # Train_acc[epoch] = train_acc\\n        # Test_acc[epoch] = test_acc\\n        # stop the nvidia-smi command\\n        # nvidia_smi_process.terminate()\\n        # calculate the energy consumption of each epoch\\n        # GPU_df = pd.read_csv(\\'gpu_power_usage.csv\\')\\n        # for row in range(len(GPU_df)):\\n        #     GPU_df.iloc[row,0] = GPU_df.iloc[row,0].replace(\\' W\\',\\'\\')\\n        # Consumption_df = GPU_df.astype(float)  \\n        # EnergyDatai = Consumption_df.iloc[:,0].values # 将数据转换为numpy数组\\n        # store the energy data\\n        # Epoch_energy[epoch,0] = EnergyDatai\\n    # 关闭 NVML\\n    nvmlShutdown()\\n    # return Layers_time, Train_part_time, Train_time, Epoch_time\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_func(net, train_iter, test_iter, alexlayer, num_epochs, lr, device):\n",
    "    def init_weights(m): # 初始化权重\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    # create a ndarray to store each layer's total running time of each epoch\n",
    "    # Layers_time = np.zeros((len(alexlayer), num_epochs)) # each row is a layer, each column is an epoch\n",
    "    print(f'The name of the layers are: {alexlayer}')\n",
    "    # Train_part_time = np.zeros((6, num_epochs)) # store the time to device, forward and backward time, and test time of each epoch\n",
    "    # Train_time = np.zeros(num_epochs) # store the total training time of each epoch\n",
    "    # Epoch_time = np.zeros(num_epochs) # store the total time of each epoch\n",
    "    # Epoch_energy = np.zeros((num_epochs,1), dtype='object') # store the total energy of each epoch\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    timer = d2l.Timer()\n",
    "    # train_timer = d2l.Timer()\n",
    "    ttd_timer = d2l.Timer()\n",
    "    # forward_timer = d2l.Timer()\n",
    "    # loss_timer = d2l.Timer()\n",
    "    # backward_timer = d2l.Timer()\n",
    "    # opt_timer = d2l.Timer()\n",
    "    # layer_timer = d2l.Timer()\n",
    "    # test_timer = d2l.Timer()\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    # 创建一个队列来存储功率数据\n",
    "    power_data_queue = queue.Queue()\n",
    "\n",
    "    # 创建一个线程停止事件\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # 启动采样线程，每10ms获取一次功耗数据\n",
    "    sampling_interval = 0.005\n",
    "\n",
    "    sampler_thread1 = threading.Thread(target=nvml_sampling_thread, args=(handle, power_data_queue, stop_event, sampling_interval))\n",
    "    start_time = time.time()\n",
    "    ttd_total = []\n",
    "    # start training\n",
    "    for epoch in range(num_epochs):\n",
    "        ttd_epoch = 0.0\n",
    "        print('The epoch is:', epoch+1)\n",
    "        timer.start()\n",
    "        net.train()\n",
    "        # train_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch= 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        # layer_epoch = np.zeros((len(alexlayer), 1)) # store the total running time of each layer in one epoch\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "        ##########################################################################################################   \n",
    "        # start the nvidia-smi command\n",
    "        # with open('gpu_power_usage.csv', 'w') as file:\n",
    "        #     # Start the nvidia-smi command\n",
    "        #     nvidia_smi_process = subprocess.Popen(\n",
    "        #         [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv\", \"--loop-ms=10\"],\n",
    "        #         stdout=file,  # Redirect the output directly to the file\n",
    "        #         stderr=subprocess.PIPE,\n",
    "        #         text=True)\n",
    "        ##########################################################################################################\n",
    "        # train_timer.start()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            # lnamenum = 0\n",
    "            print('The batch is:', i+1)\n",
    "            optimizer.zero_grad()\n",
    "            # to device\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            ttd_timer.start()\n",
    "            sampler_thread1.start()\n",
    "            # start_power = nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            duration = ttd_timer.stop()\n",
    "            # 训练结束后，通知线程停止并等待其结束\n",
    "            stop_event.set()\n",
    "            sampler_thread1.join()\n",
    "            # end_power = nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
    "            # avg_power = (start_power + end_power) / 2.0\n",
    "            # energy_joules = avg_power * duration\n",
    "            # print(f\"Data transfer took {duration:.6f}s, Estimated Energy: {energy_joules:.6f}J\")\n",
    "            # forward\n",
    "            # forward_timer.start()\n",
    "            y_hat = net(X)\n",
    "            # for layer in net:\n",
    "            #     name = layer.__class__.__name__ # 获取层的名字\n",
    "            #     lnamenum += 1\n",
    "            #     lname = name[:4] + str(lnamenum)\n",
    "            #     layer_index = alexlayer.index(lname)\n",
    "            #     layer_timer.start()\n",
    "            #     y_hat = layer(y_hat)\n",
    "            #     torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            #     layer_epoch[layer_index] += layer_timer.stop()\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            # forward_epoch += forward_timer.stop()\n",
    "            # loss\n",
    "            # loss_timer.start()\n",
    "            l = loss_fn(y_hat, y)\n",
    "            # backward\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            # loss_epoch += loss_timer.stop()\n",
    "            # backward_timer.start()\n",
    "            l.backward()\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            # backward_epoch += backward_timer.stop()\n",
    "            # optimize\n",
    "            # opt_timer.start()   \n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()  # 等待数据传输完成\n",
    "            # opt_epoch += opt_timer.stop()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_acc = metric[1] / metric[2]\n",
    "        # train_epoch = train_timer.stop()\n",
    "        # test_timer.start()\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        # testtime_epoch = test_timer.stop()\n",
    "        print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "        print('epoch %d, time %f sec' % (epoch+1, timer.sum()))\n",
    "        # store the time and acc data\n",
    "        # Epoch_time[epoch] = timer.stop()\n",
    "        # print(f'The total time of the {epoch} is:', Epoch_time[epoch])\n",
    "        # Layers_time[:, epoch] = layer_epoch.flatten()\n",
    "        # Train_part_time[:, epoch] = ttd_epoch, forward_epoch, loss_epoch, backward_epoch, opt_epoch, testtime_epoch\n",
    "        # Train_time[epoch] = train_epoch\n",
    "        # Train_acc[epoch] = train_acc\n",
    "        # Test_acc[epoch] = test_acc\n",
    "        # stop the nvidia-smi command\n",
    "        # nvidia_smi_process.terminate()\n",
    "        # calculate the energy consumption of each epoch\n",
    "        # GPU_df = pd.read_csv('gpu_power_usage.csv')\n",
    "        # for row in range(len(GPU_df)):\n",
    "        #     GPU_df.iloc[row,0] = GPU_df.iloc[row,0].replace(' W','')\n",
    "        # Consumption_df = GPU_df.astype(float)  \n",
    "        # EnergyDatai = Consumption_df.iloc[:,0].values # 将数据转换为numpy数组\n",
    "        # store the energy data\n",
    "        # Epoch_energy[epoch,0] = EnergyDatai\n",
    "    # 关闭 NVML\n",
    "    nvmlShutdown()\n",
    "    # return Layers_time, Train_part_time, Train_time, Epoch_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvml_sampling_thread(handle, data_queue, stop_event, sampling_interval=0.01):\n",
    "    while not stop_event.is_set():\n",
    "        current_time = time.time()\n",
    "        current_power = nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
    "        data_queue.put((current_time, current_power))\n",
    "        time.sleep(sampling_interval)\n",
    "\n",
    "# def integrate_power_over_interval(samples, start_time, end_time):\n",
    "#     # 对 [start_time, end_time] 区间内的功率样本进行积分（简单线性近似）\n",
    "#     # samples 是 (time, power) 的列表，假定按时间排序\n",
    "#     # 过滤出在 start_time 之前和 end_time 之后的样本\n",
    "#     filtered = [(t, p) for t, p in samples if start_time <= t <= end_time]\n",
    "#     if len(filtered) < 2:\n",
    "#         # 样本过少，无法良好估计，这时可以尝试从samples中找到距离start和end最近的点\n",
    "#         # 简化处理：返回0或尝试扩展为插值\n",
    "#         return 0.0\n",
    "\n",
    "def integrate_power_over_interval(samples, start_time, end_time):\n",
    "    # 假定 samples是按时间升序排序的 (t, p)\n",
    "    # 若未排序，请先排序:\n",
    "    # samples = sorted(samples, key=lambda x: x[0])\n",
    "    \n",
    "    def interpolate(samples, target_time):\n",
    "        # 在 samples 中找到 target_time 左右最近的两个点，并进行线性插值\n",
    "        # 若 target_time 恰好等于某个样本点时间，直接返回该点功率\n",
    "        # 若无法找到两侧点（如 target_time在样本时间轴外），根据情况返回None或边界点\n",
    "        n = len(samples)\n",
    "        if n == 0:\n",
    "            return None\n",
    "        # 若 target_time 小于第一个样本点时间，无法向左插值，这里直接返回第一个点的功率值(或None)\n",
    "        if target_time <= samples[0][0]:\n",
    "            # 简化处理：返回最早样本点的功率（或None）\n",
    "            return samples[0][1]\n",
    "        # 若 target_time 大于最后一个样本点时间，无法向右插值，返回最后一个点的功率（或None）\n",
    "        if target_time >= samples[-1][0]:\n",
    "            return samples[-1][1]\n",
    "\n",
    "        # 否则，在中间插值\n",
    "        # 使用二分查找快速定位\n",
    "        import bisect\n",
    "        times = [t for t, _ in samples]\n",
    "        pos = bisect.bisect_left(times, target_time)\n",
    "        # pos是使times保持有序插入target_time的位置\n",
    "        # 因为target_time不在已有样本点中，pos不会越界且pos>0且pos<n\n",
    "        t1, p1 = samples[pos-1]\n",
    "        t2, p2 = samples[pos]\n",
    "        # 线性插值： p = p1 + (p2 - p1)*((target_time - t1)/(t2 - t1))\n",
    "        ratio = (target_time - t1) / (t2 - t1)\n",
    "        p = p1 + (p2 - p1)*ratio\n",
    "        return p\n",
    "\n",
    "    # 从原始 samples 中筛选出位于[start_time, end_time]内的点\n",
    "    filtered = [(t, p) for t, p in samples if start_time <= t <= end_time]\n",
    "\n",
    "    # 如果不足2个点，则尝试使用插值\n",
    "    if len(filtered) < 2:\n",
    "        # 无论如何都需要在边界处插值出两个点(起码start和end)\n",
    "        start_power = interpolate(samples, start_time)\n",
    "        end_power = interpolate(samples, end_time)\n",
    "\n",
    "        # 如果从样本中无法插值出任何有意义的点（比如samples为空或无法插值），返回0.0\n",
    "        if start_power is None or end_power is None:\n",
    "            return 0.0\n",
    "\n",
    "        # 将插值的边界点加入到 filtered\n",
    "        # 注意：如果filtered中有一个点在区间内，我们也需要确保边界有两点以上\n",
    "        # 例如filtered只有一个点在中间，则需要在start和end插值点全部加入。\n",
    "        # 若filtered为空，则只用start/end两点插值点求积分\n",
    "        new_filtered = [(start_time, start_power)] + filtered + [(end_time, end_power)]\n",
    "        # 确保按时间排序\n",
    "        new_filtered.sort(key=lambda x: x[0])\n",
    "        filtered = new_filtered\n",
    "\n",
    "    # 正常积分计算\n",
    "    if len(filtered) < 2:\n",
    "        # 经过插值仍不够，返回0\n",
    "        return 0.0\n",
    "\n",
    "    total_energy = 0.0\n",
    "    for i in range(len(filtered)-1):\n",
    "        t1, p1 = filtered[i]\n",
    "        t2, p2 = filtered[i+1]\n",
    "        dt = t2 - t1\n",
    "        avg_p = (p1 + p2)/2.0\n",
    "        total_energy += avg_p * dt\n",
    "\n",
    "    return total_energy\n",
    "    \n",
    "\n",
    "# def train_func(net, train_iter, test_iter, alexlayer, num_epochs, lr, device):\n",
    "def train_func(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    # print(f'The name of the layers are: {alexlayer}')\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 初始化NVML和采样线程\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    power_data_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "    sampling_interval = 0.005\n",
    "    sampler_thread = threading.Thread(target=nvml_sampling_thread, args=(handle, power_data_queue, stop_event, sampling_interval))\n",
    "    sampler_thread.start()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('The epoch is:', epoch+1)\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "        to_device_intervals = []  # 用来记录本epoch每个batch的to_device时间段\n",
    "        forward_intervals = []  # 用来记录本epoch每个batch的forward时间段\n",
    "        backward_intervals = []  # \n",
    "\n",
    "        to_device_time_consump_total = 0\n",
    "        forward_time_consump_total = 0\n",
    "        backward_time_consump_total = 0\n",
    "\n",
    "        to_device_energy_consump_total = 0\n",
    "        forward_energy_consump_total = 0\n",
    "        backward_energy_consump_total = 0\n",
    "        \n",
    "\n",
    "        # 记录epoch开始时队列中已有的数据条数，用于后面区分本epoch的数据\n",
    "        initial_queue_size = power_data_queue.qsize()\n",
    "\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            print('The batch is:', i+1)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 记录to_device前后的时间戳\n",
    "            start_ttd_time = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            end_ttd_time = time.time()\n",
    "            to_device_intervals.append((start_ttd_time, end_ttd_time))\n",
    "            ttd_time_consump = end_ttd_time - start_ttd_time\n",
    "            to_device_time_consump_total += ttd_time_consump\n",
    "\n",
    "            # time.sleep(0.5)\n",
    "            # forward\n",
    "            start_forward_time = time.time()\n",
    "            y_hat = net(X)\n",
    "            torch.cuda.synchronize()\n",
    "            end_forward_time = time.time()\n",
    "            forward_intervals.append((start_forward_time, end_forward_time))\n",
    "            forward_time_consump = end_forward_time - start_forward_time\n",
    "            forward_time_consump_total += forward_time_consump\n",
    "\n",
    "            l = loss_fn(y_hat, y)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start_backward_time = time.time()\n",
    "            l.backward()\n",
    "            torch.cuda.synchronize()\n",
    "            end_backward_time = time.time()\n",
    "            backward_intervals.append((start_backward_time, end_backward_time))\n",
    "            backward_time_consump = end_backward_time - start_backward_time\n",
    "            backward_time_consump_total += backward_time_consump\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "\n",
    "        # 从队列中取出本epoch采样数据\n",
    "        epoch_samples = []\n",
    "        # 这里取出initial_queue_size之后加入的所有数据\n",
    "        total_samples_in_epoch = power_data_queue.qsize() - initial_queue_size\n",
    "        for _ in range(total_samples_in_epoch):\n",
    "            epoch_samples.append(power_data_queue.get())\n",
    "\n",
    "        # 对epoch中每个batch的to_device间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(to_device_intervals):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            to_device_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, to_device Energy: {batch_energy} J\")\n",
    "        \n",
    "        # 对epoch中每个batch的forward间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(forward_intervals):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            forward_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, forward Energy: {batch_energy} J\")\n",
    "\n",
    "        # 对epoch中每个batch的backward间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(backward_intervals):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            backward_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, backward Energy: {batch_energy} J\")\n",
    "        \n",
    "\n",
    "        print(f'The total energy consumption in to_device part is: {to_device_energy_consump_total}, and the total time consumed is: {to_device_time_consump_total}.')\n",
    "        print(f'The total energy consumption in forward part is: {forward_energy_consump_total}, and the total time consumed is: {forward_time_consump_total}.')\n",
    "        print(f'The total energy consumption in backward part is: {backward_energy_consump_total}, and the total time consumed is: {backward_time_consump_total}')\n",
    "        # print(f'The total time consumed is: {to_device_time_consump_total}.')\n",
    "        # print(f'The total time consumed is: {forward_time_consump_total}.')\n",
    "        # print(f'The total time consumed is: {backward_time_consump_total}')\n",
    "\n",
    "\n",
    "    # 训练结束后关闭线程\n",
    "    stop_event.set()\n",
    "    sampler_thread.join()\n",
    "\n",
    "    nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用示例\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 初始化 NVML\n",
    "#     nvmlInit()\n",
    "#     handle = nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "#     # 创建一个队列来存储功率数据\n",
    "#     power_data_queue = queue.Queue()\n",
    "\n",
    "#     # 创建一个线程停止事件\n",
    "#     stop_event = threading.Event()\n",
    "\n",
    "#     # 启动采样线程，每10ms获取一次功耗数据\n",
    "#     sampling_interval = 0.005\n",
    "#     sampler_thread = threading.Thread(target=nvml_sampling_thread, args=(handle, power_data_queue, stop_event, sampling_interval))\n",
    "#     sampler_thread.start()\n",
    "\n",
    "#     # 模拟训练过程\n",
    "#     # 在真实使用中,这里是你的训练主循环\n",
    "#     print(\"Training started...\")\n",
    "#     for epoch in range(3):\n",
    "#         print(f\"Epoch {epoch+1}...\")\n",
    "#         time.sleep(2)  # 模拟训练耗时\n",
    "#     print(\"Training finished.\")\n",
    "\n",
    "#     # 训练结束后，通知线程停止并等待其结束\n",
    "#     stop_event.set()\n",
    "#     sampler_thread.join()\n",
    "\n",
    "#     # 获取所有采样数据\n",
    "#     sampled_data = []\n",
    "#     while not power_data_queue.empty():\n",
    "#         sampled_data.append(power_data_queue.get())\n",
    "\n",
    "#     # 处理数据 (示例：打印前10条数据)\n",
    "#     print(\"Sampled power data (first 10):\", sampled_data[:10])\n",
    "\n",
    "#     # 关闭 NVML\n",
    "#     nvmlShutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_f(main_folder, batch_size, num_epochs, round, lr, device):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    # # 判断文件是否存在\n",
    "    # if epoch_batch_folder.exists():\n",
    "    #     print(\"文件存在。\")\n",
    "    # else:\n",
    "    #     os.makedirs(epoch_batch_folder)\n",
    "    #     print(\"文件不存在，已创建。\")\n",
    "    #     print(\"文件创建于：\", epoch_batch_folder)\n",
    "    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "    # show the shape of the data\n",
    "    list_of_i = []\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        if i < 3:\n",
    "            print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "        else:\n",
    "            pass\n",
    "        list_of_i.append(i)\n",
    "    print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "    train_func(resnet_f, train_iter, test_iter, num_epochs, lr, device)\n",
    "    # Layers_time, Train_part_time, Train_time, Train_acc, Test_acc, \\\n",
    "    #     Epoch_time, Epoch_energy = train_func(resnet_f, train_iter, test_iter, LayerName, block_num, num_epochs, lr, device)\n",
    "    # # save the data\n",
    "    # np.save(epoch_batch_folder/'Layers_time.npy', Layers_time)\n",
    "    # np.save(epoch_batch_folder/'Train_part_time.npy', Train_part_time)\n",
    "    # np.save(epoch_batch_folder/'Train_time.npy', Train_time)\n",
    "    # np.save(epoch_batch_folder/'Train_acc.npy', Train_acc)\n",
    "    # np.save(epoch_batch_folder/'Test_acc.npy', Test_acc)\n",
    "    # np.save(epoch_batch_folder/'Epoch_time.npy', Epoch_time)\n",
    "    # np.save(epoch_batch_folder/'Epoch_energy.npy', Epoch_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_f(main_folder, batch_size, num_epochs, round, lr, device):\n",
    "#     print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "#     # create the folder to store the data\n",
    "#     epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "#     # 判断文件是否存在\n",
    "#     if epoch_batch_folder.exists():\n",
    "#         print(\"文件存在。\")\n",
    "#     else:\n",
    "#         os.makedirs(epoch_batch_folder)\n",
    "#         print(\"文件不存在，已创建。\")\n",
    "#         print(\"文件创建于：\", epoch_batch_folder)\n",
    "#         train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "#         # show the shape of the data\n",
    "#         list_of_i = []\n",
    "#         for i, (X, y) in enumerate(train_iter):\n",
    "#             if i < 3:\n",
    "#                 print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "#             else:\n",
    "#                 pass\n",
    "#             list_of_i.append(i)\n",
    "#         print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "#         train_func(alexnet_f, train_iter, test_iter, num_epochs, lr, device)\n",
    "#         # Layers_time, Train_part_time, Train_time, Train_acc, Test_acc, \\\n",
    "#         #     Epoch_time, Epoch_energy = train_func(alexnet_f, train_iter, test_iter, alexlayer, num_epochs, lr, device)\n",
    "#         # save the data\n",
    "#         # np.save(epoch_batch_folder/'Layers_time.npy', Layers_time)\n",
    "#         # np.save(epoch_batch_folder/'Train_part_time.npy', Train_part_time)\n",
    "#         # np.save(epoch_batch_folder/'Train_time.npy', Train_time)\n",
    "#         # np.save(epoch_batch_folder/'Epoch_time.npy', Epoch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_model_f(main_folder, batch_size, num_epochs, round, lr, device):\\n    print(f\\'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running\\')\\n    # create the folder to store the data\\n    epoch_batch_folder = main_folder/f\\'E{num_epochs}_B{batch_size}_R{round}\\'\\n    # 判断文件是否存在\\n    if epoch_batch_folder.exists():\\n        print(\"文件存在。\")\\n    else:\\n        os.makedirs(epoch_batch_folder)\\n        print(\"文件不存在，已创建。\")\\n        print(\"文件创建于：\", epoch_batch_folder)\\n        train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\\n        # show the shape of the data\\n        list_of_i = []\\n        for i, (X, y) in enumerate(train_iter):\\n            if i < 3:\\n                print(\\'the shape of the\\', i, \\'batch of the train_iter is:\\', X.shape)\\n            else:\\n                pass\\n            list_of_i.append(i)\\n        print(f\\'The number of batches is: {np.array(list_of_i).shape}\\')\\n        Layers_time, Train_part_time, Train_time, Train_acc, Test_acc,             Epoch_time, Epoch_energy = train_func(alexnet_f, train_iter, test_iter, alexlayer, num_epochs, lr, device)\\n        # save the data\\n        np.save(epoch_batch_folder/\\'Layers_time.npy\\', Layers_time)\\n        np.save(epoch_batch_folder/\\'Train_part_time.npy\\', Train_part_time)\\n        np.save(epoch_batch_folder/\\'Train_time.npy\\', Train_time)\\n        np.save(epoch_batch_folder/\\'Train_acc.npy\\', Train_acc)\\n        np.save(epoch_batch_folder/\\'Test_acc.npy\\', Test_acc)\\n        np.save(epoch_batch_folder/\\'Epoch_time.npy\\', Epoch_time)\\n        np.save(epoch_batch_folder/\\'Epoch_energy.npy\\', Epoch_energy)\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_model_f(main_folder, batch_size, num_epochs, round, lr, device):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    # 判断文件是否存在\n",
    "    if epoch_batch_folder.exists():\n",
    "        print(\"文件存在。\")\n",
    "    else:\n",
    "        os.makedirs(epoch_batch_folder)\n",
    "        print(\"文件不存在，已创建。\")\n",
    "        print(\"文件创建于：\", epoch_batch_folder)\n",
    "        train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "        # show the shape of the data\n",
    "        list_of_i = []\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            if i < 3:\n",
    "                print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "            else:\n",
    "                pass\n",
    "            list_of_i.append(i)\n",
    "        print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "        Layers_time, Train_part_time, Train_time, Train_acc, Test_acc, \\\n",
    "            Epoch_time, Epoch_energy = train_func(alexnet_f, train_iter, test_iter, alexlayer, num_epochs, lr, device)\n",
    "        # save the data\n",
    "        np.save(epoch_batch_folder/'Layers_time.npy', Layers_time)\n",
    "        np.save(epoch_batch_folder/'Train_part_time.npy', Train_part_time)\n",
    "        np.save(epoch_batch_folder/'Train_time.npy', Train_time)\n",
    "        np.save(epoch_batch_folder/'Train_acc.npy', Train_acc)\n",
    "        np.save(epoch_batch_folder/'Test_acc.npy', Test_acc)\n",
    "        np.save(epoch_batch_folder/'Epoch_time.npy', Epoch_time)\n",
    "        np.save(epoch_batch_folder/'Epoch_energy.npy', Epoch_energy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_model_c(main_folder, batch_size, num_epochs, round, lr, device):\\n    print(f\\'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running\\')\\n    # create the folder to store the data\\n    epoch_batch_folder = main_folder/f\\'E{num_epochs}_B{batch_size}_R{round}\\'\\n    # 判断文件是否存在\\n    if epoch_batch_folder.exists():\\n        print(\"文件存在。\")\\n        pass\\n    else:\\n        os.makedirs(epoch_batch_folder)\\n        print(\"文件不存在，已创建。\")\\n        print(\"文件创建于：\", epoch_batch_folder)\\n        train_iter, test_iter = load_data_cifar100(batch_size, resize=224)\\n        # show the shape of the data\\n        list_of_i = []\\n        for i, (X, y) in enumerate(train_iter):\\n            if i < 3:\\n                print(\\'the shape of the\\', i, \\'batch of the train_iter is:\\', X.shape)\\n            else:\\n                pass\\n            list_of_i.append(i)\\n        print(f\\'The number of batches is: {np.array(list_of_i).shape}\\')\\n        Layers_time, Train_part_time, Train_time, Train_acc, Test_acc,             Epoch_time, Epoch_energy = train_func(alexnet_c, train_iter, test_iter, alexlayer, num_epochs, lr, device)\\n        # save the data\\n        np.save(epoch_batch_folder/\\'Layers_time.npy\\', Layers_time)\\n        np.save(epoch_batch_folder/\\'Train_part_time.npy\\', Train_part_time)\\n        np.save(epoch_batch_folder/\\'Train_time.npy\\', Train_time)\\n        np.save(epoch_batch_folder/\\'Train_acc.npy\\', Train_acc)\\n        np.save(epoch_batch_folder/\\'Test_acc.npy\\', Test_acc)\\n        np.save(epoch_batch_folder/\\'Epoch_time.npy\\', Epoch_time)\\n        np.save(epoch_batch_folder/\\'Epoch_energy.npy\\', Epoch_energy)\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_model_c(main_folder, batch_size, num_epochs, round, lr, device):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    # 判断文件是否存在\n",
    "    if epoch_batch_folder.exists():\n",
    "        print(\"文件存在。\")\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(epoch_batch_folder)\n",
    "        print(\"文件不存在，已创建。\")\n",
    "        print(\"文件创建于：\", epoch_batch_folder)\n",
    "        train_iter, test_iter = load_data_cifar100(batch_size, resize=224)\n",
    "        # show the shape of the data\n",
    "        list_of_i = []\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            if i < 3:\n",
    "                print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "            else:\n",
    "                pass\n",
    "            list_of_i.append(i)\n",
    "        print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "        Layers_time, Train_part_time, Train_time, Train_acc, Test_acc, \\\n",
    "            Epoch_time, Epoch_energy = train_func(alexnet_c, train_iter, test_iter, alexlayer, num_epochs, lr, device)\n",
    "        # save the data\n",
    "        np.save(epoch_batch_folder/'Layers_time.npy', Layers_time)\n",
    "        np.save(epoch_batch_folder/'Train_part_time.npy', Train_part_time)\n",
    "        np.save(epoch_batch_folder/'Train_time.npy', Train_time)\n",
    "        np.save(epoch_batch_folder/'Train_acc.npy', Train_acc)\n",
    "        np.save(epoch_batch_folder/'Test_acc.npy', Test_acc)\n",
    "        np.save(epoch_batch_folder/'Epoch_time.npy', Epoch_time)\n",
    "        np.save(epoch_batch_folder/'Epoch_energy.npy', Epoch_energy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_model_c10(main_folder, batch_size, num_epochs, round, lr, device):\\n    print(f\\'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running\\')\\n    # create the folder to store the data\\n    epoch_batch_folder = main_folder/f\\'E{num_epochs}_B{batch_size}_R{round}\\'\\n    # 判断文件是否存在\\n    if epoch_batch_folder.exists():\\n        print(\"文件存在。\")\\n        pass\\n    else:\\n        os.makedirs(epoch_batch_folder)\\n        print(\"文件不存在，已创建。\")\\n        print(\"文件创建于：\", epoch_batch_folder)\\n        train_iter, test_iter = load_data_cifar10(batch_size, resize=224)\\n        # show the shape of the data\\n        list_of_i = []\\n        for i, (X, y) in enumerate(train_iter):\\n            if i < 3:\\n                print(\\'the shape of the\\', i, \\'batch of the train_iter is:\\', X.shape)\\n            else:\\n                pass\\n            list_of_i.append(i)\\n        print(f\\'The number of batches is: {np.array(list_of_i).shape}\\')\\n        Layers_time, Train_part_time, Train_time, Train_acc, Test_acc,             Epoch_time, Epoch_energy = train_func(alexnet_c, train_iter, test_iter, alexlayer, num_epochs, lr, device)\\n        # save the data\\n        np.save(epoch_batch_folder/\\'Layers_time.npy\\', Layers_time)\\n        np.save(epoch_batch_folder/\\'Train_part_time.npy\\', Train_part_time)\\n        np.save(epoch_batch_folder/\\'Train_time.npy\\', Train_time)\\n        np.save(epoch_batch_folder/\\'Train_acc.npy\\', Train_acc)\\n        np.save(epoch_batch_folder/\\'Test_acc.npy\\', Test_acc)\\n        np.save(epoch_batch_folder/\\'Epoch_time.npy\\', Epoch_time)\\n        np.save(epoch_batch_folder/\\'Epoch_energy.npy\\', Epoch_energy)\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_model_c10(main_folder, batch_size, num_epochs, round, lr, device):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    # 判断文件是否存在\n",
    "    if epoch_batch_folder.exists():\n",
    "        print(\"文件存在。\")\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(epoch_batch_folder)\n",
    "        print(\"文件不存在，已创建。\")\n",
    "        print(\"文件创建于：\", epoch_batch_folder)\n",
    "        train_iter, test_iter = load_data_cifar10(batch_size, resize=224)\n",
    "        # show the shape of the data\n",
    "        list_of_i = []\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            if i < 3:\n",
    "                print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "            else:\n",
    "                pass\n",
    "            list_of_i.append(i)\n",
    "        print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "        Layers_time, Train_part_time, Train_time, Train_acc, Test_acc, \\\n",
    "            Epoch_time, Epoch_energy = train_func(alexnet_c, train_iter, test_iter, alexlayer, num_epochs, lr, device)\n",
    "        # save the data\n",
    "        np.save(epoch_batch_folder/'Layers_time.npy', Layers_time)\n",
    "        np.save(epoch_batch_folder/'Train_part_time.npy', Train_part_time)\n",
    "        np.save(epoch_batch_folder/'Train_time.npy', Train_time)\n",
    "        np.save(epoch_batch_folder/'Train_acc.npy', Train_acc)\n",
    "        np.save(epoch_batch_folder/'Test_acc.npy', Test_acc)\n",
    "        np.save(epoch_batch_folder/'Epoch_time.npy', Epoch_time)\n",
    "        np.save(epoch_batch_folder/'Epoch_energy.npy', Epoch_energy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cuda\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('The device is:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is: /home/GreenAI/3080/Test_data\n",
      "文件存在。\n",
      "The epoch is set: 1, batch is set: 256, is in 1th running\n",
      "the shape of the 0 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 1 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 2 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches is: (235,)\n",
      "training on cuda\n",
      "The epoch is: 1\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.766, test acc 0.801\n",
      "Epoch 1, Batch 1, to_device Energy: 0.7799364095926284 J\n",
      "Epoch 1, Batch 2, to_device Energy: 0.6048459796905518 J\n",
      "Epoch 1, Batch 3, to_device Energy: 0.5373246726989747 J\n",
      "Epoch 1, Batch 4, to_device Energy: 1.0291320562362671 J\n",
      "Epoch 1, Batch 5, to_device Energy: 2.422606656551361 J\n",
      "Epoch 1, Batch 6, to_device Energy: 1.4946311945915223 J\n",
      "Epoch 1, Batch 7, to_device Energy: 1.525512229681015 J\n",
      "Epoch 1, Batch 8, to_device Energy: 2.42831330037117 J\n",
      "Epoch 1, Batch 9, to_device Energy: 2.990097345113754 J\n",
      "Epoch 1, Batch 10, to_device Energy: 1.4945223331451416 J\n",
      "Epoch 1, Batch 11, to_device Energy: 2.535418737411499 J\n",
      "Epoch 1, Batch 12, to_device Energy: 1.4899804422855378 J\n",
      "Epoch 1, Batch 13, to_device Energy: 1.4757718963623045 J\n",
      "Epoch 1, Batch 14, to_device Energy: 1.5222947716712951 J\n",
      "Epoch 1, Batch 15, to_device Energy: 1.4874501907825468 J\n",
      "Epoch 1, Batch 16, to_device Energy: 1.4976261091232301 J\n",
      "Epoch 1, Batch 17, to_device Energy: 1.5076969919204712 J\n",
      "Epoch 1, Batch 18, to_device Energy: 1.4642427535057068 J\n",
      "Epoch 1, Batch 19, to_device Energy: 1.5199926495552063 J\n",
      "Epoch 1, Batch 20, to_device Energy: 1.5325386250019073 J\n",
      "Epoch 1, Batch 21, to_device Energy: 1.4606589138507844 J\n",
      "Epoch 1, Batch 22, to_device Energy: 1.548474271059036 J\n",
      "Epoch 1, Batch 23, to_device Energy: 1.5015760457515717 J\n",
      "Epoch 1, Batch 24, to_device Energy: 1.5098613715171814 J\n",
      "Epoch 1, Batch 25, to_device Energy: 1.5054583349227904 J\n",
      "Epoch 1, Batch 26, to_device Energy: 1.509560764312744 J\n",
      "Epoch 1, Batch 27, to_device Energy: 2.464626889228821 J\n",
      "Epoch 1, Batch 28, to_device Energy: 1.5204531157016754 J\n",
      "Epoch 1, Batch 29, to_device Energy: 2.9920360565185544 J\n",
      "Epoch 1, Batch 30, to_device Energy: 1.4722197289466856 J\n",
      "Epoch 1, Batch 31, to_device Energy: 1.522754035949707 J\n",
      "Epoch 1, Batch 32, to_device Energy: 1.4927718830108643 J\n",
      "Epoch 1, Batch 33, to_device Energy: 2.9257393798828124 J\n",
      "Epoch 1, Batch 34, to_device Energy: 1.499075028181076 J\n",
      "Epoch 1, Batch 35, to_device Energy: 1.4896878652572632 J\n",
      "Epoch 1, Batch 36, to_device Energy: 1.4613694155216215 J\n",
      "Epoch 1, Batch 37, to_device Energy: 1.472074568271637 J\n",
      "Epoch 1, Batch 38, to_device Energy: 2.346695425987244 J\n",
      "Epoch 1, Batch 39, to_device Energy: 1.4484231460094452 J\n",
      "Epoch 1, Batch 40, to_device Energy: 3.0298925781250006 J\n",
      "Epoch 1, Batch 41, to_device Energy: 2.632470093727112 J\n",
      "Epoch 1, Batch 42, to_device Energy: 2.4750351905822754 J\n",
      "Epoch 1, Batch 43, to_device Energy: 1.5038585815429688 J\n",
      "Epoch 1, Batch 44, to_device Energy: 1.5073907890319826 J\n",
      "Epoch 1, Batch 45, to_device Energy: 1.464402982711792 J\n",
      "Epoch 1, Batch 46, to_device Energy: 1.5340022842884065 J\n",
      "Epoch 1, Batch 47, to_device Energy: 1.4813229315280916 J\n",
      "Epoch 1, Batch 48, to_device Energy: 1.4728064942359924 J\n",
      "Epoch 1, Batch 49, to_device Energy: 1.5289807832241058 J\n",
      "Epoch 1, Batch 50, to_device Energy: 1.4968776075839996 J\n",
      "Epoch 1, Batch 51, to_device Energy: 1.506973443031311 J\n",
      "Epoch 1, Batch 52, to_device Energy: 1.5527434730529783 J\n",
      "Epoch 1, Batch 53, to_device Energy: 2.4780611038208007 J\n",
      "Epoch 1, Batch 54, to_device Energy: 2.347800145140149 J\n",
      "Epoch 1, Batch 55, to_device Energy: 1.4692154459953308 J\n",
      "Epoch 1, Batch 56, to_device Energy: 1.5216144061088563 J\n",
      "Epoch 1, Batch 57, to_device Energy: 1.4593287386894227 J\n",
      "Epoch 1, Batch 58, to_device Energy: 1.502815135717392 J\n",
      "Epoch 1, Batch 59, to_device Energy: 1.4942689576148986 J\n",
      "Epoch 1, Batch 60, to_device Energy: 1.4544675276279448 J\n",
      "Epoch 1, Batch 61, to_device Energy: 1.5004748845100404 J\n",
      "Epoch 1, Batch 62, to_device Energy: 1.541645975112915 J\n",
      "Epoch 1, Batch 63, to_device Energy: 1.5607562227249145 J\n",
      "Epoch 1, Batch 64, to_device Energy: 1.5072614135742188 J\n",
      "Epoch 1, Batch 65, to_device Energy: 1.4999256134033203 J\n",
      "Epoch 1, Batch 66, to_device Energy: 1.5057214260101317 J\n",
      "Epoch 1, Batch 67, to_device Energy: 1.5305767879486085 J\n",
      "Epoch 1, Batch 68, to_device Energy: 1.5029045650959014 J\n",
      "Epoch 1, Batch 69, to_device Energy: 1.4806004583835601 J\n",
      "Epoch 1, Batch 70, to_device Energy: 1.5128537311553956 J\n",
      "Epoch 1, Batch 71, to_device Energy: 1.509888038635254 J\n",
      "Epoch 1, Batch 72, to_device Energy: 1.475609881401062 J\n",
      "Epoch 1, Batch 73, to_device Energy: 2.774458641052246 J\n",
      "Epoch 1, Batch 74, to_device Energy: 1.5285383248329163 J\n",
      "Epoch 1, Batch 75, to_device Energy: 1.4677739775180818 J\n",
      "Epoch 1, Batch 76, to_device Energy: 1.5113992824554443 J\n",
      "Epoch 1, Batch 77, to_device Energy: 1.5252693138122557 J\n",
      "Epoch 1, Batch 78, to_device Energy: 1.4692901611328124 J\n",
      "Epoch 1, Batch 79, to_device Energy: 2.5518374767303467 J\n",
      "Epoch 1, Batch 80, to_device Energy: 1.5053881211280822 J\n",
      "Epoch 1, Batch 81, to_device Energy: 1.4737106058597564 J\n",
      "Epoch 1, Batch 82, to_device Energy: 1.5012515053749083 J\n",
      "Epoch 1, Batch 83, to_device Energy: 2.7725942900180813 J\n",
      "Epoch 1, Batch 84, to_device Energy: 1.5103249111175536 J\n",
      "Epoch 1, Batch 85, to_device Energy: 1.5180156877040862 J\n",
      "Epoch 1, Batch 86, to_device Energy: 1.5087459669113161 J\n",
      "Epoch 1, Batch 87, to_device Energy: 1.4770723452568053 J\n",
      "Epoch 1, Batch 88, to_device Energy: 1.512987519979477 J\n",
      "Epoch 1, Batch 89, to_device Energy: 1.5006899981498718 J\n",
      "Epoch 1, Batch 90, to_device Energy: 1.4754946281909942 J\n",
      "Epoch 1, Batch 91, to_device Energy: 1.5072515046596526 J\n",
      "Epoch 1, Batch 92, to_device Energy: 1.5422730460166931 J\n",
      "Epoch 1, Batch 93, to_device Energy: 1.4777050256729127 J\n",
      "Epoch 1, Batch 94, to_device Energy: 3.0543905687332153 J\n",
      "Epoch 1, Batch 95, to_device Energy: 1.4939605677127838 J\n",
      "Epoch 1, Batch 96, to_device Energy: 3.028367757797241 J\n",
      "Epoch 1, Batch 97, to_device Energy: 1.553172693014145 J\n",
      "Epoch 1, Batch 98, to_device Energy: 1.4864915323257446 J\n",
      "Epoch 1, Batch 99, to_device Energy: 3.1231894330978394 J\n",
      "Epoch 1, Batch 100, to_device Energy: 1.507729461669922 J\n",
      "Epoch 1, Batch 101, to_device Energy: 1.5176562485694884 J\n",
      "Epoch 1, Batch 102, to_device Energy: 1.5041487560272215 J\n",
      "Epoch 1, Batch 103, to_device Energy: 2.628822250366211 J\n",
      "Epoch 1, Batch 104, to_device Energy: 1.521748412132263 J\n",
      "Epoch 1, Batch 105, to_device Energy: 1.5090238378047942 J\n",
      "Epoch 1, Batch 106, to_device Energy: 1.5331615047454832 J\n",
      "Epoch 1, Batch 107, to_device Energy: 3.020429050445556 J\n",
      "Epoch 1, Batch 108, to_device Energy: 1.5235633254051208 J\n",
      "Epoch 1, Batch 109, to_device Energy: 1.4744458508491518 J\n",
      "Epoch 1, Batch 110, to_device Energy: 1.4976091132164002 J\n",
      "Epoch 1, Batch 111, to_device Energy: 1.4984539813995361 J\n",
      "Epoch 1, Batch 112, to_device Energy: 1.482876627445221 J\n",
      "Epoch 1, Batch 113, to_device Energy: 1.540119924545288 J\n",
      "Epoch 1, Batch 114, to_device Energy: 1.5389915418624878 J\n",
      "Epoch 1, Batch 115, to_device Energy: 1.4852047362327576 J\n",
      "Epoch 1, Batch 116, to_device Energy: 1.5221823501586915 J\n",
      "Epoch 1, Batch 117, to_device Energy: 1.5172404670715331 J\n",
      "Epoch 1, Batch 118, to_device Energy: 2.6670575710144493 J\n",
      "Epoch 1, Batch 119, to_device Energy: 1.5264431610107423 J\n",
      "Epoch 1, Batch 120, to_device Energy: 1.5094273524284363 J\n",
      "Epoch 1, Batch 121, to_device Energy: 1.5367466032505035 J\n",
      "Epoch 1, Batch 122, to_device Energy: 1.5128628070354462 J\n",
      "Epoch 1, Batch 123, to_device Energy: 1.4895741996765137 J\n",
      "Epoch 1, Batch 124, to_device Energy: 1.5186132278442384 J\n",
      "Epoch 1, Batch 125, to_device Energy: 1.5110487132072448 J\n",
      "Epoch 1, Batch 126, to_device Energy: 1.4886521100997925 J\n",
      "Epoch 1, Batch 127, to_device Energy: 1.5337807588577272 J\n",
      "Epoch 1, Batch 128, to_device Energy: 1.5443442120552062 J\n",
      "Epoch 1, Batch 129, to_device Energy: 1.4806369171142577 J\n",
      "Epoch 1, Batch 130, to_device Energy: 1.5388634548187257 J\n",
      "Epoch 1, Batch 131, to_device Energy: 1.51818546295166 J\n",
      "Epoch 1, Batch 132, to_device Energy: 1.4922309279441834 J\n",
      "Epoch 1, Batch 133, to_device Energy: 1.5644203956127167 J\n",
      "Epoch 1, Batch 134, to_device Energy: 2.6159068250656126 J\n",
      "Epoch 1, Batch 135, to_device Energy: 1.485909919977188 J\n",
      "Epoch 1, Batch 136, to_device Energy: 1.5085578060150147 J\n",
      "Epoch 1, Batch 137, to_device Energy: 1.5167800483703613 J\n",
      "Epoch 1, Batch 138, to_device Energy: 1.4920340585708618 J\n",
      "Epoch 1, Batch 139, to_device Energy: 1.554704558134079 J\n",
      "Epoch 1, Batch 140, to_device Energy: 1.533018193721771 J\n",
      "Epoch 1, Batch 141, to_device Energy: 2.7894285185337067 J\n",
      "Epoch 1, Batch 142, to_device Energy: 3.0792590057849885 J\n",
      "Epoch 1, Batch 143, to_device Energy: 1.5201719856262206 J\n",
      "Epoch 1, Batch 144, to_device Energy: 1.4881339037418366 J\n",
      "Epoch 1, Batch 145, to_device Energy: 1.556870913505554 J\n",
      "Epoch 1, Batch 146, to_device Energy: 1.5094715194702146 J\n",
      "Epoch 1, Batch 147, to_device Energy: 1.481273185968399 J\n",
      "Epoch 1, Batch 148, to_device Energy: 1.523756547451019 J\n",
      "Epoch 1, Batch 149, to_device Energy: 1.529430459022522 J\n",
      "Epoch 1, Batch 150, to_device Energy: 1.4883210525512696 J\n",
      "Epoch 1, Batch 151, to_device Energy: 1.539295703172684 J\n",
      "Epoch 1, Batch 152, to_device Energy: 1.5122098278999327 J\n",
      "Epoch 1, Batch 153, to_device Energy: 2.464617905139923 J\n",
      "Epoch 1, Batch 154, to_device Energy: 1.5735024390220642 J\n",
      "Epoch 1, Batch 155, to_device Energy: 1.5273480098247527 J\n",
      "Epoch 1, Batch 156, to_device Energy: 1.542504454135895 J\n",
      "Epoch 1, Batch 157, to_device Energy: 1.5901487474441527 J\n",
      "Epoch 1, Batch 158, to_device Energy: 1.5302322664260866 J\n",
      "Epoch 1, Batch 159, to_device Energy: 1.5453004007339477 J\n",
      "Epoch 1, Batch 160, to_device Energy: 1.5148897743225098 J\n",
      "Epoch 1, Batch 161, to_device Energy: 1.5986668331623077 J\n",
      "Epoch 1, Batch 162, to_device Energy: 1.5152134380340576 J\n",
      "Epoch 1, Batch 163, to_device Energy: 2.7206631068724234 J\n",
      "Epoch 1, Batch 164, to_device Energy: 1.5732814993858337 J\n",
      "Epoch 1, Batch 165, to_device Energy: 1.5415876901149748 J\n",
      "Epoch 1, Batch 166, to_device Energy: 1.5288540551662446 J\n",
      "Epoch 1, Batch 167, to_device Energy: 2.6620062708854673 J\n",
      "Epoch 1, Batch 168, to_device Energy: 1.5147244527339936 J\n",
      "Epoch 1, Batch 169, to_device Energy: 2.4099169921875 J\n",
      "Epoch 1, Batch 170, to_device Energy: 1.5573235194683075 J\n",
      "Epoch 1, Batch 171, to_device Energy: 1.5304728279113768 J\n",
      "Epoch 1, Batch 172, to_device Energy: 1.5020012741088866 J\n",
      "Epoch 1, Batch 173, to_device Energy: 1.5603649826049806 J\n",
      "Epoch 1, Batch 174, to_device Energy: 1.5188207626342776 J\n",
      "Epoch 1, Batch 175, to_device Energy: 1.5223380556106567 J\n",
      "Epoch 1, Batch 176, to_device Energy: 1.56381511759758 J\n",
      "Epoch 1, Batch 177, to_device Energy: 2.7036503100395204 J\n",
      "Epoch 1, Batch 178, to_device Energy: 1.5287286806106568 J\n",
      "Epoch 1, Batch 179, to_device Energy: 2.9968920514583584 J\n",
      "Epoch 1, Batch 180, to_device Energy: 1.5215672664642335 J\n",
      "Epoch 1, Batch 181, to_device Energy: 1.5190263030529023 J\n",
      "Epoch 1, Batch 182, to_device Energy: 1.5837627792358397 J\n",
      "Epoch 1, Batch 183, to_device Energy: 1.559046314239502 J\n",
      "Epoch 1, Batch 184, to_device Energy: 1.5372221221923827 J\n",
      "Epoch 1, Batch 185, to_device Energy: 1.5384799110889433 J\n",
      "Epoch 1, Batch 186, to_device Energy: 1.5169181578159332 J\n",
      "Epoch 1, Batch 187, to_device Energy: 1.5328369232416155 J\n",
      "Epoch 1, Batch 188, to_device Energy: 1.545657445907593 J\n",
      "Epoch 1, Batch 189, to_device Energy: 1.5679776515960693 J\n",
      "Epoch 1, Batch 190, to_device Energy: 1.5402936398983 J\n",
      "Epoch 1, Batch 191, to_device Energy: 3.1228732814788813 J\n",
      "Epoch 1, Batch 192, to_device Energy: 2.8334618022441864 J\n",
      "Epoch 1, Batch 193, to_device Energy: 1.5434363985061645 J\n",
      "Epoch 1, Batch 194, to_device Energy: 1.5350071263313292 J\n",
      "Epoch 1, Batch 195, to_device Energy: 1.5544757952690125 J\n",
      "Epoch 1, Batch 196, to_device Energy: 1.5468155140876771 J\n",
      "Epoch 1, Batch 197, to_device Energy: 1.5480140895843506 J\n",
      "Epoch 1, Batch 198, to_device Energy: 1.491619532585144 J\n",
      "Epoch 1, Batch 199, to_device Energy: 1.5857063369750977 J\n",
      "Epoch 1, Batch 200, to_device Energy: 2.605164813995361 J\n",
      "Epoch 1, Batch 201, to_device Energy: 1.5267514929771422 J\n",
      "Epoch 1, Batch 202, to_device Energy: 1.539043600320816 J\n",
      "Epoch 1, Batch 203, to_device Energy: 1.509902542114258 J\n",
      "Epoch 1, Batch 204, to_device Energy: 1.4912154889106752 J\n",
      "Epoch 1, Batch 205, to_device Energy: 1.5271066088676453 J\n",
      "Epoch 1, Batch 206, to_device Energy: 1.4998447036743163 J\n",
      "Epoch 1, Batch 207, to_device Energy: 1.526216799259186 J\n",
      "Epoch 1, Batch 208, to_device Energy: 1.5632164764404297 J\n",
      "Epoch 1, Batch 209, to_device Energy: 1.5270567631721497 J\n",
      "Epoch 1, Batch 210, to_device Energy: 1.5622012491226198 J\n",
      "Epoch 1, Batch 211, to_device Energy: 1.5340291113853457 J\n",
      "Epoch 1, Batch 212, to_device Energy: 1.5525123023986818 J\n",
      "Epoch 1, Batch 213, to_device Energy: 1.5759728176593781 J\n",
      "Epoch 1, Batch 214, to_device Energy: 3.0375740718841557 J\n",
      "Epoch 1, Batch 215, to_device Energy: 1.5121586208343507 J\n",
      "Epoch 1, Batch 216, to_device Energy: 1.511297345161438 J\n",
      "Epoch 1, Batch 217, to_device Energy: 1.5346391971111297 J\n",
      "Epoch 1, Batch 218, to_device Energy: 1.520551191329956 J\n",
      "Epoch 1, Batch 219, to_device Energy: 1.5194068281650543 J\n",
      "Epoch 1, Batch 220, to_device Energy: 1.580157793045044 J\n",
      "Epoch 1, Batch 221, to_device Energy: 1.5195001878738403 J\n",
      "Epoch 1, Batch 222, to_device Energy: 1.5536398282051085 J\n",
      "Epoch 1, Batch 223, to_device Energy: 1.5589232654571532 J\n",
      "Epoch 1, Batch 224, to_device Energy: 1.5553692903518677 J\n",
      "Epoch 1, Batch 225, to_device Energy: 1.5257649910449982 J\n",
      "Epoch 1, Batch 226, to_device Energy: 1.5667814712524413 J\n",
      "Epoch 1, Batch 227, to_device Energy: 1.5323433196544647 J\n",
      "Epoch 1, Batch 228, to_device Energy: 3.1341015849113463 J\n",
      "Epoch 1, Batch 229, to_device Energy: 1.5268793532848357 J\n",
      "Epoch 1, Batch 230, to_device Energy: 1.5307634825706482 J\n",
      "Epoch 1, Batch 231, to_device Energy: 1.5581974391937254 J\n",
      "Epoch 1, Batch 232, to_device Energy: 1.556632828474045 J\n",
      "Epoch 1, Batch 233, to_device Energy: 2.846183653486616 J\n",
      "Epoch 1, Batch 234, to_device Energy: 2.822121351957321 J\n",
      "Epoch 1, Batch 235, to_device Energy: 1.1454617500305175 J\n",
      "Epoch 1, Batch 1, forward Energy: 2.650431983470917 J\n",
      "Epoch 1, Batch 2, forward Energy: 4.714809374332428 J\n",
      "Epoch 1, Batch 3, forward Energy: 6.342419158935548 J\n",
      "Epoch 1, Batch 4, forward Energy: 12.509994256019594 J\n",
      "Epoch 1, Batch 5, forward Energy: 15.526182246208192 J\n",
      "Epoch 1, Batch 6, forward Energy: 15.860401922464371 J\n",
      "Epoch 1, Batch 7, forward Energy: 18.091134153842926 J\n",
      "Epoch 1, Batch 8, forward Energy: 18.048610939502716 J\n",
      "Epoch 1, Batch 9, forward Energy: 16.62788835334778 J\n",
      "Epoch 1, Batch 10, forward Energy: 17.999831114530565 J\n",
      "Epoch 1, Batch 11, forward Energy: 18.099984004974367 J\n",
      "Epoch 1, Batch 12, forward Energy: 17.686802182674406 J\n",
      "Epoch 1, Batch 13, forward Energy: 17.919714935302736 J\n",
      "Epoch 1, Batch 14, forward Energy: 16.458934770345685 J\n",
      "Epoch 1, Batch 15, forward Energy: 16.23774887740612 J\n",
      "Epoch 1, Batch 16, forward Energy: 17.89745374202728 J\n",
      "Epoch 1, Batch 17, forward Energy: 18.012272429704666 J\n",
      "Epoch 1, Batch 18, forward Energy: 17.747240014553068 J\n",
      "Epoch 1, Batch 19, forward Energy: 16.76053673028946 J\n",
      "Epoch 1, Batch 20, forward Energy: 18.25631833672524 J\n",
      "Epoch 1, Batch 21, forward Energy: 17.830606422424314 J\n",
      "Epoch 1, Batch 22, forward Energy: 18.205626381635664 J\n",
      "Epoch 1, Batch 23, forward Energy: 18.15175282025337 J\n",
      "Epoch 1, Batch 24, forward Energy: 16.43457342863083 J\n",
      "Epoch 1, Batch 25, forward Energy: 17.864024005889892 J\n",
      "Epoch 1, Batch 26, forward Energy: 16.545899750828745 J\n",
      "Epoch 1, Batch 27, forward Energy: 18.071848111391063 J\n",
      "Epoch 1, Batch 28, forward Energy: 17.94018428707123 J\n",
      "Epoch 1, Batch 29, forward Energy: 16.694490434646603 J\n",
      "Epoch 1, Batch 30, forward Energy: 17.732137310743333 J\n",
      "Epoch 1, Batch 31, forward Energy: 18.488981740951537 J\n",
      "Epoch 1, Batch 32, forward Energy: 18.04154768061638 J\n",
      "Epoch 1, Batch 33, forward Energy: 17.892211534500124 J\n",
      "Epoch 1, Batch 34, forward Energy: 16.52643309235573 J\n",
      "Epoch 1, Batch 35, forward Energy: 18.007703530073165 J\n",
      "Epoch 1, Batch 36, forward Energy: 17.86362107765675 J\n",
      "Epoch 1, Batch 37, forward Energy: 18.052645176649094 J\n",
      "Epoch 1, Batch 38, forward Energy: 17.99793614256382 J\n",
      "Epoch 1, Batch 39, forward Energy: 17.820376608133316 J\n",
      "Epoch 1, Batch 40, forward Energy: 18.172656250000003 J\n",
      "Epoch 1, Batch 41, forward Energy: 17.891419717788693 J\n",
      "Epoch 1, Batch 42, forward Energy: 17.756097945213316 J\n",
      "Epoch 1, Batch 43, forward Energy: 17.919813411235808 J\n",
      "Epoch 1, Batch 44, forward Energy: 17.977939432382588 J\n",
      "Epoch 1, Batch 45, forward Energy: 17.93896517252922 J\n",
      "Epoch 1, Batch 46, forward Energy: 18.100772272109985 J\n",
      "Epoch 1, Batch 47, forward Energy: 18.072716831445696 J\n",
      "Epoch 1, Batch 48, forward Energy: 16.630592863082885 J\n",
      "Epoch 1, Batch 49, forward Energy: 17.92612367630005 J\n",
      "Epoch 1, Batch 50, forward Energy: 17.91950882387161 J\n",
      "Epoch 1, Batch 51, forward Energy: 16.675821748375892 J\n",
      "Epoch 1, Batch 52, forward Energy: 19.296718654632564 J\n",
      "Epoch 1, Batch 53, forward Energy: 17.91920312333107 J\n",
      "Epoch 1, Batch 54, forward Energy: 14.848760937452315 J\n",
      "Epoch 1, Batch 55, forward Energy: 17.641439101696015 J\n",
      "Epoch 1, Batch 56, forward Energy: 16.61295011687279 J\n",
      "Epoch 1, Batch 57, forward Energy: 17.73137944984436 J\n",
      "Epoch 1, Batch 58, forward Energy: 16.372018185853957 J\n",
      "Epoch 1, Batch 59, forward Energy: 18.01059731936455 J\n",
      "Epoch 1, Batch 60, forward Energy: 17.879753051757813 J\n",
      "Epoch 1, Batch 61, forward Energy: 18.021558036208155 J\n",
      "Epoch 1, Batch 62, forward Energy: 16.877433876395223 J\n",
      "Epoch 1, Batch 63, forward Energy: 16.367451727151874 J\n",
      "Epoch 1, Batch 64, forward Energy: 18.301145614743238 J\n",
      "Epoch 1, Batch 65, forward Energy: 18.082063314437868 J\n",
      "Epoch 1, Batch 66, forward Energy: 17.966713664054872 J\n",
      "Epoch 1, Batch 67, forward Energy: 18.03420237731934 J\n",
      "Epoch 1, Batch 68, forward Energy: 18.17327022373676 J\n",
      "Epoch 1, Batch 69, forward Energy: 16.585617032289505 J\n",
      "Epoch 1, Batch 70, forward Energy: 18.101305633544925 J\n",
      "Epoch 1, Batch 71, forward Energy: 17.9893892660141 J\n",
      "Epoch 1, Batch 72, forward Energy: 17.924216598987577 J\n",
      "Epoch 1, Batch 73, forward Energy: 18.01752415466309 J\n",
      "Epoch 1, Batch 74, forward Energy: 16.60083751344681 J\n",
      "Epoch 1, Batch 75, forward Energy: 17.832018385648727 J\n",
      "Epoch 1, Batch 76, forward Energy: 18.405533546447753 J\n",
      "Epoch 1, Batch 77, forward Energy: 18.060187330245974 J\n",
      "Epoch 1, Batch 78, forward Energy: 18.104061298370365 J\n",
      "Epoch 1, Batch 79, forward Energy: 17.97087079048157 J\n",
      "Epoch 1, Batch 80, forward Energy: 17.94116293525696 J\n",
      "Epoch 1, Batch 81, forward Energy: 18.002958583831788 J\n",
      "Epoch 1, Batch 82, forward Energy: 17.937443484544755 J\n",
      "Epoch 1, Batch 83, forward Energy: 17.92483128762245 J\n",
      "Epoch 1, Batch 84, forward Energy: 17.013668632507322 J\n",
      "Epoch 1, Batch 85, forward Energy: 18.00958839559555 J\n",
      "Epoch 1, Batch 86, forward Energy: 16.65325546836853 J\n",
      "Epoch 1, Batch 87, forward Energy: 17.912393754720686 J\n",
      "Epoch 1, Batch 88, forward Energy: 18.079137061953542 J\n",
      "Epoch 1, Batch 89, forward Energy: 18.142789406776426 J\n",
      "Epoch 1, Batch 90, forward Energy: 17.992398447990418 J\n",
      "Epoch 1, Batch 91, forward Energy: 18.074836697578426 J\n",
      "Epoch 1, Batch 92, forward Energy: 18.254307814002036 J\n",
      "Epoch 1, Batch 93, forward Energy: 18.28139335560798 J\n",
      "Epoch 1, Batch 94, forward Energy: 16.975158354759216 J\n",
      "Epoch 1, Batch 95, forward Energy: 12.094667457103728 J\n",
      "Epoch 1, Batch 96, forward Energy: 17.931898713111877 J\n",
      "Epoch 1, Batch 97, forward Energy: 18.300903787612913 J\n",
      "Epoch 1, Batch 98, forward Energy: 17.798503249406814 J\n",
      "Epoch 1, Batch 99, forward Energy: 16.384312967538833 J\n",
      "Epoch 1, Batch 100, forward Energy: 18.01717956542969 J\n",
      "Epoch 1, Batch 101, forward Energy: 18.256591260671616 J\n",
      "Epoch 1, Batch 102, forward Energy: 19.325135868072508 J\n",
      "Epoch 1, Batch 103, forward Energy: 18.06409012615681 J\n",
      "Epoch 1, Batch 104, forward Energy: 16.760832733869552 J\n",
      "Epoch 1, Batch 105, forward Energy: 18.23034704971313 J\n",
      "Epoch 1, Batch 106, forward Energy: 16.619536585569385 J\n",
      "Epoch 1, Batch 107, forward Energy: 17.712240084648133 J\n",
      "Epoch 1, Batch 108, forward Energy: 18.209136176228522 J\n",
      "Epoch 1, Batch 109, forward Energy: 17.905122960805894 J\n",
      "Epoch 1, Batch 110, forward Energy: 17.853357768774035 J\n",
      "Epoch 1, Batch 111, forward Energy: 17.998586475014687 J\n",
      "Epoch 1, Batch 112, forward Energy: 18.031183415055274 J\n",
      "Epoch 1, Batch 113, forward Energy: 18.487588519096377 J\n",
      "Epoch 1, Batch 114, forward Energy: 16.740189887762067 J\n",
      "Epoch 1, Batch 115, forward Energy: 17.901223856449125 J\n",
      "Epoch 1, Batch 116, forward Energy: 18.162025022506715 J\n",
      "Epoch 1, Batch 117, forward Energy: 18.266383019685748 J\n",
      "Epoch 1, Batch 118, forward Energy: 17.93202333474159 J\n",
      "Epoch 1, Batch 119, forward Energy: 18.080753897190093 J\n",
      "Epoch 1, Batch 120, forward Energy: 18.215050627231598 J\n",
      "Epoch 1, Batch 121, forward Energy: 18.007405332326886 J\n",
      "Epoch 1, Batch 122, forward Energy: 18.359593015432356 J\n",
      "Epoch 1, Batch 123, forward Energy: 17.95376745033264 J\n",
      "Epoch 1, Batch 124, forward Energy: 18.085146786212924 J\n",
      "Epoch 1, Batch 125, forward Energy: 16.638898732185364 J\n",
      "Epoch 1, Batch 126, forward Energy: 17.95363099122048 J\n",
      "Epoch 1, Batch 127, forward Energy: 18.068152451992034 J\n",
      "Epoch 1, Batch 128, forward Energy: 18.511247540473935 J\n",
      "Epoch 1, Batch 129, forward Energy: 17.923127652168276 J\n",
      "Epoch 1, Batch 130, forward Energy: 18.16832508277893 J\n",
      "Epoch 1, Batch 131, forward Energy: 18.28559405732155 J\n",
      "Epoch 1, Batch 132, forward Energy: 16.616353261470792 J\n",
      "Epoch 1, Batch 133, forward Energy: 18.156934274911883 J\n",
      "Epoch 1, Batch 134, forward Energy: 18.250788938283915 J\n",
      "Epoch 1, Batch 135, forward Energy: 18.02773985147476 J\n",
      "Epoch 1, Batch 136, forward Energy: 17.86604493904114 J\n",
      "Epoch 1, Batch 137, forward Energy: 18.00864444565773 J\n",
      "Epoch 1, Batch 138, forward Energy: 17.913578649759287 J\n",
      "Epoch 1, Batch 139, forward Energy: 18.133180134058 J\n",
      "Epoch 1, Batch 140, forward Energy: 18.542455264091494 J\n",
      "Epoch 1, Batch 141, forward Energy: 18.005543590188026 J\n",
      "Epoch 1, Batch 142, forward Energy: 16.74295578742027 J\n",
      "Epoch 1, Batch 143, forward Energy: 16.78045846867561 J\n",
      "Epoch 1, Batch 144, forward Energy: 18.13107090353966 J\n",
      "Epoch 1, Batch 145, forward Energy: 18.242011266946793 J\n",
      "Epoch 1, Batch 146, forward Energy: 17.999021110892294 J\n",
      "Epoch 1, Batch 147, forward Energy: 18.149963902950287 J\n",
      "Epoch 1, Batch 148, forward Energy: 18.19442760276794 J\n",
      "Epoch 1, Batch 149, forward Energy: 18.406053141593933 J\n",
      "Epoch 1, Batch 150, forward Energy: 18.26400102925301 J\n",
      "Epoch 1, Batch 151, forward Energy: 18.215033940553667 J\n",
      "Epoch 1, Batch 152, forward Energy: 18.236813120126723 J\n",
      "Epoch 1, Batch 153, forward Energy: 18.24580631065369 J\n",
      "Epoch 1, Batch 154, forward Energy: 18.31713757252693 J\n",
      "Epoch 1, Batch 155, forward Energy: 16.813332993984222 J\n",
      "Epoch 1, Batch 156, forward Energy: 16.94853602695465 J\n",
      "Epoch 1, Batch 157, forward Energy: 18.54295234537124 J\n",
      "Epoch 1, Batch 158, forward Energy: 18.36318418979645 J\n",
      "Epoch 1, Batch 159, forward Energy: 18.292840056657784 J\n",
      "Epoch 1, Batch 160, forward Energy: 18.184077721595767 J\n",
      "Epoch 1, Batch 161, forward Energy: 18.383111739993097 J\n",
      "Epoch 1, Batch 162, forward Energy: 18.198749014377594 J\n",
      "Epoch 1, Batch 163, forward Energy: 18.256797961711882 J\n",
      "Epoch 1, Batch 164, forward Energy: 18.407207972407342 J\n",
      "Epoch 1, Batch 165, forward Energy: 18.377390717983246 J\n",
      "Epoch 1, Batch 166, forward Energy: 18.19964234972 J\n",
      "Epoch 1, Batch 167, forward Energy: 18.24754458618164 J\n",
      "Epoch 1, Batch 168, forward Energy: 18.32668605875969 J\n",
      "Epoch 1, Batch 169, forward Energy: 18.311443828344345 J\n",
      "Epoch 1, Batch 170, forward Energy: 18.432650113105776 J\n",
      "Epoch 1, Batch 171, forward Energy: 18.50809463047981 J\n",
      "Epoch 1, Batch 172, forward Energy: 18.063112901687624 J\n",
      "Epoch 1, Batch 173, forward Energy: 18.386669030189516 J\n",
      "Epoch 1, Batch 174, forward Energy: 18.392524626731873 J\n",
      "Epoch 1, Batch 175, forward Energy: 18.555854316949848 J\n",
      "Epoch 1, Batch 176, forward Energy: 18.27267524886131 J\n",
      "Epoch 1, Batch 177, forward Energy: 18.42605756998062 J\n",
      "Epoch 1, Batch 178, forward Energy: 16.65355702114105 J\n",
      "Epoch 1, Batch 179, forward Energy: 19.70970301580429 J\n",
      "Epoch 1, Batch 180, forward Energy: 18.003573222160338 J\n",
      "Epoch 1, Batch 181, forward Energy: 18.158433890819552 J\n",
      "Epoch 1, Batch 182, forward Energy: 18.238708421111102 J\n",
      "Epoch 1, Batch 183, forward Energy: 17.07476703250408 J\n",
      "Epoch 1, Batch 184, forward Energy: 18.282353276729584 J\n",
      "Epoch 1, Batch 185, forward Energy: 18.360820652484893 J\n",
      "Epoch 1, Batch 186, forward Energy: 18.279742963075638 J\n",
      "Epoch 1, Batch 187, forward Energy: 18.314032218933107 J\n",
      "Epoch 1, Batch 188, forward Energy: 18.30592154598236 J\n",
      "Epoch 1, Batch 189, forward Energy: 18.50094610643387 J\n",
      "Epoch 1, Batch 190, forward Energy: 18.75008971595764 J\n",
      "Epoch 1, Batch 191, forward Energy: 17.40457114589214 J\n",
      "Epoch 1, Batch 192, forward Energy: 18.353814256429676 J\n",
      "Epoch 1, Batch 193, forward Energy: 18.309865181684494 J\n",
      "Epoch 1, Batch 194, forward Energy: 18.22175492322445 J\n",
      "Epoch 1, Batch 195, forward Energy: 18.21548437356949 J\n",
      "Epoch 1, Batch 196, forward Energy: 18.17087331604958 J\n",
      "Epoch 1, Batch 197, forward Energy: 18.46921303844452 J\n",
      "Epoch 1, Batch 198, forward Energy: 18.26095517539978 J\n",
      "Epoch 1, Batch 199, forward Energy: 16.850680433273315 J\n",
      "Epoch 1, Batch 200, forward Energy: 18.787389823913575 J\n",
      "Epoch 1, Batch 201, forward Energy: 18.339601840972904 J\n",
      "Epoch 1, Batch 202, forward Energy: 18.27873257470131 J\n",
      "Epoch 1, Batch 203, forward Energy: 18.132952136993406 J\n",
      "Epoch 1, Batch 204, forward Energy: 18.349267498254772 J\n",
      "Epoch 1, Batch 205, forward Energy: 18.29102585506439 J\n",
      "Epoch 1, Batch 206, forward Energy: 18.253713832139972 J\n",
      "Epoch 1, Batch 207, forward Energy: 18.32611665844917 J\n",
      "Epoch 1, Batch 208, forward Energy: 18.64495470237732 J\n",
      "Epoch 1, Batch 209, forward Energy: 18.24551517009735 J\n",
      "Epoch 1, Batch 210, forward Energy: 18.367741629600527 J\n",
      "Epoch 1, Batch 211, forward Energy: 18.334327644109727 J\n",
      "Epoch 1, Batch 212, forward Energy: 18.37748812520504 J\n",
      "Epoch 1, Batch 213, forward Energy: 18.54258213186264 J\n",
      "Epoch 1, Batch 214, forward Energy: 18.140750692248346 J\n",
      "Epoch 1, Batch 215, forward Energy: 17.23300506591797 J\n",
      "Epoch 1, Batch 216, forward Energy: 18.34682999253273 J\n",
      "Epoch 1, Batch 217, forward Energy: 18.56040833866596 J\n",
      "Epoch 1, Batch 218, forward Energy: 18.50467935347557 J\n",
      "Epoch 1, Batch 219, forward Energy: 18.612751134395598 J\n",
      "Epoch 1, Batch 220, forward Energy: 18.511761158704758 J\n",
      "Epoch 1, Batch 221, forward Energy: 18.29790488159657 J\n",
      "Epoch 1, Batch 222, forward Energy: 18.42180907869339 J\n",
      "Epoch 1, Batch 223, forward Energy: 18.45544507598877 J\n",
      "Epoch 1, Batch 224, forward Energy: 16.764605344176292 J\n",
      "Epoch 1, Batch 225, forward Energy: 18.374636684656142 J\n",
      "Epoch 1, Batch 226, forward Energy: 18.836948826670643 J\n",
      "Epoch 1, Batch 227, forward Energy: 18.3665114415884 J\n",
      "Epoch 1, Batch 228, forward Energy: 16.933934337377547 J\n",
      "Epoch 1, Batch 229, forward Energy: 17.061967195510864 J\n",
      "Epoch 1, Batch 230, forward Energy: 18.358942364454265 J\n",
      "Epoch 1, Batch 231, forward Energy: 18.622250903129576 J\n",
      "Epoch 1, Batch 232, forward Energy: 18.61491766643524 J\n",
      "Epoch 1, Batch 233, forward Energy: 18.294256732940674 J\n",
      "Epoch 1, Batch 234, forward Energy: 18.508448989272118 J\n",
      "Epoch 1, Batch 235, forward Energy: 7.62618307876587 J\n",
      "Epoch 1, Batch 1, backward Energy: 7.10279041302204 J\n",
      "Epoch 1, Batch 2, backward Energy: 11.081658934831621 J\n",
      "Epoch 1, Batch 3, backward Energy: 22.7807049381733 J\n",
      "Epoch 1, Batch 4, backward Energy: 28.724455808162688 J\n",
      "Epoch 1, Batch 5, backward Energy: 34.363128698348994 J\n",
      "Epoch 1, Batch 6, backward Energy: 37.88930755233765 J\n",
      "Epoch 1, Batch 7, backward Energy: 37.4319509820938 J\n",
      "Epoch 1, Batch 8, backward Energy: 37.55439842247962 J\n",
      "Epoch 1, Batch 9, backward Energy: 38.4346176840067 J\n",
      "Epoch 1, Batch 10, backward Energy: 37.19618404722214 J\n",
      "Epoch 1, Batch 11, backward Energy: 38.04590853929519 J\n",
      "Epoch 1, Batch 12, backward Energy: 36.91676675462723 J\n",
      "Epoch 1, Batch 13, backward Energy: 37.55957378530501 J\n",
      "Epoch 1, Batch 14, backward Energy: 38.469169554710376 J\n",
      "Epoch 1, Batch 15, backward Energy: 36.88926936244966 J\n",
      "Epoch 1, Batch 16, backward Energy: 37.74663483214378 J\n",
      "Epoch 1, Batch 17, backward Energy: 37.24089382827282 J\n",
      "Epoch 1, Batch 18, backward Energy: 36.81685114908218 J\n",
      "Epoch 1, Batch 19, backward Energy: 38.03333909535407 J\n",
      "Epoch 1, Batch 20, backward Energy: 37.37835738754272 J\n",
      "Epoch 1, Batch 21, backward Energy: 38.49739005041123 J\n",
      "Epoch 1, Batch 22, backward Energy: 37.755289692997934 J\n",
      "Epoch 1, Batch 23, backward Energy: 37.266019233226764 J\n",
      "Epoch 1, Batch 24, backward Energy: 38.58104273283482 J\n",
      "Epoch 1, Batch 25, backward Energy: 38.84711018741132 J\n",
      "Epoch 1, Batch 26, backward Energy: 37.26999813365937 J\n",
      "Epoch 1, Batch 27, backward Energy: 37.531810443878165 J\n",
      "Epoch 1, Batch 28, backward Energy: 37.251853191137315 J\n",
      "Epoch 1, Batch 29, backward Energy: 38.76702038526534 J\n",
      "Epoch 1, Batch 30, backward Energy: 37.03970969557761 J\n",
      "Epoch 1, Batch 31, backward Energy: 36.384431802392 J\n",
      "Epoch 1, Batch 32, backward Energy: 37.27256132233143 J\n",
      "Epoch 1, Batch 33, backward Energy: 36.739081119894976 J\n",
      "Epoch 1, Batch 34, backward Energy: 37.236968569278716 J\n",
      "Epoch 1, Batch 35, backward Energy: 37.048458006858816 J\n",
      "Epoch 1, Batch 36, backward Energy: 36.5167104358673 J\n",
      "Epoch 1, Batch 37, backward Energy: 37.082328266143804 J\n",
      "Epoch 1, Batch 38, backward Energy: 36.790504004240034 J\n",
      "Epoch 1, Batch 39, backward Energy: 37.01460355496407 J\n",
      "Epoch 1, Batch 40, backward Energy: 37.09234393465518 J\n",
      "Epoch 1, Batch 41, backward Energy: 38.33198170995713 J\n",
      "Epoch 1, Batch 42, backward Energy: 38.2757124259472 J\n",
      "Epoch 1, Batch 43, backward Energy: 37.90067648649216 J\n",
      "Epoch 1, Batch 44, backward Energy: 37.44263398075104 J\n",
      "Epoch 1, Batch 45, backward Energy: 37.41943144547938 J\n",
      "Epoch 1, Batch 46, backward Energy: 37.43529516530038 J\n",
      "Epoch 1, Batch 47, backward Energy: 37.252735219717025 J\n",
      "Epoch 1, Batch 48, backward Energy: 39.08005009555816 J\n",
      "Epoch 1, Batch 49, backward Energy: 37.46338192033768 J\n",
      "Epoch 1, Batch 50, backward Energy: 36.96786817240715 J\n",
      "Epoch 1, Batch 51, backward Energy: 37.33808556461334 J\n",
      "Epoch 1, Batch 52, backward Energy: 37.69356686401367 J\n",
      "Epoch 1, Batch 53, backward Energy: 25.058799400925636 J\n",
      "Epoch 1, Batch 54, backward Energy: 37.45503570699692 J\n",
      "Epoch 1, Batch 55, backward Energy: 38.61385890841484 J\n",
      "Epoch 1, Batch 56, backward Energy: 38.303464147806174 J\n",
      "Epoch 1, Batch 57, backward Energy: 37.107203614234926 J\n",
      "Epoch 1, Batch 58, backward Energy: 37.72161276578904 J\n",
      "Epoch 1, Batch 59, backward Energy: 37.13367814576626 J\n",
      "Epoch 1, Batch 60, backward Energy: 39.114870085954664 J\n",
      "Epoch 1, Batch 61, backward Energy: 38.025408173561104 J\n",
      "Epoch 1, Batch 62, backward Energy: 37.84057402217388 J\n",
      "Epoch 1, Batch 63, backward Energy: 38.5960683504343 J\n",
      "Epoch 1, Batch 64, backward Energy: 37.52753310906887 J\n",
      "Epoch 1, Batch 65, backward Energy: 37.652887852311146 J\n",
      "Epoch 1, Batch 66, backward Energy: 37.607345387697215 J\n",
      "Epoch 1, Batch 67, backward Energy: 37.969843898177146 J\n",
      "Epoch 1, Batch 68, backward Energy: 37.598011227846136 J\n",
      "Epoch 1, Batch 69, backward Energy: 38.85945888411999 J\n",
      "Epoch 1, Batch 70, backward Energy: 37.558934489250184 J\n",
      "Epoch 1, Batch 71, backward Energy: 38.380522833824166 J\n",
      "Epoch 1, Batch 72, backward Energy: 37.62517372107506 J\n",
      "Epoch 1, Batch 73, backward Energy: 39.1060250338316 J\n",
      "Epoch 1, Batch 74, backward Energy: 37.47884862542153 J\n",
      "Epoch 1, Batch 75, backward Energy: 38.59391501617432 J\n",
      "Epoch 1, Batch 76, backward Energy: 37.609196370601666 J\n",
      "Epoch 1, Batch 77, backward Energy: 37.418115059018135 J\n",
      "Epoch 1, Batch 78, backward Energy: 37.52206747746468 J\n",
      "Epoch 1, Batch 79, backward Energy: 38.78471887493133 J\n",
      "Epoch 1, Batch 80, backward Energy: 37.12589017522334 J\n",
      "Epoch 1, Batch 81, backward Energy: 37.82845193576813 J\n",
      "Epoch 1, Batch 82, backward Energy: 37.616267743110654 J\n",
      "Epoch 1, Batch 83, backward Energy: 37.39932208371162 J\n",
      "Epoch 1, Batch 84, backward Energy: 38.810029228210446 J\n",
      "Epoch 1, Batch 85, backward Energy: 37.919055256962785 J\n",
      "Epoch 1, Batch 86, backward Energy: 37.11160515260696 J\n",
      "Epoch 1, Batch 87, backward Energy: 37.3466324917078 J\n",
      "Epoch 1, Batch 88, backward Energy: 37.9570192078352 J\n",
      "Epoch 1, Batch 89, backward Energy: 37.35840548729896 J\n",
      "Epoch 1, Batch 90, backward Energy: 38.86463951849937 J\n",
      "Epoch 1, Batch 91, backward Energy: 37.94525898909569 J\n",
      "Epoch 1, Batch 92, backward Energy: 37.56047374927998 J\n",
      "Epoch 1, Batch 93, backward Energy: 37.95961269247532 J\n",
      "Epoch 1, Batch 94, backward Energy: 38.761862359642976 J\n",
      "Epoch 1, Batch 95, backward Energy: 20.76655667638779 J\n",
      "Epoch 1, Batch 96, backward Energy: 37.95103156888485 J\n",
      "Epoch 1, Batch 97, backward Energy: 37.58545753669739 J\n",
      "Epoch 1, Batch 98, backward Energy: 37.40268814849853 J\n",
      "Epoch 1, Batch 99, backward Energy: 37.46604062867165 J\n",
      "Epoch 1, Batch 100, backward Energy: 37.917369858264934 J\n",
      "Epoch 1, Batch 101, backward Energy: 38.56473484277725 J\n",
      "Epoch 1, Batch 102, backward Energy: 37.45582631349564 J\n",
      "Epoch 1, Batch 103, backward Energy: 39.103457111358644 J\n",
      "Epoch 1, Batch 104, backward Energy: 37.19834490776062 J\n",
      "Epoch 1, Batch 105, backward Energy: 38.511697661399836 J\n",
      "Epoch 1, Batch 106, backward Energy: 39.01611466288566 J\n",
      "Epoch 1, Batch 107, backward Energy: 38.47634518706799 J\n",
      "Epoch 1, Batch 108, backward Energy: 37.575353163480756 J\n",
      "Epoch 1, Batch 109, backward Energy: 38.10447962391376 J\n",
      "Epoch 1, Batch 110, backward Energy: 37.56453387928009 J\n",
      "Epoch 1, Batch 111, backward Energy: 37.2402858928442 J\n",
      "Epoch 1, Batch 112, backward Energy: 37.51280848407746 J\n",
      "Epoch 1, Batch 113, backward Energy: 39.20245366334915 J\n",
      "Epoch 1, Batch 114, backward Energy: 37.826625180602065 J\n",
      "Epoch 1, Batch 115, backward Energy: 37.315235675573355 J\n",
      "Epoch 1, Batch 116, backward Energy: 38.00149777293205 J\n",
      "Epoch 1, Batch 117, backward Energy: 37.4367220300436 J\n",
      "Epoch 1, Batch 118, backward Energy: 37.39361806273461 J\n",
      "Epoch 1, Batch 119, backward Energy: 37.77061744880676 J\n",
      "Epoch 1, Batch 120, backward Energy: 38.99372916054726 J\n",
      "Epoch 1, Batch 121, backward Energy: 39.24049705266953 J\n",
      "Epoch 1, Batch 122, backward Energy: 37.619140353322024 J\n",
      "Epoch 1, Batch 123, backward Energy: 37.52729017233849 J\n",
      "Epoch 1, Batch 124, backward Energy: 37.97247202026844 J\n",
      "Epoch 1, Batch 125, backward Energy: 37.759597536563874 J\n",
      "Epoch 1, Batch 126, backward Energy: 37.37215928137302 J\n",
      "Epoch 1, Batch 127, backward Energy: 37.9250007095337 J\n",
      "Epoch 1, Batch 128, backward Energy: 37.47594900870323 J\n",
      "Epoch 1, Batch 129, backward Energy: 36.99031873202323 J\n",
      "Epoch 1, Batch 130, backward Energy: 38.32221016645431 J\n",
      "Epoch 1, Batch 131, backward Energy: 38.721739579200744 J\n",
      "Epoch 1, Batch 132, backward Energy: 37.80661649513245 J\n",
      "Epoch 1, Batch 133, backward Energy: 39.75045188236236 J\n",
      "Epoch 1, Batch 134, backward Energy: 39.13478881883621 J\n",
      "Epoch 1, Batch 135, backward Energy: 37.77210258603096 J\n",
      "Epoch 1, Batch 136, backward Energy: 38.84563091552257 J\n",
      "Epoch 1, Batch 137, backward Energy: 37.40737804889679 J\n",
      "Epoch 1, Batch 138, backward Energy: 37.21152402698993 J\n",
      "Epoch 1, Batch 139, backward Energy: 37.76415762746334 J\n",
      "Epoch 1, Batch 140, backward Energy: 37.724863969802854 J\n",
      "Epoch 1, Batch 141, backward Energy: 38.89043700957299 J\n",
      "Epoch 1, Batch 142, backward Energy: 39.648800941109656 J\n",
      "Epoch 1, Batch 143, backward Energy: 37.432776158809666 J\n",
      "Epoch 1, Batch 144, backward Energy: 37.67571760797501 J\n",
      "Epoch 1, Batch 145, backward Energy: 37.951018695354456 J\n",
      "Epoch 1, Batch 146, backward Energy: 37.3155499227047 J\n",
      "Epoch 1, Batch 147, backward Energy: 37.63629827189446 J\n",
      "Epoch 1, Batch 148, backward Energy: 37.93691378271579 J\n",
      "Epoch 1, Batch 149, backward Energy: 37.877867243885994 J\n",
      "Epoch 1, Batch 150, backward Energy: 37.80216149997711 J\n",
      "Epoch 1, Batch 151, backward Energy: 39.13371486532687 J\n",
      "Epoch 1, Batch 152, backward Energy: 37.500851932287226 J\n",
      "Epoch 1, Batch 153, backward Energy: 37.91316877031326 J\n",
      "Epoch 1, Batch 154, backward Energy: 38.22946434700488 J\n",
      "Epoch 1, Batch 155, backward Energy: 39.50825244045257 J\n",
      "Epoch 1, Batch 156, backward Energy: 37.98304049062728 J\n",
      "Epoch 1, Batch 157, backward Energy: 38.1708509967327 J\n",
      "Epoch 1, Batch 158, backward Energy: 38.32522880101204 J\n",
      "Epoch 1, Batch 159, backward Energy: 38.41681464231014 J\n",
      "Epoch 1, Batch 160, backward Energy: 39.0717108848095 J\n",
      "Epoch 1, Batch 161, backward Energy: 38.25147836232185 J\n",
      "Epoch 1, Batch 162, backward Energy: 37.647942131757745 J\n",
      "Epoch 1, Batch 163, backward Energy: 39.70247110462189 J\n",
      "Epoch 1, Batch 164, backward Energy: 38.09603363883496 J\n",
      "Epoch 1, Batch 165, backward Energy: 37.6809792419672 J\n",
      "Epoch 1, Batch 166, backward Energy: 39.24680921447276 J\n",
      "Epoch 1, Batch 167, backward Energy: 38.14188993787766 J\n",
      "Epoch 1, Batch 168, backward Energy: 37.68491146755219 J\n",
      "Epoch 1, Batch 169, backward Energy: 39.64439780509472 J\n",
      "Epoch 1, Batch 170, backward Energy: 38.41874278116226 J\n",
      "Epoch 1, Batch 171, backward Energy: 38.497850309371955 J\n",
      "Epoch 1, Batch 172, backward Energy: 37.961304400682444 J\n",
      "Epoch 1, Batch 173, backward Energy: 38.15699677371979 J\n",
      "Epoch 1, Batch 174, backward Energy: 37.86727967619897 J\n",
      "Epoch 1, Batch 175, backward Energy: 37.77781296622753 J\n",
      "Epoch 1, Batch 176, backward Energy: 39.70165352869034 J\n",
      "Epoch 1, Batch 177, backward Energy: 38.16507550525666 J\n",
      "Epoch 1, Batch 178, backward Energy: 39.26989453411102 J\n",
      "Epoch 1, Batch 179, backward Energy: 37.36591550683975 J\n",
      "Epoch 1, Batch 180, backward Energy: 39.38087289941311 J\n",
      "Epoch 1, Batch 181, backward Energy: 38.01059975552558 J\n",
      "Epoch 1, Batch 182, backward Energy: 39.40863076090813 J\n",
      "Epoch 1, Batch 183, backward Energy: 39.24798780179023 J\n",
      "Epoch 1, Batch 184, backward Energy: 37.48961443126201 J\n",
      "Epoch 1, Batch 185, backward Energy: 38.26753092670442 J\n",
      "Epoch 1, Batch 186, backward Energy: 38.054169950485225 J\n",
      "Epoch 1, Batch 187, backward Energy: 38.075062278032306 J\n",
      "Epoch 1, Batch 188, backward Energy: 38.55593649959564 J\n",
      "Epoch 1, Batch 189, backward Energy: 38.19006983065606 J\n",
      "Epoch 1, Batch 190, backward Energy: 38.87440356719493 J\n",
      "Epoch 1, Batch 191, backward Energy: 38.34063783788681 J\n",
      "Epoch 1, Batch 192, backward Energy: 38.90348270916939 J\n",
      "Epoch 1, Batch 193, backward Energy: 37.72423843574524 J\n",
      "Epoch 1, Batch 194, backward Energy: 38.53376642417908 J\n",
      "Epoch 1, Batch 195, backward Energy: 38.713946760416036 J\n",
      "Epoch 1, Batch 196, backward Energy: 38.63854571461677 J\n",
      "Epoch 1, Batch 197, backward Energy: 37.75003916835785 J\n",
      "Epoch 1, Batch 198, backward Energy: 37.678977166295056 J\n",
      "Epoch 1, Batch 199, backward Energy: 38.5070912283659 J\n",
      "Epoch 1, Batch 200, backward Energy: 39.811398682594294 J\n",
      "Epoch 1, Batch 201, backward Energy: 39.61872138166427 J\n",
      "Epoch 1, Batch 202, backward Energy: 39.78415485405922 J\n",
      "Epoch 1, Batch 203, backward Energy: 37.58310943090916 J\n",
      "Epoch 1, Batch 204, backward Energy: 39.47067122519016 J\n",
      "Epoch 1, Batch 205, backward Energy: 38.57053983139993 J\n",
      "Epoch 1, Batch 206, backward Energy: 38.34354891538621 J\n",
      "Epoch 1, Batch 207, backward Energy: 37.916125023603435 J\n",
      "Epoch 1, Batch 208, backward Energy: 38.646452323675156 J\n",
      "Epoch 1, Batch 209, backward Energy: 37.87853961706163 J\n",
      "Epoch 1, Batch 210, backward Energy: 38.08640937662125 J\n",
      "Epoch 1, Batch 211, backward Energy: 39.446876223921784 J\n",
      "Epoch 1, Batch 212, backward Energy: 39.3772275891304 J\n",
      "Epoch 1, Batch 213, backward Energy: 37.87590122783185 J\n",
      "Epoch 1, Batch 214, backward Energy: 37.90766370177269 J\n",
      "Epoch 1, Batch 215, backward Energy: 39.269544743061054 J\n",
      "Epoch 1, Batch 216, backward Energy: 39.57515468525886 J\n",
      "Epoch 1, Batch 217, backward Energy: 38.36117040932179 J\n",
      "Epoch 1, Batch 218, backward Energy: 38.222708778619776 J\n",
      "Epoch 1, Batch 219, backward Energy: 38.66833691728116 J\n",
      "Epoch 1, Batch 220, backward Energy: 38.54071746397018 J\n",
      "Epoch 1, Batch 221, backward Energy: 39.47311332285405 J\n",
      "Epoch 1, Batch 222, backward Energy: 38.308519455671316 J\n",
      "Epoch 1, Batch 223, backward Energy: 38.97729287004471 J\n",
      "Epoch 1, Batch 224, backward Energy: 39.22149767017364 J\n",
      "Epoch 1, Batch 225, backward Energy: 39.73830208432675 J\n",
      "Epoch 1, Batch 226, backward Energy: 38.493446666479116 J\n",
      "Epoch 1, Batch 227, backward Energy: 38.66623960661887 J\n",
      "Epoch 1, Batch 228, backward Energy: 38.297181546807295 J\n",
      "Epoch 1, Batch 229, backward Energy: 37.81030135548115 J\n",
      "Epoch 1, Batch 230, backward Energy: 38.79739002025127 J\n",
      "Epoch 1, Batch 231, backward Energy: 38.636265286803244 J\n",
      "Epoch 1, Batch 232, backward Energy: 37.930124426841736 J\n",
      "Epoch 1, Batch 233, backward Energy: 39.791447891354565 J\n",
      "Epoch 1, Batch 234, backward Energy: 40.2468995501995 J\n",
      "Epoch 1, Batch 235, backward Energy: 15.226737748146057 J\n",
      "The total energy consumption in to_device part is: 400.109221435227, and the total time consumed is: 2.323862314224243.\n",
      "The total energy consumption in forward part is: 4148.698407112719, and the total time consumed is: 15.808207273483276.\n",
      "The total energy consumption in backward part is: 8812.383241205815, and the total time consumed is: 32.49524736404419\n"
     ]
    }
   ],
   "source": [
    "# create the folder to store the data\n",
    "main_folder = data_path\n",
    "print('The folder is:', main_folder)\n",
    "# find out that if the folder exists in the data path\n",
    "# 判断文件是否存在\n",
    "if main_folder.exists():\n",
    "    print(\"文件存在。\")\n",
    "else:\n",
    "    os.makedirs(main_folder)\n",
    "    print(\"文件不存在，已创建。\")\n",
    "    print(\"文件创建于：\", main_folder)\n",
    "for epoch in epochs:\n",
    "    for batch in batch_size:\n",
    "        for round in range(rounds):\n",
    "            train_model_f(main_folder, batch, epoch, round, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create the folder to store the data\\nmain_folder = data_path/\\'cifar100\\'\\nprint(\\'The folder is:\\', main_folder)\\n# find out that if the folder exists in the data path\\n# 判断文件是否存在\\nif main_folder.exists():\\n    print(\"文件存在。\")\\nelse:\\n    os.makedirs(main_folder)\\n    print(\"文件不存在，已创建。\")\\n    print(\"文件创建于：\", main_folder)\\nfor epoch in epochs:\\n    for batch in batch_size:\\n        for round in range(rounds):\\n            train_model_c(main_folder, batch, epoch, round, lr, device)\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# create the folder to store the data\n",
    "main_folder = data_path/'cifar100'\n",
    "print('The folder is:', main_folder)\n",
    "# find out that if the folder exists in the data path\n",
    "# 判断文件是否存在\n",
    "if main_folder.exists():\n",
    "    print(\"文件存在。\")\n",
    "else:\n",
    "    os.makedirs(main_folder)\n",
    "    print(\"文件不存在，已创建。\")\n",
    "    print(\"文件创建于：\", main_folder)\n",
    "for epoch in epochs:\n",
    "    for batch in batch_size:\n",
    "        for round in range(rounds):\n",
    "            train_model_c(main_folder, batch, epoch, round, lr, device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create the folder to store the data\\nmain_folder = data_path/\\'cifar10\\'\\nprint(\\'The folder is:\\', main_folder)\\n# find out that if the folder exists in the data path\\n# 判断文件是否存在\\nif main_folder.exists():\\n    print(\"文件存在。\")\\nelse:\\n    os.makedirs(main_folder)\\n    print(\"文件不存在，已创建。\")\\n    print(\"文件创建于：\", main_folder)\\nfor epoch in epochs:\\n    for batch in batch_size:\\n        for round in range(rounds):\\n            train_model_c10(main_folder, batch, epoch, round, lr, device)\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# create the folder to store the data\n",
    "main_folder = data_path/'cifar10'\n",
    "print('The folder is:', main_folder)\n",
    "# find out that if the folder exists in the data path\n",
    "# 判断文件是否存在\n",
    "if main_folder.exists():\n",
    "    print(\"文件存在。\")\n",
    "else:\n",
    "    os.makedirs(main_folder)\n",
    "    print(\"文件不存在，已创建。\")\n",
    "    print(\"文件创建于：\", main_folder)\n",
    "for epoch in epochs:\n",
    "    for batch in batch_size:\n",
    "        for round in range(rounds):\n",
    "            train_model_c10(main_folder, batch, epoch, round, lr, device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
