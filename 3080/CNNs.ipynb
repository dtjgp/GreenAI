{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to gather the information of the energy consumption of the whole training process of different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ptflops import get_model_complexity_info\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import pynvml\n",
    "import threading\n",
    "import queue\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current path is: /home/GreenAI/3080\n",
      "The data path is: /home/GreenAI/3080/ModelsData\n"
     ]
    }
   ],
   "source": [
    "'''find the Model path'''\n",
    "# find the current path\n",
    "from pathlib import Path\n",
    "\n",
    "# find the current path\n",
    "current_path = Path.cwd()\n",
    "print('The current path is:', current_path)\n",
    "\n",
    "# find the data path\n",
    "data_path = Path(current_path / 'ModelsData')\n",
    "print('The data path is:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = ['alexnet', \n",
    "               'vgg11', 'vgg13', 'vgg16', \n",
    "               'resnet18', 'resnet34', 'resnet50',\n",
    "               'googlenet_origin', 'googlenet_mod1', 'googlenet_mod2', 'googlenet_mod3',\n",
    "               'googlenet_mod4', 'googlenet_mod5', 'googlenet_mod6', 'googlenet_mod7', \n",
    "               'googlenet_mod8', 'googlenet_mod9',\n",
    "               'mobilenetv1_path', 'mobilenetv2_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/GreenAI/3080/ModelsData/alexnet'), PosixPath('/home/GreenAI/3080/ModelsData/vgg11'), PosixPath('/home/GreenAI/3080/ModelsData/vgg13'), PosixPath('/home/GreenAI/3080/ModelsData/vgg16'), PosixPath('/home/GreenAI/3080/ModelsData/resnet18'), PosixPath('/home/GreenAI/3080/ModelsData/resnet34'), PosixPath('/home/GreenAI/3080/ModelsData/resnet50'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_origin'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod1'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod2'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod3'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod4'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod5'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod6'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod7'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod8'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod9'), PosixPath('/home/GreenAI/3080/ModelsData/mobilenetv1_path'), PosixPath('/home/GreenAI/3080/ModelsData/mobilenetv2_path')]\n"
     ]
    }
   ],
   "source": [
    "DataList = [Path(f\"{data_path}/{i}\") for i in models_name]\n",
    "print(DataList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(img_channel, num_labels):\n",
    "    net = nn.Sequential(\n",
    "        # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "        # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "        # 另外，输出通道的数目远大于LeNet\n",
    "        nn.Conv2d(img_channel, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "        nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "        # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "        # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "        nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.AdaptiveAvgPool2d((6, 6)),   # 使用全局平均池化对每个通道中所有元素求平均并直接将结果传递到全连接层\n",
    "        nn.Flatten(),\n",
    "        # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "        nn.Linear(256 * 6 * 6, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "        nn.Linear(4096, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG11 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg11(input_channels, output_channels):\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg11_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG13 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg13_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg13(input_channels, output_channels):\n",
    "    # VGG-13 architecture\n",
    "    conv_arch = [(2, 64), (2, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg13_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg16(input_channels, output_channels):\n",
    "    conv_arch = [(2, 64), (2, 128), (3, 256), (3, 512), (3, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg16_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual18(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "    \n",
    "def resnet(img_channel, num_labels):\n",
    "    # blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                    first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual18(input_channels, num_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual18(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                        nn.AdaptiveAvgPool2d((1,1)),\n",
    "                        nn.Flatten(), nn.Linear(512, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet34 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual34(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "    \n",
    "def resnet(img_channel, num_labels):\n",
    "    # blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                    first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual34(input_channels, num_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual34(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 3, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 4))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 6))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 3))\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                        nn.AdaptiveAvgPool2d((1,1)),\n",
    "                        nn.Flatten(), nn.Linear(512, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual50(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels , kernel_size=1, stride=strides, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels )\n",
    "        self.conv2 = nn.Conv2d(num_channels , num_channels , kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels )\n",
    "        self.conv3 = nn.Conv2d(num_channels , num_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "        if use_1x1conv or strides != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides, bias=False),\n",
    "                nn.BatchNorm2d(num_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        if self.downsample:\n",
    "            X = self.downsample(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "def resnet(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(\n",
    "        nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    )\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual50(input_channels, num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual50(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 3, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 4))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 6))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 3))\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        b1, b2, b3, b4, b5,\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(), nn.Linear(512, num_labels)\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogleNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet Model(orinigal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod1(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod1, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod1(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod1(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod1(256-64, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod1(480-128, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod1(512-192, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod1(512-160, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod1(512-128, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod1(528-112, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod1(832-256, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod1(832-256, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod2(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod2, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        # p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod2(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod2(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod2(256-128, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod2(480-192, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod2(512-208, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod2(512-224, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod2(512-256, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod2(528-288, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod2(832-320, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod2(832-320, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod3(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod3, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        # self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        # self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod3(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod3(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod3(256-32, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod3(480-96, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod3(512-48, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod3(512-64, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod3(512-64, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod3(528-64, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod3(832-128, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod3(832-128, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-128, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod4(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod4, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3), dim=1)\n",
    "    \n",
    "def Googlenet_mod4(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod4(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod4(256-32, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod4(480-64, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod4(512-64, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod4(512-64, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod4(512-64, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod4(528-64, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod4(832-128, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod4(832-128, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-128, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod5(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod5, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        # self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        # self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat([p2], dim=1)\n",
    "    \n",
    "def Googlenet_mod5(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod5(192, (96, 128)),\n",
    "                   Inception_mod5(128, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod5(192, (96, 208)),\n",
    "                   Inception_mod5(208, (112, 224)),\n",
    "                   Inception_mod5(224, (128, 256)),\n",
    "                   Inception_mod5(256, (144, 288)),\n",
    "                   Inception_mod5(288, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod5(320, (160, 320)),\n",
    "                   Inception_mod5(320, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version6 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod6(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod6, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3), dim=1)\n",
    "    \n",
    "def Googlenet_mod6(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod6(192, (96, 128)),\n",
    "                   Inception_mod6(128*2, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod6(192*2, (96, 208)),\n",
    "                   Inception_mod6(208*2, (112, 224)),\n",
    "                   Inception_mod6(224*2, (128, 256)),\n",
    "                   Inception_mod6(256*2, (144, 288)),\n",
    "                   Inception_mod6(288*2, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod6(320*2, (160, 320)),\n",
    "                   Inception_mod6(320*2, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*2, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod7(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod7, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod7(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod7(192, (96, 128)),\n",
    "                   Inception_mod7(128*3, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod7(192*3, (96, 208)),\n",
    "                   Inception_mod7(208*3, (112, 224)),\n",
    "                   Inception_mod7(224*3, (128, 256)),\n",
    "                   Inception_mod7(256*3, (144, 288)),\n",
    "                   Inception_mod7(288*3, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod7(320*3, (160, 320)),\n",
    "                   Inception_mod7(320*3, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*3, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod8(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod8, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p1_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_2(F.relu(self.p1_1(x))))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod8(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod8(192, (96, 128)),\n",
    "                   Inception_mod8(128*4, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod8(192*4, (96, 208)),\n",
    "                   Inception_mod8(208*4, (112, 224)),\n",
    "                   Inception_mod8(224*4, (128, 256)),\n",
    "                   Inception_mod8(256*4, (144, 288)),\n",
    "                   Inception_mod8(288*4, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod8(320*4, (160, 320)),\n",
    "                   Inception_mod8(320*4, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*4, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version9 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Inception_mod8(nn.Module):\n",
    "#     # c1--c4是每条路径的输出通道数\n",
    "#     def __init__(self, in_channels, c2, **kwargs):\n",
    "#         super(Inception_mod8, self).__init__(**kwargs)\n",
    "#         # 线路1，单1x1卷积层\n",
    "#         self.p1_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p1_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路2，1x1卷积层后接3x3卷积层\n",
    "#         self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路3，1x1卷积层后接5x5卷积层\n",
    "#         self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "#         self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         p1 = F.relu(self.p1_2(F.relu(self.p1_1(x))))\n",
    "#         p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "#         p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "#         p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "#         # 在通道维度上连结输出\n",
    "#         return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod9(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b3 = nn.Sequential(Inception_mod8(192, (96, 128)),\n",
    "    #                Inception_mod8(128*4, (128, 192)),\n",
    "    #                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b4 = nn.Sequential(Inception_mod8(192*4, (96, 208)),\n",
    "    #                Inception_mod8(208*4, (112, 224)),\n",
    "    #                Inception_mod8(224*4, (128, 256)),\n",
    "    #                Inception_mod8(256*4, (144, 288)),\n",
    "    #                Inception_mod8(288*4, (160, 320)),\n",
    "    #                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b5 = nn.Sequential(Inception_mod8(320*4, (160, 320)),\n",
    "    #                Inception_mod8(320*4, (192, 384)),\n",
    "    #                nn.AdaptiveAvgPool2d((1,1)),\n",
    "    #                nn.Flatten())\n",
    "    \n",
    "    b5 = nn.Sequential(\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b5, nn.Linear(192, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MobileNetV1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            DepthwiseSeparableConv(32, 64, 1),\n",
    "            DepthwiseSeparableConv(64, 128, 2),\n",
    "            DepthwiseSeparableConv(128, 128, 1),\n",
    "            DepthwiseSeparableConv(128, 256, 2),\n",
    "            DepthwiseSeparableConv(256, 256, 1),\n",
    "            DepthwiseSeparableConv(256, 512, 2),\n",
    "\n",
    "            # Typically, 5 Depthwise Separable Convolutions are repeated here, each with stride 1\n",
    "            *[DepthwiseSeparableConv(512, 512, 1) for _ in range(5)],\n",
    "\n",
    "            DepthwiseSeparableConv(512, 1024, 2),\n",
    "            DepthwiseSeparableConv(1024, 1024, 1),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, output_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_residual = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # expand\n",
    "            nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # depthwise\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # project\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.layers(x)\n",
    "        else:\n",
    "            return self.layers(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.inverted_residual_blocks = nn.Sequential(\n",
    "            InvertedResidual(32, 16, 1, 1),\n",
    "            InvertedResidual(16, 24, 2, 6),\n",
    "            InvertedResidual(24, 24, 1, 6),\n",
    "            InvertedResidual(24, 32, 2, 6),\n",
    "            InvertedResidual(32, 32, 1, 6),\n",
    "            InvertedResidual(32, 32, 1, 6),\n",
    "            InvertedResidual(32, 64, 2, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 96, 1, 6),\n",
    "            InvertedResidual(96, 96, 1, 6),\n",
    "            InvertedResidual(96, 96, 1, 6),\n",
    "            InvertedResidual(96, 160, 2, 6),\n",
    "            InvertedResidual(160, 160, 1, 6),\n",
    "            InvertedResidual(160, 160, 1, 6),\n",
    "            InvertedResidual(160, 320, 1, 6)\n",
    "        )\n",
    "\n",
    "        self.last_layers = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, output_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = self.inverted_residual_blocks(x)\n",
    "        x = self.last_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALEXNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer name is: ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
      "The number of layers is: 7\n",
      "The layer name after orged is: ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
      "The layer name is: ['Conv1', 'ReLU2', 'MaxP3', 'Conv4', 'ReLU5', 'MaxP6', 'Conv7', 'ReLU8', 'Conv9', 'ReLU10', 'Conv11', 'ReLU12', 'MaxP13', 'Adap14', 'Flat15', 'Line16', 'ReLU17', 'Drop18', 'Line19', 'ReLU20', 'Drop21', 'Line22']\n",
      "The number of layers is: 22\n"
     ]
    }
   ],
   "source": [
    "# print the model structure\n",
    "alexnet_f = alexnet(1, 10)    \n",
    "alexnet_c = alexnet(3, 10)\n",
    "# print each layer\n",
    "layer_name = []\n",
    "for layer in alexnet_f:\n",
    "    name = layer.__class__.__name__\n",
    "    layer_name.append(name)\n",
    "# find the unique layer name, and fix the order\n",
    "layer_name = sorted(list(set(layer_name)))\n",
    "print('The layer name is:', layer_name)\n",
    "# the number of layers, which contains ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
    "num_layers = len(layer_name) \n",
    "print('The number of layers is:', num_layers)\n",
    "\n",
    "alexlayer = []\n",
    "CNNlayer = []\n",
    "num = 0\n",
    "for layer in alexnet_f:\n",
    "    num += 1\n",
    "    name = layer.__class__.__name__\n",
    "    layer_name = name[:4] + str(num)\n",
    "    CNNlayer.append(name)\n",
    "    alexlayer.append(layer_name)\n",
    "# find the unique layer name, and fix the order\n",
    "CNNlayer_org = sorted(list(set(CNNlayer)))\n",
    "\n",
    "print('The layer name after orged is:', CNNlayer_org)\n",
    "print('The layer name is:', alexlayer)\n",
    "print('The number of layers is:', len(alexlayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the MACs of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Sequential(\n",
      "  57.03 M, 100.000% Params, 664.65 MMac, 99.868% MACs, \n",
      "  (0): Conv2d(7.81 k, 0.014% Params, 23.62 MMac, 3.549% MACs, 1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(0, 0.000% Params, 193.6 KMac, 0.029% MACs, )\n",
      "  (2): MaxPool2d(0, 0.000% Params, 193.6 KMac, 0.029% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(307.39 k, 0.539% Params, 224.09 MMac, 33.671% MACs, 64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(0, 0.000% Params, 139.97 KMac, 0.021% MACs, )\n",
      "  (5): MaxPool2d(0, 0.000% Params, 139.97 KMac, 0.021% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(663.94 k, 1.164% Params, 112.21 MMac, 16.859% MACs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(0, 0.000% Params, 64.9 KMac, 0.010% MACs, )\n",
      "  (8): Conv2d(884.99 k, 1.552% Params, 149.56 MMac, 22.473% MACs, 384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(0, 0.000% Params, 43.26 KMac, 0.007% MACs, )\n",
      "  (10): Conv2d(590.08 k, 1.035% Params, 99.72 MMac, 14.984% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(0, 0.000% Params, 43.26 KMac, 0.007% MACs, )\n",
      "  (12): MaxPool2d(0, 0.000% Params, 43.26 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): AdaptiveAvgPool2d(0, 0.000% Params, 9.22 KMac, 0.001% MACs, output_size=(6, 6))\n",
      "  (14): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (15): Linear(37.75 M, 66.199% Params, 37.75 MMac, 5.673% MACs, in_features=9216, out_features=4096, bias=True)\n",
      "  (16): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (17): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (18): Linear(16.78 M, 29.426% Params, 16.78 MMac, 2.521% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "  (19): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (20): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (21): Linear(40.97 k, 0.072% Params, 40.97 KMac, 0.006% MACs, in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       665.53 MMac\n",
      "Number of parameters:           57.03 M \n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Sequential(\n",
      "  57.04 M, 100.000% Params, 711.51 MMac, 99.877% MACs, \n",
      "  (0): Conv2d(23.3 k, 0.041% Params, 70.47 MMac, 9.892% MACs, 3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(0, 0.000% Params, 193.6 KMac, 0.027% MACs, )\n",
      "  (2): MaxPool2d(0, 0.000% Params, 193.6 KMac, 0.027% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(307.39 k, 0.539% Params, 224.09 MMac, 31.456% MACs, 64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(0, 0.000% Params, 139.97 KMac, 0.020% MACs, )\n",
      "  (5): MaxPool2d(0, 0.000% Params, 139.97 KMac, 0.020% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(663.94 k, 1.164% Params, 112.21 MMac, 15.751% MACs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(0, 0.000% Params, 64.9 KMac, 0.009% MACs, )\n",
      "  (8): Conv2d(884.99 k, 1.551% Params, 149.56 MMac, 20.995% MACs, 384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(0, 0.000% Params, 43.26 KMac, 0.006% MACs, )\n",
      "  (10): Conv2d(590.08 k, 1.034% Params, 99.72 MMac, 13.999% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(0, 0.000% Params, 43.26 KMac, 0.006% MACs, )\n",
      "  (12): MaxPool2d(0, 0.000% Params, 43.26 KMac, 0.006% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): AdaptiveAvgPool2d(0, 0.000% Params, 9.22 KMac, 0.001% MACs, output_size=(6, 6))\n",
      "  (14): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (15): Linear(37.75 M, 66.181% Params, 37.75 MMac, 5.299% MACs, in_features=9216, out_features=4096, bias=True)\n",
      "  (16): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (17): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (18): Linear(16.78 M, 29.418% Params, 16.78 MMac, 2.356% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "  (19): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (20): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (21): Linear(40.97 k, 0.072% Params, 40.97 KMac, 0.006% MACs, in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       712.39 MMac\n",
      "Number of parameters:           57.04 M \n"
     ]
    }
   ],
   "source": [
    "input_channel_f = 1\n",
    "input_channel_c = 3\n",
    "with torch.cuda.device(0):\n",
    "    macs_f, params_f = get_model_complexity_info(alexnet_f, (input_channel_f, 224, 224), as_strings=True,\n",
    "                                            print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_f))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params_f))\n",
    "\n",
    "# cifar100\n",
    "with torch.cuda.device(0):\n",
    "    macs_c, params_c = get_model_complexity_info(alexnet_c, (input_channel_c, 224, 224), as_strings=True,\n",
    "                                            print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_c))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the output size of each layers after the picture is passed through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 64, 55, 55])\n",
      "ReLU output shape:\t torch.Size([1, 64, 55, 55])\n",
      "MaxPool2d output shape:\t torch.Size([1, 64, 27, 27])\n",
      "Conv2d output shape:\t torch.Size([1, 192, 27, 27])\n",
      "ReLU output shape:\t torch.Size([1, 192, 27, 27])\n",
      "MaxPool2d output shape:\t torch.Size([1, 192, 13, 13])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 13, 13])\n",
      "ReLU output shape:\t torch.Size([1, 384, 13, 13])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 13, 13])\n",
      "ReLU output shape:\t torch.Size([1, 256, 13, 13])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 13, 13])\n",
      "ReLU output shape:\t torch.Size([1, 256, 13, 13])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 6, 6])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 256, 6, 6])\n",
      "Flatten output shape:\t torch.Size([1, 9216])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X_f = torch.randn(size=(1, 1, 224, 224), dtype=torch.float32) # fashion mnist\n",
    "\n",
    "for layer in alexnet_f:\n",
    "    X_f=layer(X_f)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the datas:  \n",
    "    1. FashionMNIST\n",
    "    2. CIFAR100\n",
    "    3. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# fashion mnist\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集, 然后将其加载到内存中\n",
    "\n",
    "    Defined in :numref:`sec_fashion_mnist`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "def load_data_cifar100(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    \n",
    "def load_data_cifar10(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [256]\n",
    "epochs = [2]\n",
    "rounds = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cuda\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('The device is:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using pynvml to get the GPU power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvml_sampling_thread(handle, filename, stop_event, sampling_interval):\n",
    "    \"\"\"\n",
    "    在单独的线程中定期调用 NVML, 获取功耗数据并存储到 data_queue 中。\n",
    "    参数：\n",
    "    - handle: nvmlDeviceGetHandleByIndex(0) 得到的 GPU 句柄\n",
    "    - data_queue: 用于存放 (timestamp, power_in_watts) 数据的队列\n",
    "    - stop_event: 当此事件被设置时，线程应结束循环\n",
    "    - sampling_interval: 采样间隔（秒）\n",
    "    \"\"\"\n",
    "    with open(filename/'energy_consumption_file.csv', 'a') as f:  # 追加模式\n",
    "        # 写入列名\n",
    "        f.write(\"timestamp,power_in_watts\\n\")\n",
    "        while not stop_event.is_set():\n",
    "            try:\n",
    "                # 采集功率和时间戳\n",
    "                current_time = time.time()\n",
    "                current_power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # 转换 mW -> W\n",
    "                # 写入文件\n",
    "                f.write(f\"{current_time},{current_power}\\n\")\n",
    "                # 等待下一次采样\n",
    "                time.sleep(sampling_interval)\n",
    "            except pynvml.NVMLError as e:\n",
    "                print(f\"NVML Error: {e}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the interval of the power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_power_over_interval(samples, start_time, end_time):\n",
    "    # 假定 samples是按时间升序排序的 (t, p)\n",
    "    # 若未排序，请先排序:\n",
    "    # samples = sorted(samples, key=lambda x: x[0])\n",
    "    \n",
    "    def interpolate(samples, target_time):\n",
    "        # 在 samples 中找到 target_time 左右最近的两个点，并进行线性插值\n",
    "        # 若 target_time 恰好等于某个样本点时间，直接返回该点功率\n",
    "        # 若无法找到两侧点（如 target_time在样本时间轴外），根据情况返回None或边界点\n",
    "        n = len(samples)\n",
    "        if n == 0:\n",
    "            return None\n",
    "        # 若 target_time 小于第一个样本点时间，无法向左插值，这里直接返回第一个点的功率值(或None)\n",
    "        if target_time <= samples[0][0]:\n",
    "            # 简化处理：返回最早样本点的功率（或None）\n",
    "            return samples[0][1]\n",
    "        # 若 target_time 大于最后一个样本点时间，无法向右插值，返回最后一个点的功率（或None）\n",
    "        if target_time >= samples[-1][0]:\n",
    "            return samples[-1][1]\n",
    "\n",
    "        # 否则，在中间插值\n",
    "        # 使用二分查找快速定位\n",
    "        import bisect\n",
    "        times = [t for t, _ in samples]\n",
    "        pos = bisect.bisect_left(times, target_time)\n",
    "        # pos是使times保持有序插入target_time的位置\n",
    "        # 因为target_time不在已有样本点中，pos不会越界且pos>0且pos<n\n",
    "        t1, p1 = samples[pos-1]\n",
    "        t2, p2 = samples[pos]\n",
    "        # 线性插值： p = p1 + (p2 - p1)*((target_time - t1)/(t2 - t1))\n",
    "        ratio = (target_time - t1) / (t2 - t1)\n",
    "        p = p1 + (p2 - p1)*ratio\n",
    "        return p\n",
    "\n",
    "    # 从原始 samples 中筛选出位于[start_time, end_time]内的点\n",
    "    filtered = [(t, p) for t, p in samples if start_time <= t <= end_time]\n",
    "\n",
    "    # 如果不足2个点，则尝试使用插值\n",
    "    if len(filtered) < 2:\n",
    "        # 无论如何都需要在边界处插值出两个点(起码start和end)\n",
    "        start_power = interpolate(samples, start_time)\n",
    "        end_power = interpolate(samples, end_time)\n",
    "\n",
    "        # 如果从样本中无法插值出任何有意义的点（比如samples为空或无法插值），返回0.0\n",
    "        if start_power is None or end_power is None:\n",
    "            return 0.0\n",
    "\n",
    "        # 将插值的边界点加入到 filtered\n",
    "        # 注意：如果filtered中有一个点在区间内，我们也需要确保边界有两点以上\n",
    "        # 例如filtered只有一个点在中间，则需要在start和end插值点全部加入。\n",
    "        # 若filtered为空，则只用start/end两点插值点求积分\n",
    "        new_filtered = [(start_time, start_power)] + filtered + [(end_time, end_power)]\n",
    "        # 确保按时间排序\n",
    "        new_filtered.sort(key=lambda x: x[0])\n",
    "        filtered = new_filtered\n",
    "\n",
    "    # 正常积分计算\n",
    "    if len(filtered) < 2:\n",
    "        # 经过插值仍不够，返回0\n",
    "        return 0.0\n",
    "\n",
    "    total_energy = 0.0\n",
    "    for i in range(len(filtered)-1):\n",
    "        t1, p1 = filtered[i]\n",
    "        t2, p2 = filtered[i+1]\n",
    "        dt = t2 - t1\n",
    "        avg_p = (p1 + p2)/2.0\n",
    "        total_energy += avg_p * dt\n",
    "\n",
    "    return total_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(net, train_iter, test_iter, num_epochs, lr, device, filename):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    # print(f'The name of the layers are: {alexlayer}')\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # save all epochs time data using list\n",
    "    to_device_intervals_total = []\n",
    "    forward_intervals_total = []\n",
    "    loss_intervals_total = []\n",
    "    backward_intervals_total = []\n",
    "    optimize_intervals_total = []\n",
    "    \n",
    "    # 初始化NVML和采样线程\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    power_data_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "    sampling_interval = 0.01\n",
    "    sampler_thread = threading.Thread(target=nvml_sampling_thread, args=(handle, filename, stop_event, sampling_interval))\n",
    "    sampler_thread.start()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        time.sleep(1)\n",
    "        print('The epoch is:', epoch+1)\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "        to_device_intervals_epoch = []  # 用来记录本epoch每个batch的to_device时间段\n",
    "        forward_intervals_epoch = []  # 用来记录本epoch每个batch的forward时间段\n",
    "        loss_intervals_epoch = []  # 用来记录本epoch每个batch的loss时间段\n",
    "        backward_intervals_epoch = [] \n",
    "        optimize_intervals_epoch = []\n",
    "\n",
    "        to_device_time_consump_total = 0\n",
    "        forward_time_consump_total = 0\n",
    "        loss_time_consump_total = 0\n",
    "        backward_time_consump_total = 0\n",
    "        optimize_time_consump_total = 0\n",
    "\n",
    "        to_device_energy_consump_total = 0\n",
    "        forward_energy_consump_total = 0\n",
    "        loss_energy_intervals_total = 0\n",
    "        backward_energy_consump_total = 0\n",
    "        optimize_energy_consump_total = 0\n",
    "        \n",
    "\n",
    "        # 记录epoch开始时队列中已有的数据条数，用于后面区分本epoch的数据\n",
    "        initial_queue_size = power_data_queue.qsize()\n",
    "\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            print('The batch is:', i+1)\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # 记录to_device前后的时间戳\n",
    "            start_ttd_time = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            end_ttd_time = time.time()\n",
    "            to_device_intervals_epoch.append((start_ttd_time, end_ttd_time))\n",
    "            ttd_time_consump = end_ttd_time - start_ttd_time\n",
    "            to_device_time_consump_total += ttd_time_consump\n",
    "\n",
    "            # forward\n",
    "            start_forward_time = time.time()\n",
    "            y_hat = net(X)\n",
    "            torch.cuda.synchronize()\n",
    "            end_forward_time = time.time()\n",
    "            forward_intervals_epoch.append((start_forward_time, end_forward_time))\n",
    "            forward_time_consump = end_forward_time - start_forward_time\n",
    "            forward_time_consump_total += forward_time_consump\n",
    "\n",
    "            # loss\n",
    "            start_loss_time = time.time()\n",
    "            l = loss_fn(y_hat, y)\n",
    "            torch.cuda.synchronize()\n",
    "            end_loss_time = time.time()\n",
    "            loss_intervals_epoch.append((start_loss_time, end_loss_time))\n",
    "            loss_time_consump = end_loss_time - start_loss_time\n",
    "            loss_time_consump_total += loss_time_consump\n",
    "\n",
    "            # backward\n",
    "            start_backward_time = time.time()\n",
    "            l.backward()\n",
    "            torch.cuda.synchronize()\n",
    "            end_backward_time = time.time()\n",
    "            backward_intervals_epoch.append((start_backward_time, end_backward_time))\n",
    "            backward_time_consump = end_backward_time - start_backward_time\n",
    "            backward_time_consump_total += backward_time_consump\n",
    "\n",
    "            # optimize\n",
    "            start_optimize_time = time.time()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            end_optimize_time = time.time()\n",
    "            optimize_intervals_epoch.append((start_optimize_time, end_optimize_time))\n",
    "            optimize_time_consump = end_optimize_time - start_optimize_time\n",
    "            optimize_time_consump_total += optimize_time_consump\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "\n",
    "        # data need to be saved\n",
    "        # add the intervals_epoch to intervals_total\n",
    "        to_device_intervals_total.append(to_device_intervals_epoch)\n",
    "        forward_intervals_total.append(forward_intervals_epoch)\n",
    "        loss_intervals_total.append(loss_intervals_epoch)\n",
    "        backward_intervals_total.append(backward_intervals_epoch)\n",
    "        optimize_intervals_total.append(optimize_intervals_epoch)\n",
    "\n",
    "        # data display\n",
    "        # 从队列中取出本epoch采样数据\n",
    "        epoch_samples = []\n",
    "        # 这里取出initial_queue_size之后加入的所有数据\n",
    "        total_samples_in_epoch = power_data_queue.qsize() - initial_queue_size\n",
    "        for _ in range(total_samples_in_epoch):\n",
    "            epoch_samples.append(power_data_queue.get())\n",
    "\n",
    "        # 对epoch中每个batch的to_device间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(to_device_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            to_device_energy_consump_total += batch_energy\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, to_device Energy: {batch_energy} J\")\n",
    "        \n",
    "        # 对epoch中每个batch的forward间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(forward_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            forward_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, forward Energy: {batch_energy} J\")\n",
    "\n",
    "        # 对epoch中每个batch的loss间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(loss_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            loss_energy_intervals_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, loss Energy: {batch_energy} J\")\n",
    "\n",
    "        # 对epoch中每个batch的backward间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(backward_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            backward_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, backward Energy: {batch_energy} J\")\n",
    "\n",
    "        # 对epoch中每个batch的optimize间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(optimize_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            optimize_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, optimize Energy: {batch_energy} J\")\n",
    "        \n",
    "\n",
    "        print(f'The total energy consumption in to_device part is: {to_device_energy_consump_total}, and the total time consumed is: {to_device_time_consump_total}.')\n",
    "        print(f'The total energy consumption in forward part is: {forward_energy_consump_total}, and the total time consumed is: {forward_time_consump_total}.')\n",
    "        print(f'The total energy consumption in loss part is: {loss_intervals_total}, and the total time consumed is: {loss_time_consump_total}.')\n",
    "        print(f'The total energy consumption in backward part is: {backward_energy_consump_total}, and the total time consumed is: {backward_time_consump_total}')\n",
    "        print(f'The total energy consumption in optimize part is: {optimize_energy_consump_total}, and the total time consumed is: {optimize_time_consump_total}')\n",
    "\n",
    "    # 训练结束后关闭线程\n",
    "    stop_event.set()\n",
    "    sampler_thread.join()\n",
    "\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    return to_device_intervals_total, forward_intervals_total, loss_intervals_total, backward_intervals_total, optimize_intervals_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set a function to train the model with FashionMNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_f(main_folder, batch_size, num_epochs, round, lr, device):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    # epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    epoch_batch_folder = main_folder\n",
    "    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "    # show the shape of the data\n",
    "    list_of_i = []\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        if i < 3:\n",
    "            print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "        else:\n",
    "            pass\n",
    "        list_of_i.append(i)\n",
    "    print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "    to_device_intervals_total, forward_intervals_total, loss_intervals_total,\\\n",
    "          backward_intervals_total, optimize_intervals_total = train_func(alexnet_f, train_iter, test_iter, num_epochs, lr, device, epoch_batch_folder)\n",
    "\n",
    "    # transfer the data to the numpy array\n",
    "    to_device_data = np.array(to_device_intervals_total)\n",
    "    forward_time = np.array(forward_intervals_total)\n",
    "    loss_time = np.array(loss_intervals_total)\n",
    "    backward_time = np.array(backward_intervals_total)\n",
    "    optimize_time = np.array(optimize_intervals_total)\n",
    "\n",
    "    # save the data\n",
    "    np.save(epoch_batch_folder/'to_device.npy', to_device_data, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'forward.npy', forward_time, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'loss.npy', loss_time, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'backward.npy', backward_time, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'optimize.npy', optimize_time, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is: /home/GreenAI/3080/ModelsData/alexnet\n",
      "文件不存在，已创建。\n",
      "文件创建于： /home/GreenAI/3080/ModelsData/alexnet\n",
      "The epoch is set: 2, batch is set: 256, is in 1th running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the 0 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 1 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 2 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "The number of batches is: (235,)\n",
      "training on cuda\n",
      "The epoch is: 1\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.377, test acc 0.520\n",
      "Epoch 1, Batch 1, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 2, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 3, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 4, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 5, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 6, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 7, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 8, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 9, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 10, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 11, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 12, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 13, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 14, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 15, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 16, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 17, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 18, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 19, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 20, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 21, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 22, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 23, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 24, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 25, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 26, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 27, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 28, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 29, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 30, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 31, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 32, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 33, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 34, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 35, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 36, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 37, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 38, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 39, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 40, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 41, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 42, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 43, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 44, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 45, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 46, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 47, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 48, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 49, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 50, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 51, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 52, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 53, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 54, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 55, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 56, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 57, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 58, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 59, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 60, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 61, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 62, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 63, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 64, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 65, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 66, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 67, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 68, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 69, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 70, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 71, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 72, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 73, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 74, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 75, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 76, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 77, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 78, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 79, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 80, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 81, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 82, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 83, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 84, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 85, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 86, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 87, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 88, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 89, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 90, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 91, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 92, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 93, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 94, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 95, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 96, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 97, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 98, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 99, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 100, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 101, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 102, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 103, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 104, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 105, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 106, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 107, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 108, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 109, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 110, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 111, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 112, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 113, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 114, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 115, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 116, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 117, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 118, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 119, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 120, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 121, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 122, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 123, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 124, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 125, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 126, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 127, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 128, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 129, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 130, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 131, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 132, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 133, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 134, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 135, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 136, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 137, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 138, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 139, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 140, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 141, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 142, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 143, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 144, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 145, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 146, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 147, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 148, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 149, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 150, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 151, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 152, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 153, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 154, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 155, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 156, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 157, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 158, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 159, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 160, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 161, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 162, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 163, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 164, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 165, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 166, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 167, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 168, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 169, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 170, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 171, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 172, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 173, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 174, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 175, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 176, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 177, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 178, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 179, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 180, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 181, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 182, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 183, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 184, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 185, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 186, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 187, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 188, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 189, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 190, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 191, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 192, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 193, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 194, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 195, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 196, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 197, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 198, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 199, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 200, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 201, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 202, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 203, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 204, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 205, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 206, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 207, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 208, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 209, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 210, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 211, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 212, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 213, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 214, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 215, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 216, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 217, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 218, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 219, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 220, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 221, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 222, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 223, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 224, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 225, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 226, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 227, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 228, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 229, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 230, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 231, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 232, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 233, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 234, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 235, to_device Energy: 0.0 J\n",
      "Epoch 1, Batch 1, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 2, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 3, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 4, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 5, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 6, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 7, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 8, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 9, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 10, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 11, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 12, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 13, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 14, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 15, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 16, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 17, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 18, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 19, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 20, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 21, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 22, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 23, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 24, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 25, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 26, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 27, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 28, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 29, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 30, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 31, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 32, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 33, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 34, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 35, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 36, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 37, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 38, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 39, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 40, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 41, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 42, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 43, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 44, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 45, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 46, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 47, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 48, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 49, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 50, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 51, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 52, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 53, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 54, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 55, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 56, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 57, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 58, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 59, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 60, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 61, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 62, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 63, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 64, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 65, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 66, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 67, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 68, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 69, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 70, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 71, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 72, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 73, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 74, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 75, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 76, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 77, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 78, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 79, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 80, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 81, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 82, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 83, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 84, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 85, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 86, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 87, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 88, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 89, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 90, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 91, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 92, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 93, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 94, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 95, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 96, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 97, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 98, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 99, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 100, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 101, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 102, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 103, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 104, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 105, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 106, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 107, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 108, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 109, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 110, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 111, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 112, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 113, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 114, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 115, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 116, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 117, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 118, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 119, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 120, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 121, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 122, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 123, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 124, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 125, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 126, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 127, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 128, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 129, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 130, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 131, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 132, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 133, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 134, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 135, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 136, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 137, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 138, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 139, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 140, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 141, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 142, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 143, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 144, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 145, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 146, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 147, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 148, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 149, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 150, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 151, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 152, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 153, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 154, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 155, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 156, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 157, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 158, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 159, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 160, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 161, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 162, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 163, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 164, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 165, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 166, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 167, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 168, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 169, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 170, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 171, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 172, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 173, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 174, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 175, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 176, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 177, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 178, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 179, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 180, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 181, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 182, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 183, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 184, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 185, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 186, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 187, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 188, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 189, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 190, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 191, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 192, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 193, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 194, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 195, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 196, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 197, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 198, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 199, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 200, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 201, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 202, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 203, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 204, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 205, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 206, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 207, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 208, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 209, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 210, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 211, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 212, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 213, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 214, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 215, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 216, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 217, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 218, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 219, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 220, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 221, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 222, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 223, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 224, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 225, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 226, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 227, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 228, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 229, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 230, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 231, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 232, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 233, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 234, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 235, forward Energy: 0.0 J\n",
      "Epoch 1, Batch 1, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 2, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 3, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 4, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 5, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 6, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 7, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 8, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 9, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 10, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 11, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 12, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 13, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 14, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 15, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 16, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 17, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 18, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 19, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 20, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 21, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 22, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 23, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 24, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 25, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 26, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 27, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 28, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 29, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 30, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 31, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 32, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 33, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 34, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 35, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 36, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 37, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 38, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 39, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 40, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 41, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 42, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 43, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 44, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 45, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 46, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 47, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 48, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 49, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 50, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 51, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 52, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 53, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 54, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 55, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 56, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 57, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 58, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 59, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 60, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 61, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 62, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 63, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 64, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 65, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 66, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 67, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 68, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 69, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 70, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 71, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 72, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 73, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 74, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 75, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 76, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 77, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 78, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 79, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 80, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 81, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 82, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 83, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 84, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 85, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 86, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 87, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 88, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 89, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 90, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 91, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 92, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 93, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 94, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 95, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 96, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 97, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 98, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 99, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 100, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 101, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 102, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 103, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 104, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 105, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 106, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 107, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 108, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 109, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 110, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 111, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 112, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 113, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 114, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 115, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 116, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 117, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 118, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 119, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 120, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 121, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 122, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 123, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 124, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 125, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 126, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 127, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 128, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 129, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 130, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 131, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 132, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 133, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 134, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 135, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 136, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 137, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 138, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 139, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 140, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 141, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 142, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 143, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 144, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 145, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 146, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 147, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 148, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 149, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 150, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 151, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 152, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 153, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 154, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 155, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 156, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 157, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 158, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 159, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 160, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 161, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 162, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 163, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 164, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 165, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 166, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 167, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 168, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 169, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 170, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 171, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 172, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 173, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 174, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 175, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 176, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 177, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 178, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 179, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 180, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 181, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 182, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 183, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 184, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 185, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 186, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 187, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 188, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 189, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 190, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 191, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 192, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 193, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 194, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 195, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 196, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 197, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 198, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 199, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 200, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 201, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 202, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 203, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 204, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 205, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 206, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 207, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 208, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 209, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 210, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 211, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 212, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 213, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 214, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 215, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 216, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 217, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 218, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 219, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 220, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 221, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 222, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 223, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 224, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 225, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 226, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 227, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 228, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 229, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 230, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 231, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 232, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 233, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 234, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 235, loss Energy: 0.0 J\n",
      "Epoch 1, Batch 1, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 2, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 3, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 4, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 5, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 6, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 7, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 8, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 9, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 10, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 11, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 12, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 13, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 14, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 15, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 16, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 17, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 18, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 19, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 20, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 21, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 22, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 23, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 24, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 25, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 26, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 27, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 28, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 29, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 30, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 31, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 32, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 33, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 34, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 35, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 36, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 37, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 38, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 39, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 40, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 41, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 42, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 43, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 44, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 45, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 46, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 47, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 48, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 49, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 50, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 51, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 52, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 53, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 54, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 55, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 56, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 57, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 58, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 59, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 60, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 61, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 62, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 63, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 64, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 65, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 66, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 67, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 68, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 69, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 70, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 71, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 72, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 73, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 74, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 75, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 76, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 77, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 78, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 79, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 80, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 81, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 82, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 83, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 84, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 85, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 86, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 87, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 88, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 89, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 90, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 91, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 92, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 93, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 94, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 95, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 96, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 97, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 98, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 99, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 100, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 101, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 102, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 103, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 104, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 105, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 106, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 107, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 108, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 109, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 110, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 111, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 112, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 113, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 114, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 115, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 116, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 117, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 118, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 119, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 120, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 121, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 122, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 123, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 124, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 125, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 126, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 127, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 128, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 129, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 130, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 131, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 132, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 133, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 134, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 135, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 136, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 137, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 138, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 139, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 140, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 141, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 142, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 143, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 144, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 145, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 146, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 147, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 148, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 149, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 150, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 151, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 152, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 153, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 154, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 155, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 156, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 157, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 158, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 159, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 160, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 161, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 162, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 163, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 164, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 165, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 166, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 167, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 168, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 169, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 170, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 171, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 172, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 173, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 174, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 175, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 176, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 177, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 178, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 179, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 180, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 181, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 182, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 183, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 184, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 185, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 186, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 187, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 188, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 189, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 190, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 191, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 192, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 193, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 194, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 195, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 196, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 197, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 198, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 199, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 200, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 201, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 202, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 203, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 204, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 205, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 206, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 207, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 208, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 209, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 210, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 211, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 212, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 213, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 214, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 215, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 216, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 217, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 218, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 219, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 220, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 221, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 222, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 223, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 224, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 225, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 226, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 227, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 228, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 229, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 230, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 231, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 232, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 233, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 234, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 235, backward Energy: 0.0 J\n",
      "Epoch 1, Batch 1, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 2, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 3, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 4, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 5, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 6, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 7, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 8, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 9, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 10, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 11, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 12, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 13, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 14, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 15, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 16, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 17, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 18, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 19, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 20, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 21, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 22, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 23, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 24, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 25, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 26, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 27, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 28, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 29, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 30, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 31, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 32, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 33, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 34, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 35, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 36, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 37, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 38, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 39, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 40, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 41, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 42, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 43, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 44, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 45, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 46, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 47, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 48, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 49, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 50, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 51, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 52, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 53, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 54, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 55, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 56, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 57, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 58, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 59, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 60, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 61, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 62, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 63, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 64, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 65, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 66, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 67, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 68, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 69, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 70, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 71, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 72, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 73, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 74, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 75, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 76, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 77, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 78, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 79, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 80, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 81, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 82, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 83, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 84, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 85, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 86, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 87, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 88, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 89, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 90, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 91, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 92, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 93, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 94, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 95, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 96, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 97, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 98, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 99, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 100, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 101, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 102, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 103, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 104, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 105, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 106, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 107, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 108, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 109, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 110, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 111, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 112, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 113, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 114, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 115, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 116, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 117, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 118, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 119, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 120, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 121, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 122, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 123, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 124, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 125, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 126, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 127, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 128, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 129, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 130, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 131, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 132, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 133, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 134, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 135, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 136, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 137, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 138, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 139, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 140, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 141, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 142, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 143, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 144, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 145, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 146, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 147, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 148, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 149, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 150, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 151, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 152, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 153, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 154, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 155, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 156, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 157, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 158, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 159, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 160, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 161, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 162, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 163, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 164, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 165, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 166, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 167, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 168, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 169, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 170, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 171, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 172, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 173, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 174, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 175, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 176, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 177, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 178, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 179, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 180, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 181, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 182, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 183, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 184, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 185, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 186, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 187, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 188, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 189, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 190, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 191, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 192, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 193, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 194, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 195, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 196, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 197, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 198, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 199, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 200, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 201, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 202, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 203, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 204, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 205, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 206, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 207, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 208, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 209, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 210, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 211, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 212, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 213, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 214, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 215, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 216, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 217, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 218, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 219, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 220, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 221, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 222, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 223, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 224, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 225, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 226, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 227, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 228, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 229, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 230, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 231, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 232, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 233, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 234, optimize Energy: 0.0 J\n",
      "Epoch 1, Batch 235, optimize Energy: 0.0 J\n",
      "The total energy consumption in to_device part is: 0.0, and the total time consumed is: 1.9743399620056152.\n",
      "The total energy consumption in forward part is: 0.0, and the total time consumed is: 4.480673313140869.\n",
      "The total energy consumption in loss part is: [[(1733695889.6372924, 1733695889.6581337), (1733695889.8525774, 1733695889.8528023), (1733695889.9419444, 1733695889.942145), (1733695890.0267441, 1733695890.0269563), (1733695890.1106436, 1733695890.1108162), (1733695890.1900468, 1733695890.1901975), (1733695890.2654006, 1733695890.2655468), (1733695890.3411818, 1733695890.3413417), (1733695890.4196193, 1733695890.4197764), (1733695890.4975276, 1733695890.4977179), (1733695890.57684, 1733695890.5769885), (1733695890.6563027, 1733695890.656448), (1733695890.7339916, 1733695890.734308), (1733695890.8120975, 1733695890.8122408), (1733695890.8895173, 1733695890.8896644), (1733695890.9666178, 1733695890.9667645), (1733695891.0445113, 1733695891.0446534), (1733695891.1221647, 1733695891.1223094), (1733695891.2004042, 1733695891.2005568), (1733695891.2786481, 1733695891.2787871), (1733695891.3562696, 1733695891.3564188), (1733695891.4333713, 1733695891.4335232), (1733695891.5115993, 1733695891.5117376), (1733695891.589061, 1733695891.5891972), (1733695891.6665897, 1733695891.6667295), (1733695891.7441113, 1733695891.7442486), (1733695891.8216758, 1733695891.821822), (1733695891.8995905, 1733695891.8997452), (1733695891.9776204, 1733695891.9777675), (1733695892.055538, 1733695892.0556939), (1733695892.1337183, 1733695892.1338713), (1733695892.211998, 1733695892.2121344), (1733695892.2876668, 1733695892.287812), (1733695892.3619907, 1733695892.3621023), (1733695892.4355683, 1733695892.435697), (1733695892.509908, 1733695892.5100281), (1733695892.5850675, 1733695892.5851882), (1733695892.659098, 1733695892.6592112), (1733695892.7329865, 1733695892.7331026), (1733695892.8077147, 1733695892.8078353), (1733695892.8824985, 1733695892.8826094), (1733695892.9557989, 1733695892.9559228), (1733695893.029027, 1733695893.0291338), (1733695893.1037238, 1733695893.1038432), (1733695893.1786625, 1733695893.1787846), (1733695893.2532647, 1733695893.25338), (1733695893.3285174, 1733695893.3286376), (1733695893.4033835, 1733695893.4035027), (1733695893.4776227, 1733695893.4777367), (1733695893.55256, 1733695893.5526686), (1733695893.6268084, 1733695893.626929), (1733695893.7018123, 1733695893.701931), (1733695893.7770822, 1733695893.7771995), (1733695893.8519118, 1733695893.8520477), (1733695893.9262912, 1733695893.9264112), (1733695894.0012505, 1733695894.0013666), (1733695894.0753222, 1733695894.0754404), (1733695894.1509569, 1733695894.1510785), (1733695894.2255208, 1733695894.2256396), (1733695894.3002174, 1733695894.3003252), (1733695894.3759377, 1733695894.376071), (1733695894.4507685, 1733695894.450908), (1733695894.525491, 1733695894.5256028), (1733695894.5997882, 1733695894.5999105), (1733695894.6757066, 1733695894.675834), (1733695894.7511358, 1733695894.7512634), (1733695894.8255744, 1733695894.8256881), (1733695894.9009297, 1733695894.9010625), (1733695894.9754882, 1733695894.975616), (1733695895.0503907, 1733695895.050552), (1733695895.1263244, 1733695895.1264598), (1733695895.2014172, 1733695895.2015471), (1733695895.276566, 1733695895.276693), (1733695895.3526478, 1733695895.3528056), (1733695895.4278111, 1733695895.427937), (1733695895.5039897, 1733695895.5041196), (1733695895.579233, 1733695895.5793736), (1733695895.6537607, 1733695895.6538885), (1733695895.7285469, 1733695895.7286923), (1733695895.8039124, 1733695895.8040352), (1733695895.8788908, 1733695895.879006), (1733695895.951934, 1733695895.9520478), (1733695896.0267396, 1733695896.0268545), (1733695896.1017268, 1733695896.1018522), (1733695896.1771507, 1733695896.177272), (1733695896.252458, 1733695896.2525685), (1733695896.3269942, 1733695896.3271081), (1733695896.4014645, 1733695896.4015715), (1733695896.4754148, 1733695896.475534), (1733695896.5499723, 1733695896.550082), (1733695896.6252255, 1733695896.6253448), (1733695896.7011056, 1733695896.7012281), (1733695896.776743, 1733695896.7768602), (1733695896.8514738, 1733695896.8516567), (1733695896.926104, 1733695896.9262178), (1733695897.0007744, 1733695897.0008926), (1733695897.0752842, 1733695897.0754073), (1733695897.150524, 1733695897.1506543), (1733695897.2253635, 1733695897.225482), (1733695897.2999399, 1733695897.3000631), (1733695897.3757296, 1733695897.3758483), (1733695897.4501996, 1733695897.4503198), (1733695897.525185, 1733695897.5253034), (1733695897.6007414, 1733695897.6008599), (1733695897.6751785, 1733695897.6753008), (1733695897.7497263, 1733695897.749847), (1733695897.8243904, 1733695897.8245084), (1733695897.8992841, 1733695897.8994799), (1733695897.9738133, 1733695897.973939), (1733695898.049057, 1733695898.0491693), (1733695898.1242065, 1733695898.1243234), (1733695898.199092, 1733695898.1992135), (1733695898.2740724, 1733695898.2741942), (1733695898.3484147, 1733695898.3485374), (1733695898.422434, 1733695898.4225538), (1733695898.4975646, 1733695898.497687), (1733695898.5723462, 1733695898.572469), (1733695898.646982, 1733695898.6470928), (1733695898.7226562, 1733695898.7227752), (1733695898.7977567, 1733695898.797878), (1733695898.8733788, 1733695898.8734982), (1733695898.9483047, 1733695898.9484234), (1733695899.0224369, 1733695899.0225484), (1733695899.0977974, 1733695899.0979218), (1733695899.1722038, 1733695899.1723208), (1733695899.2475288, 1733695899.2476513), (1733695899.322106, 1733695899.3222237), (1733695899.3967087, 1733695899.3968248), (1733695899.4714355, 1733695899.4715567), (1733695899.545335, 1733695899.545447), (1733695899.6197622, 1733695899.6198711), (1733695899.6936407, 1733695899.6937623), (1733695899.7685432, 1733695899.768668), (1733695899.843735, 1733695899.8438578), (1733695899.9192834, 1733695899.919416), (1733695899.9947886, 1733695899.9949017), (1733695900.0715141, 1733695900.0716484), (1733695900.1473777, 1733695900.1474986), (1733695900.2219741, 1733695900.2221193), (1733695900.2973728, 1733695900.297493), (1733695900.3736193, 1733695900.3737433), (1733695900.4479208, 1733695900.4480429), (1733695900.5232458, 1733695900.523442), (1733695900.598582, 1733695900.5986912), (1733695900.6728752, 1733695900.6729934), (1733695900.7480688, 1733695900.7481873), (1733695900.8222919, 1733695900.822401), (1733695900.8966327, 1733695900.896751), (1733695900.9715846, 1733695900.971692), (1733695901.0471108, 1733695901.0472288), (1733695901.1216228, 1733695901.1217318), (1733695901.196552, 1733695901.1966693), (1733695901.2719626, 1733695901.2720838), (1733695901.3468065, 1733695901.346925), (1733695901.4216087, 1733695901.4217215), (1733695901.4957087, 1733695901.4958339), (1733695901.570719, 1733695901.5708408), (1733695901.6452663, 1733695901.645383), (1733695901.7198992, 1733695901.7200077), (1733695901.794845, 1733695901.7949643), (1733695901.8701828, 1733695901.870303), (1733695901.945469, 1733695901.9456015), (1733695902.0199435, 1733695902.0200682), (1733695902.0943677, 1733695902.0944865), (1733695902.1695454, 1733695902.1696606), (1733695902.2442074, 1733695902.244322), (1733695902.3189497, 1733695902.3190632), (1733695902.3937328, 1733695902.3938508), (1733695902.4683256, 1733695902.4684331), (1733695902.543171, 1733695902.5432901), (1733695902.616671, 1733695902.616797), (1733695902.6913881, 1733695902.6915066), (1733695902.7671366, 1733695902.7672553), (1733695902.8415744, 1733695902.8416834), (1733695902.916083, 1733695902.9162002), (1733695902.9913843, 1733695902.991501), (1733695903.0661917, 1733695903.066297), (1733695903.140544, 1733695903.1406608), (1733695903.2150736, 1733695903.2151814), (1733695903.2898538, 1733695903.28997), (1733695903.3647463, 1733695903.3648546), (1733695903.43851, 1733695903.438623), (1733695903.5128143, 1733695903.5129359), (1733695903.5881195, 1733695903.5882394), (1733695903.6646893, 1733695903.6648107), (1733695903.7398684, 1733695903.7399805), (1733695903.814424, 1733695903.8145323), (1733695903.8895104, 1733695903.8896441), (1733695903.9642067, 1733695903.9643164), (1733695904.038926, 1733695904.0390391), (1733695904.1132646, 1733695904.1133823), (1733695904.18777, 1733695904.1878872), (1733695904.2631454, 1733695904.2632546), (1733695904.3383973, 1733695904.3385248), (1733695904.413444, 1733695904.413565), (1733695904.488392, 1733695904.4885106), (1733695904.5640423, 1733695904.564159), (1733695904.6391087, 1733695904.6392202), (1733695904.7134986, 1733695904.7136142), (1733695904.7890441, 1733695904.7893918), (1733695904.8642716, 1733695904.8643944), (1733695904.9392977, 1733695904.9394994), (1733695905.0135448, 1733695905.0136657), (1733695905.0897908, 1733695905.08992), (1733695905.165372, 1733695905.1655025), (1733695905.2409818, 1733695905.241106), (1733695905.3148596, 1733695905.3149858), (1733695905.3898375, 1733695905.3899686), (1733695905.4654896, 1733695905.465609), (1733695905.540852, 1733695905.540977), (1733695905.615043, 1733695905.6151586), (1733695905.6901135, 1733695905.6902332), (1733695905.7650638, 1733695905.76518), (1733695905.8419626, 1733695905.842171), (1733695905.917646, 1733695905.917761), (1733695905.99242, 1733695905.992549), (1733695906.0680883, 1733695906.0682178), (1733695906.1438575, 1733695906.1439755), (1733695906.2188082, 1733695906.2189186), (1733695906.2936764, 1733695906.2937908), (1733695906.3695676, 1733695906.3696861), (1733695906.444324, 1733695906.4444351), (1733695906.5182126, 1733695906.5183215), (1733695906.592179, 1733695906.5922978), (1733695906.6671076, 1733695906.667218), (1733695906.7421956, 1733695906.742318), (1733695906.8214912, 1733695906.8216078), (1733695906.8960605, 1733695906.8961818), (1733695906.9704232, 1733695906.9705343), (1733695907.0447602, 1733695907.0448694), (1733695907.1179652, 1733695907.1180723), (1733695907.1908603, 1733695907.1909645), (1733695907.2646031, 1733695907.2646983), (1733695907.3395066, 1733695907.3396099), (1733695907.3988726, 1733695907.3989666)]], and the total time consumed is: 0.05068397521972656.\n",
      "The total energy consumption in backward part is: 0.0, and the total time consumed is: 8.494925022125244\n",
      "The total energy consumption in optimize part is: 0.0, and the total time consumed is: 0.30193352699279785\n",
      "The epoch is: 2\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.699, test acc 0.761\n",
      "Epoch 2, Batch 1, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 2, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 3, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 4, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 5, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 6, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 7, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 8, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 9, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 10, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 11, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 12, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 13, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 14, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 15, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 16, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 17, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 18, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 19, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 20, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 21, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 22, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 23, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 24, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 25, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 26, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 27, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 28, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 29, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 30, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 31, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 32, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 33, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 34, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 35, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 36, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 37, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 38, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 39, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 40, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 41, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 42, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 43, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 44, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 45, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 46, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 47, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 48, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 49, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 50, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 51, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 52, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 53, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 54, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 55, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 56, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 57, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 58, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 59, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 60, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 61, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 62, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 63, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 64, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 65, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 66, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 67, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 68, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 69, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 70, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 71, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 72, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 73, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 74, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 75, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 76, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 77, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 78, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 79, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 80, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 81, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 82, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 83, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 84, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 85, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 86, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 87, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 88, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 89, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 90, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 91, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 92, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 93, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 94, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 95, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 96, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 97, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 98, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 99, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 100, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 101, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 102, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 103, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 104, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 105, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 106, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 107, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 108, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 109, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 110, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 111, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 112, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 113, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 114, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 115, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 116, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 117, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 118, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 119, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 120, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 121, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 122, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 123, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 124, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 125, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 126, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 127, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 128, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 129, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 130, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 131, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 132, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 133, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 134, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 135, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 136, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 137, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 138, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 139, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 140, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 141, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 142, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 143, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 144, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 145, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 146, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 147, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 148, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 149, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 150, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 151, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 152, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 153, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 154, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 155, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 156, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 157, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 158, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 159, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 160, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 161, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 162, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 163, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 164, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 165, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 166, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 167, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 168, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 169, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 170, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 171, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 172, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 173, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 174, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 175, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 176, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 177, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 178, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 179, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 180, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 181, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 182, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 183, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 184, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 185, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 186, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 187, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 188, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 189, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 190, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 191, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 192, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 193, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 194, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 195, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 196, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 197, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 198, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 199, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 200, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 201, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 202, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 203, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 204, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 205, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 206, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 207, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 208, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 209, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 210, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 211, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 212, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 213, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 214, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 215, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 216, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 217, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 218, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 219, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 220, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 221, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 222, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 223, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 224, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 225, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 226, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 227, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 228, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 229, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 230, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 231, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 232, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 233, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 234, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 235, to_device Energy: 0.0 J\n",
      "Epoch 2, Batch 1, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 2, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 3, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 4, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 5, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 6, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 7, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 8, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 9, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 10, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 11, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 12, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 13, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 14, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 15, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 16, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 17, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 18, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 19, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 20, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 21, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 22, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 23, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 24, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 25, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 26, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 27, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 28, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 29, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 30, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 31, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 32, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 33, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 34, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 35, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 36, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 37, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 38, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 39, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 40, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 41, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 42, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 43, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 44, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 45, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 46, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 47, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 48, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 49, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 50, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 51, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 52, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 53, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 54, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 55, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 56, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 57, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 58, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 59, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 60, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 61, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 62, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 63, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 64, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 65, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 66, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 67, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 68, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 69, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 70, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 71, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 72, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 73, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 74, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 75, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 76, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 77, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 78, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 79, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 80, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 81, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 82, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 83, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 84, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 85, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 86, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 87, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 88, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 89, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 90, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 91, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 92, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 93, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 94, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 95, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 96, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 97, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 98, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 99, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 100, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 101, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 102, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 103, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 104, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 105, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 106, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 107, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 108, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 109, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 110, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 111, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 112, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 113, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 114, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 115, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 116, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 117, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 118, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 119, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 120, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 121, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 122, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 123, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 124, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 125, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 126, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 127, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 128, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 129, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 130, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 131, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 132, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 133, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 134, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 135, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 136, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 137, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 138, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 139, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 140, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 141, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 142, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 143, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 144, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 145, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 146, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 147, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 148, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 149, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 150, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 151, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 152, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 153, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 154, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 155, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 156, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 157, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 158, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 159, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 160, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 161, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 162, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 163, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 164, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 165, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 166, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 167, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 168, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 169, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 170, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 171, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 172, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 173, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 174, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 175, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 176, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 177, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 178, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 179, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 180, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 181, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 182, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 183, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 184, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 185, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 186, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 187, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 188, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 189, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 190, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 191, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 192, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 193, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 194, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 195, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 196, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 197, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 198, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 199, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 200, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 201, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 202, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 203, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 204, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 205, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 206, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 207, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 208, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 209, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 210, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 211, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 212, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 213, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 214, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 215, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 216, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 217, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 218, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 219, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 220, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 221, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 222, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 223, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 224, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 225, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 226, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 227, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 228, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 229, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 230, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 231, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 232, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 233, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 234, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 235, forward Energy: 0.0 J\n",
      "Epoch 2, Batch 1, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 2, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 3, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 4, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 5, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 6, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 7, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 8, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 9, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 10, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 11, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 12, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 13, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 14, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 15, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 16, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 17, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 18, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 19, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 20, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 21, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 22, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 23, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 24, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 25, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 26, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 27, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 28, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 29, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 30, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 31, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 32, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 33, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 34, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 35, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 36, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 37, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 38, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 39, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 40, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 41, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 42, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 43, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 44, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 45, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 46, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 47, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 48, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 49, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 50, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 51, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 52, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 53, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 54, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 55, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 56, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 57, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 58, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 59, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 60, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 61, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 62, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 63, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 64, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 65, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 66, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 67, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 68, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 69, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 70, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 71, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 72, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 73, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 74, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 75, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 76, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 77, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 78, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 79, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 80, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 81, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 82, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 83, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 84, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 85, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 86, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 87, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 88, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 89, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 90, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 91, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 92, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 93, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 94, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 95, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 96, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 97, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 98, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 99, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 100, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 101, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 102, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 103, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 104, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 105, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 106, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 107, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 108, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 109, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 110, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 111, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 112, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 113, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 114, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 115, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 116, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 117, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 118, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 119, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 120, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 121, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 122, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 123, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 124, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 125, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 126, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 127, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 128, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 129, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 130, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 131, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 132, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 133, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 134, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 135, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 136, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 137, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 138, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 139, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 140, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 141, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 142, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 143, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 144, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 145, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 146, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 147, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 148, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 149, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 150, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 151, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 152, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 153, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 154, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 155, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 156, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 157, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 158, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 159, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 160, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 161, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 162, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 163, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 164, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 165, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 166, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 167, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 168, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 169, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 170, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 171, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 172, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 173, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 174, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 175, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 176, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 177, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 178, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 179, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 180, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 181, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 182, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 183, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 184, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 185, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 186, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 187, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 188, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 189, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 190, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 191, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 192, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 193, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 194, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 195, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 196, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 197, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 198, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 199, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 200, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 201, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 202, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 203, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 204, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 205, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 206, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 207, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 208, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 209, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 210, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 211, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 212, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 213, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 214, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 215, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 216, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 217, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 218, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 219, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 220, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 221, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 222, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 223, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 224, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 225, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 226, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 227, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 228, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 229, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 230, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 231, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 232, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 233, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 234, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 235, loss Energy: 0.0 J\n",
      "Epoch 2, Batch 1, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 2, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 3, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 4, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 5, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 6, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 7, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 8, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 9, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 10, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 11, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 12, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 13, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 14, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 15, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 16, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 17, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 18, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 19, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 20, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 21, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 22, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 23, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 24, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 25, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 26, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 27, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 28, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 29, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 30, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 31, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 32, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 33, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 34, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 35, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 36, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 37, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 38, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 39, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 40, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 41, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 42, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 43, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 44, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 45, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 46, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 47, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 48, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 49, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 50, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 51, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 52, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 53, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 54, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 55, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 56, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 57, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 58, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 59, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 60, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 61, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 62, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 63, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 64, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 65, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 66, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 67, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 68, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 69, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 70, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 71, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 72, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 73, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 74, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 75, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 76, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 77, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 78, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 79, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 80, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 81, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 82, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 83, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 84, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 85, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 86, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 87, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 88, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 89, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 90, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 91, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 92, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 93, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 94, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 95, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 96, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 97, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 98, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 99, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 100, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 101, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 102, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 103, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 104, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 105, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 106, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 107, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 108, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 109, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 110, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 111, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 112, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 113, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 114, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 115, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 116, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 117, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 118, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 119, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 120, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 121, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 122, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 123, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 124, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 125, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 126, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 127, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 128, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 129, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 130, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 131, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 132, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 133, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 134, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 135, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 136, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 137, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 138, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 139, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 140, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 141, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 142, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 143, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 144, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 145, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 146, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 147, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 148, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 149, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 150, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 151, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 152, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 153, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 154, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 155, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 156, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 157, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 158, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 159, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 160, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 161, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 162, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 163, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 164, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 165, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 166, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 167, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 168, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 169, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 170, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 171, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 172, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 173, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 174, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 175, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 176, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 177, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 178, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 179, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 180, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 181, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 182, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 183, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 184, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 185, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 186, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 187, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 188, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 189, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 190, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 191, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 192, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 193, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 194, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 195, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 196, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 197, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 198, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 199, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 200, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 201, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 202, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 203, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 204, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 205, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 206, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 207, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 208, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 209, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 210, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 211, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 212, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 213, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 214, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 215, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 216, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 217, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 218, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 219, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 220, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 221, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 222, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 223, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 224, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 225, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 226, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 227, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 228, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 229, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 230, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 231, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 232, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 233, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 234, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 235, backward Energy: 0.0 J\n",
      "Epoch 2, Batch 1, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 2, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 3, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 4, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 5, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 6, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 7, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 8, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 9, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 10, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 11, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 12, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 13, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 14, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 15, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 16, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 17, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 18, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 19, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 20, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 21, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 22, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 23, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 24, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 25, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 26, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 27, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 28, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 29, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 30, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 31, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 32, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 33, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 34, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 35, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 36, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 37, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 38, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 39, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 40, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 41, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 42, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 43, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 44, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 45, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 46, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 47, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 48, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 49, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 50, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 51, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 52, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 53, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 54, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 55, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 56, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 57, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 58, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 59, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 60, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 61, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 62, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 63, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 64, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 65, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 66, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 67, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 68, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 69, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 70, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 71, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 72, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 73, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 74, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 75, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 76, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 77, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 78, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 79, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 80, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 81, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 82, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 83, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 84, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 85, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 86, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 87, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 88, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 89, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 90, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 91, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 92, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 93, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 94, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 95, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 96, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 97, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 98, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 99, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 100, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 101, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 102, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 103, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 104, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 105, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 106, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 107, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 108, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 109, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 110, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 111, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 112, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 113, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 114, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 115, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 116, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 117, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 118, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 119, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 120, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 121, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 122, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 123, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 124, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 125, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 126, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 127, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 128, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 129, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 130, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 131, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 132, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 133, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 134, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 135, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 136, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 137, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 138, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 139, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 140, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 141, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 142, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 143, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 144, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 145, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 146, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 147, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 148, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 149, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 150, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 151, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 152, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 153, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 154, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 155, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 156, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 157, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 158, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 159, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 160, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 161, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 162, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 163, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 164, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 165, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 166, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 167, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 168, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 169, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 170, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 171, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 172, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 173, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 174, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 175, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 176, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 177, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 178, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 179, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 180, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 181, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 182, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 183, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 184, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 185, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 186, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 187, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 188, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 189, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 190, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 191, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 192, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 193, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 194, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 195, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 196, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 197, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 198, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 199, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 200, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 201, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 202, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 203, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 204, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 205, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 206, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 207, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 208, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 209, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 210, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 211, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 212, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 213, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 214, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 215, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 216, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 217, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 218, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 219, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 220, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 221, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 222, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 223, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 224, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 225, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 226, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 227, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 228, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 229, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 230, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 231, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 232, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 233, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 234, optimize Energy: 0.0 J\n",
      "Epoch 2, Batch 235, optimize Energy: 0.0 J\n",
      "The total energy consumption in to_device part is: 0.0, and the total time consumed is: 1.9026882648468018.\n",
      "The total energy consumption in forward part is: 0.0, and the total time consumed is: 4.218000173568726.\n",
      "The total energy consumption in loss part is: [[(1733695889.6372924, 1733695889.6581337), (1733695889.8525774, 1733695889.8528023), (1733695889.9419444, 1733695889.942145), (1733695890.0267441, 1733695890.0269563), (1733695890.1106436, 1733695890.1108162), (1733695890.1900468, 1733695890.1901975), (1733695890.2654006, 1733695890.2655468), (1733695890.3411818, 1733695890.3413417), (1733695890.4196193, 1733695890.4197764), (1733695890.4975276, 1733695890.4977179), (1733695890.57684, 1733695890.5769885), (1733695890.6563027, 1733695890.656448), (1733695890.7339916, 1733695890.734308), (1733695890.8120975, 1733695890.8122408), (1733695890.8895173, 1733695890.8896644), (1733695890.9666178, 1733695890.9667645), (1733695891.0445113, 1733695891.0446534), (1733695891.1221647, 1733695891.1223094), (1733695891.2004042, 1733695891.2005568), (1733695891.2786481, 1733695891.2787871), (1733695891.3562696, 1733695891.3564188), (1733695891.4333713, 1733695891.4335232), (1733695891.5115993, 1733695891.5117376), (1733695891.589061, 1733695891.5891972), (1733695891.6665897, 1733695891.6667295), (1733695891.7441113, 1733695891.7442486), (1733695891.8216758, 1733695891.821822), (1733695891.8995905, 1733695891.8997452), (1733695891.9776204, 1733695891.9777675), (1733695892.055538, 1733695892.0556939), (1733695892.1337183, 1733695892.1338713), (1733695892.211998, 1733695892.2121344), (1733695892.2876668, 1733695892.287812), (1733695892.3619907, 1733695892.3621023), (1733695892.4355683, 1733695892.435697), (1733695892.509908, 1733695892.5100281), (1733695892.5850675, 1733695892.5851882), (1733695892.659098, 1733695892.6592112), (1733695892.7329865, 1733695892.7331026), (1733695892.8077147, 1733695892.8078353), (1733695892.8824985, 1733695892.8826094), (1733695892.9557989, 1733695892.9559228), (1733695893.029027, 1733695893.0291338), (1733695893.1037238, 1733695893.1038432), (1733695893.1786625, 1733695893.1787846), (1733695893.2532647, 1733695893.25338), (1733695893.3285174, 1733695893.3286376), (1733695893.4033835, 1733695893.4035027), (1733695893.4776227, 1733695893.4777367), (1733695893.55256, 1733695893.5526686), (1733695893.6268084, 1733695893.626929), (1733695893.7018123, 1733695893.701931), (1733695893.7770822, 1733695893.7771995), (1733695893.8519118, 1733695893.8520477), (1733695893.9262912, 1733695893.9264112), (1733695894.0012505, 1733695894.0013666), (1733695894.0753222, 1733695894.0754404), (1733695894.1509569, 1733695894.1510785), (1733695894.2255208, 1733695894.2256396), (1733695894.3002174, 1733695894.3003252), (1733695894.3759377, 1733695894.376071), (1733695894.4507685, 1733695894.450908), (1733695894.525491, 1733695894.5256028), (1733695894.5997882, 1733695894.5999105), (1733695894.6757066, 1733695894.675834), (1733695894.7511358, 1733695894.7512634), (1733695894.8255744, 1733695894.8256881), (1733695894.9009297, 1733695894.9010625), (1733695894.9754882, 1733695894.975616), (1733695895.0503907, 1733695895.050552), (1733695895.1263244, 1733695895.1264598), (1733695895.2014172, 1733695895.2015471), (1733695895.276566, 1733695895.276693), (1733695895.3526478, 1733695895.3528056), (1733695895.4278111, 1733695895.427937), (1733695895.5039897, 1733695895.5041196), (1733695895.579233, 1733695895.5793736), (1733695895.6537607, 1733695895.6538885), (1733695895.7285469, 1733695895.7286923), (1733695895.8039124, 1733695895.8040352), (1733695895.8788908, 1733695895.879006), (1733695895.951934, 1733695895.9520478), (1733695896.0267396, 1733695896.0268545), (1733695896.1017268, 1733695896.1018522), (1733695896.1771507, 1733695896.177272), (1733695896.252458, 1733695896.2525685), (1733695896.3269942, 1733695896.3271081), (1733695896.4014645, 1733695896.4015715), (1733695896.4754148, 1733695896.475534), (1733695896.5499723, 1733695896.550082), (1733695896.6252255, 1733695896.6253448), (1733695896.7011056, 1733695896.7012281), (1733695896.776743, 1733695896.7768602), (1733695896.8514738, 1733695896.8516567), (1733695896.926104, 1733695896.9262178), (1733695897.0007744, 1733695897.0008926), (1733695897.0752842, 1733695897.0754073), (1733695897.150524, 1733695897.1506543), (1733695897.2253635, 1733695897.225482), (1733695897.2999399, 1733695897.3000631), (1733695897.3757296, 1733695897.3758483), (1733695897.4501996, 1733695897.4503198), (1733695897.525185, 1733695897.5253034), (1733695897.6007414, 1733695897.6008599), (1733695897.6751785, 1733695897.6753008), (1733695897.7497263, 1733695897.749847), (1733695897.8243904, 1733695897.8245084), (1733695897.8992841, 1733695897.8994799), (1733695897.9738133, 1733695897.973939), (1733695898.049057, 1733695898.0491693), (1733695898.1242065, 1733695898.1243234), (1733695898.199092, 1733695898.1992135), (1733695898.2740724, 1733695898.2741942), (1733695898.3484147, 1733695898.3485374), (1733695898.422434, 1733695898.4225538), (1733695898.4975646, 1733695898.497687), (1733695898.5723462, 1733695898.572469), (1733695898.646982, 1733695898.6470928), (1733695898.7226562, 1733695898.7227752), (1733695898.7977567, 1733695898.797878), (1733695898.8733788, 1733695898.8734982), (1733695898.9483047, 1733695898.9484234), (1733695899.0224369, 1733695899.0225484), (1733695899.0977974, 1733695899.0979218), (1733695899.1722038, 1733695899.1723208), (1733695899.2475288, 1733695899.2476513), (1733695899.322106, 1733695899.3222237), (1733695899.3967087, 1733695899.3968248), (1733695899.4714355, 1733695899.4715567), (1733695899.545335, 1733695899.545447), (1733695899.6197622, 1733695899.6198711), (1733695899.6936407, 1733695899.6937623), (1733695899.7685432, 1733695899.768668), (1733695899.843735, 1733695899.8438578), (1733695899.9192834, 1733695899.919416), (1733695899.9947886, 1733695899.9949017), (1733695900.0715141, 1733695900.0716484), (1733695900.1473777, 1733695900.1474986), (1733695900.2219741, 1733695900.2221193), (1733695900.2973728, 1733695900.297493), (1733695900.3736193, 1733695900.3737433), (1733695900.4479208, 1733695900.4480429), (1733695900.5232458, 1733695900.523442), (1733695900.598582, 1733695900.5986912), (1733695900.6728752, 1733695900.6729934), (1733695900.7480688, 1733695900.7481873), (1733695900.8222919, 1733695900.822401), (1733695900.8966327, 1733695900.896751), (1733695900.9715846, 1733695900.971692), (1733695901.0471108, 1733695901.0472288), (1733695901.1216228, 1733695901.1217318), (1733695901.196552, 1733695901.1966693), (1733695901.2719626, 1733695901.2720838), (1733695901.3468065, 1733695901.346925), (1733695901.4216087, 1733695901.4217215), (1733695901.4957087, 1733695901.4958339), (1733695901.570719, 1733695901.5708408), (1733695901.6452663, 1733695901.645383), (1733695901.7198992, 1733695901.7200077), (1733695901.794845, 1733695901.7949643), (1733695901.8701828, 1733695901.870303), (1733695901.945469, 1733695901.9456015), (1733695902.0199435, 1733695902.0200682), (1733695902.0943677, 1733695902.0944865), (1733695902.1695454, 1733695902.1696606), (1733695902.2442074, 1733695902.244322), (1733695902.3189497, 1733695902.3190632), (1733695902.3937328, 1733695902.3938508), (1733695902.4683256, 1733695902.4684331), (1733695902.543171, 1733695902.5432901), (1733695902.616671, 1733695902.616797), (1733695902.6913881, 1733695902.6915066), (1733695902.7671366, 1733695902.7672553), (1733695902.8415744, 1733695902.8416834), (1733695902.916083, 1733695902.9162002), (1733695902.9913843, 1733695902.991501), (1733695903.0661917, 1733695903.066297), (1733695903.140544, 1733695903.1406608), (1733695903.2150736, 1733695903.2151814), (1733695903.2898538, 1733695903.28997), (1733695903.3647463, 1733695903.3648546), (1733695903.43851, 1733695903.438623), (1733695903.5128143, 1733695903.5129359), (1733695903.5881195, 1733695903.5882394), (1733695903.6646893, 1733695903.6648107), (1733695903.7398684, 1733695903.7399805), (1733695903.814424, 1733695903.8145323), (1733695903.8895104, 1733695903.8896441), (1733695903.9642067, 1733695903.9643164), (1733695904.038926, 1733695904.0390391), (1733695904.1132646, 1733695904.1133823), (1733695904.18777, 1733695904.1878872), (1733695904.2631454, 1733695904.2632546), (1733695904.3383973, 1733695904.3385248), (1733695904.413444, 1733695904.413565), (1733695904.488392, 1733695904.4885106), (1733695904.5640423, 1733695904.564159), (1733695904.6391087, 1733695904.6392202), (1733695904.7134986, 1733695904.7136142), (1733695904.7890441, 1733695904.7893918), (1733695904.8642716, 1733695904.8643944), (1733695904.9392977, 1733695904.9394994), (1733695905.0135448, 1733695905.0136657), (1733695905.0897908, 1733695905.08992), (1733695905.165372, 1733695905.1655025), (1733695905.2409818, 1733695905.241106), (1733695905.3148596, 1733695905.3149858), (1733695905.3898375, 1733695905.3899686), (1733695905.4654896, 1733695905.465609), (1733695905.540852, 1733695905.540977), (1733695905.615043, 1733695905.6151586), (1733695905.6901135, 1733695905.6902332), (1733695905.7650638, 1733695905.76518), (1733695905.8419626, 1733695905.842171), (1733695905.917646, 1733695905.917761), (1733695905.99242, 1733695905.992549), (1733695906.0680883, 1733695906.0682178), (1733695906.1438575, 1733695906.1439755), (1733695906.2188082, 1733695906.2189186), (1733695906.2936764, 1733695906.2937908), (1733695906.3695676, 1733695906.3696861), (1733695906.444324, 1733695906.4444351), (1733695906.5182126, 1733695906.5183215), (1733695906.592179, 1733695906.5922978), (1733695906.6671076, 1733695906.667218), (1733695906.7421956, 1733695906.742318), (1733695906.8214912, 1733695906.8216078), (1733695906.8960605, 1733695906.8961818), (1733695906.9704232, 1733695906.9705343), (1733695907.0447602, 1733695907.0448694), (1733695907.1179652, 1733695907.1180723), (1733695907.1908603, 1733695907.1909645), (1733695907.2646031, 1733695907.2646983), (1733695907.3395066, 1733695907.3396099), (1733695907.3988726, 1733695907.3989666)], [(1733695910.6495607, 1733695910.6498885), (1733695910.738279, 1733695910.7384408), (1733695910.8151348, 1733695910.8152895), (1733695910.8926213, 1733695910.892775), (1733695910.9714818, 1733695910.9716418), (1733695911.0513449, 1733695911.051511), (1733695911.128243, 1733695911.1283987), (1733695911.204385, 1733695911.2045388), (1733695911.283244, 1733695911.2834303), (1733695911.362811, 1733695911.3629642), (1733695911.4408522, 1733695911.4410152), (1733695911.5169394, 1733695911.517087), (1733695911.5953224, 1733695911.5954864), (1733695911.6732907, 1733695911.673447), (1733695911.7513454, 1733695911.7514944), (1733695911.8291779, 1733695911.829326), (1733695911.9064598, 1733695911.9066107), (1733695911.9838524, 1733695911.984006), (1733695912.0625432, 1733695912.0626972), (1733695912.1407828, 1733695912.1409283), (1733695912.218556, 1733695912.218706), (1733695912.296462, 1733695912.296608), (1733695912.374801, 1733695912.3749614), (1733695912.452695, 1733695912.4528484), (1733695912.5308645, 1733695912.5310104), (1733695912.608102, 1733695912.6082509), (1733695912.6862419, 1733695912.6863945), (1733695912.7645266, 1733695912.7646842), (1733695912.8421412, 1733695912.8422942), (1733695912.9197202, 1733695912.9198833), (1733695912.998191, 1733695912.9983456), (1733695913.0763538, 1733695913.0765064), (1733695913.1539702, 1733695913.154117), (1733695913.2313037, 1733695913.23155), (1733695913.3099828, 1733695913.310134), (1733695913.3884099, 1733695913.3885665), (1733695913.4628072, 1733695913.4629405), (1733695913.5369048, 1733695913.5370393), (1733695913.6116986, 1733695913.6118295), (1733695913.6872156, 1733695913.6873467), (1733695913.7613957, 1733695913.7615159), (1733695913.8358614, 1733695913.8359828), (1733695913.910846, 1733695913.9109812), (1733695913.9861455, 1733695913.986275), (1733695914.0604405, 1733695914.0605638), (1733695914.136125, 1733695914.1362407), (1733695914.2104387, 1733695914.2105656), (1733695914.2841299, 1733695914.284243), (1733695914.3585742, 1733695914.358697), (1733695914.4326599, 1733695914.4327757), (1733695914.50678, 1733695914.5069063), (1733695914.5818925, 1733695914.582009), (1733695914.6550233, 1733695914.6551518), (1733695914.7288883, 1733695914.7290137), (1733695914.8028579, 1733695914.8029842), (1733695914.8772447, 1733695914.87737), (1733695914.9517627, 1733695914.9518843), (1733695915.0259182, 1733695915.026064), (1733695915.1018813, 1733695915.1020188), (1733695915.177849, 1733695915.1779842), (1733695915.2528002, 1733695915.2529263), (1733695915.3266861, 1733695915.326806), (1733695915.401214, 1733695915.401341), (1733695915.4764168, 1733695915.476522), (1733695915.5499241, 1733695915.5500326), (1733695915.6246102, 1733695915.6247175), (1733695915.6998768, 1733695915.6999924), (1733695915.7744396, 1733695915.7745588), (1733695915.8472707, 1733695915.8473866), (1733695915.9205914, 1733695915.9206994), (1733695915.9948785, 1733695915.9949875), (1733695916.0696843, 1733695916.0698078), (1733695916.1453884, 1733695916.1455064), (1733695916.220365, 1733695916.220483), (1733695916.295396, 1733695916.2955112), (1733695916.370204, 1733695916.370312), (1733695916.4439268, 1733695916.4440324), (1733695916.517414, 1733695916.5175304), (1733695916.5926085, 1733695916.5927317), (1733695916.6676955, 1733695916.6678228), (1733695916.7426112, 1733695916.742735), (1733695916.817525, 1733695916.8176472), (1733695916.892228, 1733695916.8923392), (1733695916.9654355, 1733695916.965553), (1733695917.0401285, 1733695917.0402367), (1733695917.1128292, 1733695917.112952), (1733695917.1868222, 1733695917.1869392), (1733695917.261792, 1733695917.2619066), (1733695917.3363378, 1733695917.3364437), (1733695917.4108627, 1733695917.410982), (1733695917.4860969, 1733695917.4862244), (1733695917.5608075, 1733695917.560916), (1733695917.6347616, 1733695917.6348765), (1733695917.708021, 1733695917.708139), (1733695917.7826877, 1733695917.7828047), (1733695917.8570957, 1733695917.8572154), (1733695917.9318218, 1733695917.931938), (1733695918.0060203, 1733695918.0061398), (1733695918.080599, 1733695918.0807137), (1733695918.156295, 1733695918.1564078), (1733695918.2307277, 1733695918.2308462), (1733695918.30424, 1733695918.3043473), (1733695918.3789015, 1733695918.3790193), (1733695918.453352, 1733695918.4534664), (1733695918.5273528, 1733695918.5274634), (1733695918.6003563, 1733695918.6004703), (1733695918.6747522, 1733695918.6748621), (1733695918.7489529, 1733695918.7490704), (1733695918.8237412, 1733695918.8238494), (1733695918.8985279, 1733695918.898649), (1733695918.972925, 1733695918.9730446), (1733695919.0477657, 1733695919.0478706), (1733695919.1241627, 1733695919.1242907), (1733695919.197388, 1733695919.1974921), (1733695919.2727654, 1733695919.272883), (1733695919.348821, 1733695919.3489485), (1733695919.4236398, 1733695919.423749), (1733695919.4992564, 1733695919.4993799), (1733695919.575072, 1733695919.5751927), (1733695919.6502707, 1733695919.6503808), (1733695919.7251394, 1733695919.7252603), (1733695919.7984207, 1733695919.798529), (1733695919.8732812, 1733695919.8733993), (1733695919.9493017, 1733695919.9494185), (1733695920.0237155, 1733695920.0238488), (1733695920.0997193, 1733695920.0998454), (1733695920.1759138, 1733695920.1760428), (1733695920.2504706, 1733695920.25058), (1733695920.324398, 1733695920.3245056), (1733695920.399357, 1733695920.3994806), (1733695920.4745464, 1733695920.4746644), (1733695920.549506, 1733695920.549612), (1733695920.6250482, 1733695920.6251721), (1733695920.6993372, 1733695920.6994443), (1733695920.77422, 1733695920.774334), (1733695920.8493729, 1733695920.8494797), (1733695920.9241815, 1733695920.9242935), (1733695920.9983962, 1733695920.9985104), (1733695921.0741088, 1733695921.0742323), (1733695921.1493518, 1733695921.1494682), (1733695921.2238288, 1733695921.223947), (1733695921.2983634, 1733695921.2984707), (1733695921.3735044, 1733695921.3736358), (1733695921.4483316, 1733695921.4484425), (1733695921.5240264, 1733695921.5241406), (1733695921.5988297, 1733695921.5989468), (1733695921.6740432, 1733695921.6741643), (1733695921.750435, 1733695921.750566), (1733695921.8250954, 1733695921.8252125), (1733695921.8997133, 1733695921.8998291), (1733695921.975325, 1733695921.9754424), (1733695922.0510392, 1733695922.0511475), (1733695922.1258204, 1733695922.1259356), (1733695922.1997104, 1733695922.1998193), (1733695922.2735538, 1733695922.273675), (1733695922.3481832, 1733695922.348304), (1733695922.4237268, 1733695922.4238446), (1733695922.4996428, 1733695922.4997635), (1733695922.5746565, 1733695922.5747685), (1733695922.6540172, 1733695922.6541693), (1733695922.7320018, 1733695922.7321544), (1733695922.8093066, 1733695922.8094525), (1733695922.8877916, 1733695922.8879423), (1733695922.9663973, 1733695922.9665475), (1733695923.0458844, 1733695923.0460372), (1733695923.1240706, 1733695923.124215), (1733695923.2026856, 1733695923.2028413), (1733695923.282007, 1733695923.2821581), (1733695923.3566377, 1733695923.3567634), (1733695923.4305005, 1733695923.4306283), (1733695923.5052652, 1733695923.5053916), (1733695923.5807445, 1733695923.5808692), (1733695923.6557755, 1733695923.6558945), (1733695923.7302897, 1733695923.730416), (1733695923.806157, 1733695923.8062894), (1733695923.8819306, 1733695923.8820443), (1733695923.9568114, 1733695923.9569376), (1733695924.0315642, 1733695924.03169), (1733695924.1058145, 1733695924.105942), (1733695924.181411, 1733695924.181527), (1733695924.2552547, 1733695924.2555115), (1733695924.3302832, 1733695924.330396), (1733695924.405732, 1733695924.4058595), (1733695924.4814177, 1733695924.481545), (1733695924.5555124, 1733695924.5556314), (1733695924.629175, 1733695924.6293018), (1733695924.704152, 1733695924.7042718), (1733695924.777767, 1733695924.7778952), (1733695924.8516746, 1733695924.851793), (1733695924.9256144, 1733695924.9257343), (1733695925.0006642, 1733695925.0007813), (1733695925.076928, 1733695925.077063), (1733695925.1519566, 1733695925.1520789), (1733695925.2253325, 1733695925.225452), (1733695925.299077, 1733695925.2992046), (1733695925.373739, 1733695925.3738532), (1733695925.4481826, 1733695925.448309), (1733695925.5219612, 1733695925.5220795), (1733695925.5974495, 1733695925.597564), (1733695925.6721663, 1733695925.6722836), (1733695925.7464747, 1733695925.7466002), (1733695925.8210862, 1733695925.8212197), (1733695925.8958335, 1733695925.8959565), (1733695925.9699388, 1733695925.970065), (1733695926.0450647, 1733695926.045225), (1733695926.1202512, 1733695926.120378), (1733695926.1948884, 1733695926.1950052), (1733695926.2697055, 1733695926.2698274), (1733695926.3439412, 1733695926.3440666), (1733695926.4182222, 1733695926.4183486), (1733695926.4926968, 1733695926.4928129), (1733695926.5665748, 1733695926.566718), (1733695926.6413605, 1733695926.6414802), (1733695926.71561, 1733695926.715736), (1733695926.790408, 1733695926.7905314), (1733695926.8649592, 1733695926.865073), (1733695926.9403126, 1733695926.940434), (1733695927.014613, 1733695927.0147367), (1733695927.0893729, 1733695927.0895011), (1733695927.1642926, 1733695927.1644087), (1733695927.2388003, 1733695927.2389283), (1733695927.3124602, 1733695927.3125846), (1733695927.3876226, 1733695927.387757), (1733695927.4626336, 1733695927.4627612), (1733695927.5377352, 1733695927.5378606), (1733695927.6136422, 1733695927.61377), (1733695927.6935909, 1733695927.6937268), (1733695927.7683828, 1733695927.7685041), (1733695927.841957, 1733695927.8420906), (1733695927.915215, 1733695927.9153287), (1733695927.9890318, 1733695927.9891498), (1733695928.0635688, 1733695928.0636744), (1733695928.137415, 1733695928.137532), (1733695928.2123094, 1733695928.2124288), (1733695928.2694204, 1733695928.26952)]], and the total time consumed is: 0.03016495704650879.\n",
      "The total energy consumption in backward part is: 0.0, and the total time consumed is: 8.469930410385132\n",
      "The total energy consumption in optimize part is: 0.0, and the total time consumed is: 0.2881295680999756\n"
     ]
    }
   ],
   "source": [
    "# create the folder to store the data\n",
    "main_folder = DataList[0]\n",
    "print('The folder is:', main_folder)\n",
    "# find out that if the folder exists in the data path\n",
    "# 判断文件是否存在\n",
    "if main_folder.exists():\n",
    "    print(\"文件存在。\")\n",
    "else:\n",
    "    os.makedirs(main_folder)\n",
    "    print(\"文件不存在，已创建。\")\n",
    "    print(\"文件创建于：\", main_folder)\n",
    "for epoch in epochs:\n",
    "    for batch in batch_size:\n",
    "        for round in range(rounds):\n",
    "            train_model_f(main_folder, batch, epoch, round, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreenAI",
   "language": "python",
   "name": "greenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
