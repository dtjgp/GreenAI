{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to gather the information of the energy consumption of the whole training process of different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ptflops import get_model_complexity_info\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import pynvml\n",
    "import threading\n",
    "import queue\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current path is: /home/GreenAI/3080\n",
      "The data path is: /home/GreenAI/3080/ModelsData\n"
     ]
    }
   ],
   "source": [
    "'''find the Model path'''\n",
    "# find the current path\n",
    "from pathlib import Path\n",
    "\n",
    "# find the current path\n",
    "current_path = Path.cwd()\n",
    "print('The current path is:', current_path)\n",
    "\n",
    "# find the data path\n",
    "data_path = Path(current_path / 'ModelsData')\n",
    "print('The data path is:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = ['alexnet', \n",
    "               'vgg11', 'vgg13', 'vgg16', \n",
    "               'resnet18', 'resnet34', 'resnet50',\n",
    "               'googlenet_origin', 'googlenet_mod1', 'googlenet_mod2', 'googlenet_mod3',\n",
    "               'googlenet_mod4', 'googlenet_mod5', 'googlenet_mod6', 'googlenet_mod7', \n",
    "               'googlenet_mod8', 'googlenet_mod9',\n",
    "               'mobilenetv1_path', 'mobilenetv2_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/GreenAI/3080/ModelsData/alexnet'), PosixPath('/home/GreenAI/3080/ModelsData/vgg11'), PosixPath('/home/GreenAI/3080/ModelsData/vgg13'), PosixPath('/home/GreenAI/3080/ModelsData/vgg16'), PosixPath('/home/GreenAI/3080/ModelsData/resnet18'), PosixPath('/home/GreenAI/3080/ModelsData/resnet34'), PosixPath('/home/GreenAI/3080/ModelsData/resnet50'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_origin'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod1'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod2'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod3'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod4'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod5'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod6'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod7'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod8'), PosixPath('/home/GreenAI/3080/ModelsData/googlenet_mod9'), PosixPath('/home/GreenAI/3080/ModelsData/mobilenetv1_path'), PosixPath('/home/GreenAI/3080/ModelsData/mobilenetv2_path')]\n"
     ]
    }
   ],
   "source": [
    "DataList = [Path(f\"{data_path}/{i}\") for i in models_name]\n",
    "print(DataList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(img_channel, num_labels):\n",
    "    net = nn.Sequential(\n",
    "        # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "        # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "        # 另外，输出通道的数目远大于LeNet\n",
    "        nn.Conv2d(img_channel, 64, kernel_size=11, stride=4, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "        nn.Conv2d(64, 192, kernel_size=5, padding=2), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "        # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "        # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "        nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.AdaptiveAvgPool2d((6, 6)),   # 使用全局平均池化对每个通道中所有元素求平均并直接将结果传递到全连接层\n",
    "        nn.Flatten(),\n",
    "        # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "        nn.Linear(256 * 6 * 6, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "        nn.Linear(4096, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG11 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg11(input_channels, output_channels):\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg11_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG13 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg13_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg13(input_channels, output_channels):\n",
    "    # VGG-13 architecture\n",
    "    conv_arch = [(2, 64), (2, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg13_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg16(input_channels, output_channels):\n",
    "    conv_arch = [(2, 64), (2, 128), (3, 256), (3, 512), (3, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg16_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet18 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual18(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "    \n",
    "def resnet(img_channel, num_labels):\n",
    "    # blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                    first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual18(input_channels, num_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual18(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                        nn.AdaptiveAvgPool2d((1,1)),\n",
    "                        nn.Flatten(), nn.Linear(512, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet34 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual34(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "    \n",
    "def resnet(img_channel, num_labels):\n",
    "    # blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                    first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual34(input_channels, num_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual34(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 3, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 4))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 6))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 3))\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                        nn.AdaptiveAvgPool2d((1,1)),\n",
    "                        nn.Flatten(), nn.Linear(512, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual50(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels , kernel_size=1, stride=strides, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels )\n",
    "        self.conv2 = nn.Conv2d(num_channels , num_channels , kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels )\n",
    "        self.conv3 = nn.Conv2d(num_channels , num_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "        if use_1x1conv or strides != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides, bias=False),\n",
    "                nn.BatchNorm2d(num_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        if self.downsample:\n",
    "            X = self.downsample(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "def resnet(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(\n",
    "        nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    )\n",
    "\n",
    "    def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual50(input_channels, num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual50(num_channels, num_channels))\n",
    "        return blk\n",
    "\n",
    "    b2 = nn.Sequential(*resnet_block(64, 64, 3, first_block=True))\n",
    "    b3 = nn.Sequential(*resnet_block(64, 128, 4))\n",
    "    b4 = nn.Sequential(*resnet_block(128, 256, 6))\n",
    "    b5 = nn.Sequential(*resnet_block(256, 512, 3))\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        b1, b2, b3, b4, b5,\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(), nn.Linear(512, num_labels)\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogleNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet Model(orinigal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod1(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod1, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod1(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod1(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod1(256-64, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod1(480-128, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod1(512-192, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod1(512-160, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod1(512-128, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod1(528-112, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod1(832-256, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod1(832-256, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod2(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod2, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        # p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod2(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod2(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod2(256-128, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod2(480-192, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod2(512-208, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod2(512-224, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod2(512-256, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod2(528-288, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod2(832-320, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod2(832-320, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod3(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod3, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        # self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        # self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod3(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod3(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod3(256-32, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod3(480-96, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod3(512-48, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod3(512-64, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod3(512-64, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod3(528-64, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod3(832-128, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod3(832-128, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-128, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod4(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception_mod4, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3), dim=1)\n",
    "    \n",
    "def Googlenet_mod4(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod4(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception_mod4(256-32, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod4(480-64, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception_mod4(512-64, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception_mod4(512-64, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception_mod4(512-64, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception_mod4(528-64, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod4(832-128, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception_mod4(832-128, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024-128, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod5(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod5, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        # self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        # self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat([p2], dim=1)\n",
    "    \n",
    "def Googlenet_mod5(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod5(192, (96, 128)),\n",
    "                   Inception_mod5(128, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod5(192, (96, 208)),\n",
    "                   Inception_mod5(208, (112, 224)),\n",
    "                   Inception_mod5(224, (128, 256)),\n",
    "                   Inception_mod5(256, (144, 288)),\n",
    "                   Inception_mod5(288, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod5(320, (160, 320)),\n",
    "                   Inception_mod5(320, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version6 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod6(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod6, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        # self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        # self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3), dim=1)\n",
    "    \n",
    "def Googlenet_mod6(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod6(192, (96, 128)),\n",
    "                   Inception_mod6(128*2, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod6(192*2, (96, 208)),\n",
    "                   Inception_mod6(208*2, (112, 224)),\n",
    "                   Inception_mod6(224*2, (128, 256)),\n",
    "                   Inception_mod6(256*2, (144, 288)),\n",
    "                   Inception_mod6(288*2, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod6(320*2, (160, 320)),\n",
    "                   Inception_mod6(320*2, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*2, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version7 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod7(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod7, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        # self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod7(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod7(192, (96, 128)),\n",
    "                   Inception_mod7(128*3, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod7(192*3, (96, 208)),\n",
    "                   Inception_mod7(208*3, (112, 224)),\n",
    "                   Inception_mod7(224*3, (128, 256)),\n",
    "                   Inception_mod7(256*3, (144, 288)),\n",
    "                   Inception_mod7(288*3, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod7(320*3, (160, 320)),\n",
    "                   Inception_mod7(320*3, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*3, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_mod8(nn.Module):\n",
    "    # c1--c4是每条路径的输出通道数\n",
    "    def __init__(self, in_channels, c2, **kwargs):\n",
    "        super(Inception_mod8, self).__init__(**kwargs)\n",
    "        # 线路1，单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p1_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路2，1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3，1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "        self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_2(F.relu(self.p1_1(x))))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "        # 在通道维度上连结输出\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod8(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b3 = nn.Sequential(Inception_mod8(192, (96, 128)),\n",
    "                   Inception_mod8(128*4, (128, 192)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b4 = nn.Sequential(Inception_mod8(192*4, (96, 208)),\n",
    "                   Inception_mod8(208*4, (112, 224)),\n",
    "                   Inception_mod8(224*4, (128, 256)),\n",
    "                   Inception_mod8(256*4, (144, 288)),\n",
    "                   Inception_mod8(288*4, (160, 320)),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b5 = nn.Sequential(Inception_mod8(320*4, (160, 320)),\n",
    "                   Inception_mod8(320*4, (192, 384)),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(384*4, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GoogleNet modified version9 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Inception_mod8(nn.Module):\n",
    "#     # c1--c4是每条路径的输出通道数\n",
    "#     def __init__(self, in_channels, c2, **kwargs):\n",
    "#         super(Inception_mod8, self).__init__(**kwargs)\n",
    "#         # 线路1，单1x1卷积层\n",
    "#         self.p1_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p1_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路2，1x1卷积层后接3x3卷积层\n",
    "#         self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路3，1x1卷积层后接5x5卷积层\n",
    "#         self.p3_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p3_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "#         # 线路4，3x3最大汇聚层后接1x1卷积层\n",
    "#         self.p4_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "#         self.p4_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         p1 = F.relu(self.p1_2(F.relu(self.p1_1(x))))\n",
    "#         p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "#         p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "#         p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))\n",
    "#         # 在通道维度上连结输出\n",
    "#         return torch.cat((p1, p2, p3, p4), dim=1)\n",
    "    \n",
    "def Googlenet_mod9(img_channel, num_labels):\n",
    "    b1 = nn.Sequential(nn.Conv2d(img_channel, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b3 = nn.Sequential(Inception_mod8(192, (96, 128)),\n",
    "    #                Inception_mod8(128*4, (128, 192)),\n",
    "    #                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b4 = nn.Sequential(Inception_mod8(192*4, (96, 208)),\n",
    "    #                Inception_mod8(208*4, (112, 224)),\n",
    "    #                Inception_mod8(224*4, (128, 256)),\n",
    "    #                Inception_mod8(256*4, (144, 288)),\n",
    "    #                Inception_mod8(288*4, (160, 320)),\n",
    "    #                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "    # b5 = nn.Sequential(Inception_mod8(320*4, (160, 320)),\n",
    "    #                Inception_mod8(320*4, (192, 384)),\n",
    "    #                nn.AdaptiveAvgPool2d((1,1)),\n",
    "    #                nn.Flatten())\n",
    "    \n",
    "    b5 = nn.Sequential(\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "    net = nn.Sequential(b1, b2, b5, nn.Linear(192, num_labels))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MobileNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MobileNetV1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            DepthwiseSeparableConv(32, 64, 1),\n",
    "            DepthwiseSeparableConv(64, 128, 2),\n",
    "            DepthwiseSeparableConv(128, 128, 1),\n",
    "            DepthwiseSeparableConv(128, 256, 2),\n",
    "            DepthwiseSeparableConv(256, 256, 1),\n",
    "            DepthwiseSeparableConv(256, 512, 2),\n",
    "\n",
    "            # Typically, 5 Depthwise Separable Convolutions are repeated here, each with stride 1\n",
    "            *[DepthwiseSeparableConv(512, 512, 1) for _ in range(5)],\n",
    "\n",
    "            DepthwiseSeparableConv(512, 1024, 2),\n",
    "            DepthwiseSeparableConv(1024, 1024, 1),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, output_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.use_residual = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # expand\n",
    "            nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # depthwise\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            # project\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.layers(x)\n",
    "        else:\n",
    "            return self.layers(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.first_layer = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.inverted_residual_blocks = nn.Sequential(\n",
    "            InvertedResidual(32, 16, 1, 1),\n",
    "            InvertedResidual(16, 24, 2, 6),\n",
    "            InvertedResidual(24, 24, 1, 6),\n",
    "            InvertedResidual(24, 32, 2, 6),\n",
    "            InvertedResidual(32, 32, 1, 6),\n",
    "            InvertedResidual(32, 32, 1, 6),\n",
    "            InvertedResidual(32, 64, 2, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 64, 1, 6),\n",
    "            InvertedResidual(64, 96, 1, 6),\n",
    "            InvertedResidual(96, 96, 1, 6),\n",
    "            InvertedResidual(96, 96, 1, 6),\n",
    "            InvertedResidual(96, 160, 2, 6),\n",
    "            InvertedResidual(160, 160, 1, 6),\n",
    "            InvertedResidual(160, 160, 1, 6),\n",
    "            InvertedResidual(160, 320, 1, 6)\n",
    "        )\n",
    "\n",
    "        self.last_layers = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, output_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = self.inverted_residual_blocks(x)\n",
    "        x = self.last_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALEXNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer name is: ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
      "The number of layers is: 7\n",
      "The layer name after orged is: ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
      "The layer name is: ['Conv1', 'ReLU2', 'MaxP3', 'Conv4', 'ReLU5', 'MaxP6', 'Conv7', 'ReLU8', 'Conv9', 'ReLU10', 'Conv11', 'ReLU12', 'MaxP13', 'Adap14', 'Flat15', 'Line16', 'ReLU17', 'Drop18', 'Line19', 'ReLU20', 'Drop21', 'Line22']\n",
      "The number of layers is: 22\n"
     ]
    }
   ],
   "source": [
    "# print the model structure\n",
    "alexnet_f = alexnet(1, 10)    \n",
    "alexnet_c = alexnet(3, 10)\n",
    "# print each layer\n",
    "layer_name = []\n",
    "for layer in alexnet_f:\n",
    "    name = layer.__class__.__name__\n",
    "    layer_name.append(name)\n",
    "# find the unique layer name, and fix the order\n",
    "layer_name = sorted(list(set(layer_name)))\n",
    "print('The layer name is:', layer_name)\n",
    "# the number of layers, which contains ['AdaptiveAvgPool2d', 'Conv2d', 'Dropout', 'Flatten', 'Linear', 'MaxPool2d', 'ReLU']\n",
    "num_layers = len(layer_name) \n",
    "print('The number of layers is:', num_layers)\n",
    "\n",
    "alexlayer = []\n",
    "CNNlayer = []\n",
    "num = 0\n",
    "for layer in alexnet_f:\n",
    "    num += 1\n",
    "    name = layer.__class__.__name__\n",
    "    layer_name = name[:4] + str(num)\n",
    "    CNNlayer.append(name)\n",
    "    alexlayer.append(layer_name)\n",
    "# find the unique layer name, and fix the order\n",
    "CNNlayer_org = sorted(list(set(CNNlayer)))\n",
    "\n",
    "print('The layer name after orged is:', CNNlayer_org)\n",
    "print('The layer name is:', alexlayer)\n",
    "print('The number of layers is:', len(alexlayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the MACs of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Sequential(\n",
      "  57.03 M, 100.000% Params, 664.65 MMac, 99.868% MACs, \n",
      "  (0): Conv2d(7.81 k, 0.014% Params, 23.62 MMac, 3.549% MACs, 1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(0, 0.000% Params, 193.6 KMac, 0.029% MACs, )\n",
      "  (2): MaxPool2d(0, 0.000% Params, 193.6 KMac, 0.029% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(307.39 k, 0.539% Params, 224.09 MMac, 33.671% MACs, 64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(0, 0.000% Params, 139.97 KMac, 0.021% MACs, )\n",
      "  (5): MaxPool2d(0, 0.000% Params, 139.97 KMac, 0.021% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(663.94 k, 1.164% Params, 112.21 MMac, 16.859% MACs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(0, 0.000% Params, 64.9 KMac, 0.010% MACs, )\n",
      "  (8): Conv2d(884.99 k, 1.552% Params, 149.56 MMac, 22.473% MACs, 384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(0, 0.000% Params, 43.26 KMac, 0.007% MACs, )\n",
      "  (10): Conv2d(590.08 k, 1.035% Params, 99.72 MMac, 14.984% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(0, 0.000% Params, 43.26 KMac, 0.007% MACs, )\n",
      "  (12): MaxPool2d(0, 0.000% Params, 43.26 KMac, 0.007% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): AdaptiveAvgPool2d(0, 0.000% Params, 9.22 KMac, 0.001% MACs, output_size=(6, 6))\n",
      "  (14): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (15): Linear(37.75 M, 66.199% Params, 37.75 MMac, 5.673% MACs, in_features=9216, out_features=4096, bias=True)\n",
      "  (16): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (17): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (18): Linear(16.78 M, 29.426% Params, 16.78 MMac, 2.521% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "  (19): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (20): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (21): Linear(40.97 k, 0.072% Params, 40.97 KMac, 0.006% MACs, in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       665.53 MMac\n",
      "Number of parameters:           57.03 M \n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Sequential(\n",
      "  57.04 M, 100.000% Params, 711.51 MMac, 99.877% MACs, \n",
      "  (0): Conv2d(23.3 k, 0.041% Params, 70.47 MMac, 9.892% MACs, 3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(0, 0.000% Params, 193.6 KMac, 0.027% MACs, )\n",
      "  (2): MaxPool2d(0, 0.000% Params, 193.6 KMac, 0.027% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(307.39 k, 0.539% Params, 224.09 MMac, 31.456% MACs, 64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(0, 0.000% Params, 139.97 KMac, 0.020% MACs, )\n",
      "  (5): MaxPool2d(0, 0.000% Params, 139.97 KMac, 0.020% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(663.94 k, 1.164% Params, 112.21 MMac, 15.751% MACs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(0, 0.000% Params, 64.9 KMac, 0.009% MACs, )\n",
      "  (8): Conv2d(884.99 k, 1.551% Params, 149.56 MMac, 20.995% MACs, 384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(0, 0.000% Params, 43.26 KMac, 0.006% MACs, )\n",
      "  (10): Conv2d(590.08 k, 1.034% Params, 99.72 MMac, 13.999% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(0, 0.000% Params, 43.26 KMac, 0.006% MACs, )\n",
      "  (12): MaxPool2d(0, 0.000% Params, 43.26 KMac, 0.006% MACs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (13): AdaptiveAvgPool2d(0, 0.000% Params, 9.22 KMac, 0.001% MACs, output_size=(6, 6))\n",
      "  (14): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (15): Linear(37.75 M, 66.181% Params, 37.75 MMac, 5.299% MACs, in_features=9216, out_features=4096, bias=True)\n",
      "  (16): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (17): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (18): Linear(16.78 M, 29.418% Params, 16.78 MMac, 2.356% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "  (19): ReLU(0, 0.000% Params, 4.1 KMac, 0.001% MACs, )\n",
      "  (20): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (21): Linear(40.97 k, 0.072% Params, 40.97 KMac, 0.006% MACs, in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       712.39 MMac\n",
      "Number of parameters:           57.04 M \n"
     ]
    }
   ],
   "source": [
    "input_channel_f = 1\n",
    "input_channel_c = 3\n",
    "with torch.cuda.device(0):\n",
    "    macs_f, params_f = get_model_complexity_info(alexnet_f, (input_channel_f, 224, 224), as_strings=True,\n",
    "                                            print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_f))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params_f))\n",
    "\n",
    "# cifar100\n",
    "with torch.cuda.device(0):\n",
    "    macs_c, params_c = get_model_complexity_info(alexnet_c, (input_channel_c, 224, 224), as_strings=True,\n",
    "                                            print_per_layer_stat=True, verbose=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs_c))\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the output size of each layers after the picture is passed through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 64, 55, 55])\n",
      "ReLU output shape:\t torch.Size([1, 64, 55, 55])\n",
      "MaxPool2d output shape:\t torch.Size([1, 64, 27, 27])\n",
      "Conv2d output shape:\t torch.Size([1, 192, 27, 27])\n",
      "ReLU output shape:\t torch.Size([1, 192, 27, 27])\n",
      "MaxPool2d output shape:\t torch.Size([1, 192, 13, 13])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 13, 13])\n",
      "ReLU output shape:\t torch.Size([1, 384, 13, 13])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 13, 13])\n",
      "ReLU output shape:\t torch.Size([1, 256, 13, 13])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 13, 13])\n",
      "ReLU output shape:\t torch.Size([1, 256, 13, 13])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 6, 6])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 256, 6, 6])\n",
      "Flatten output shape:\t torch.Size([1, 9216])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X_f = torch.randn(size=(1, 1, 224, 224), dtype=torch.float32) # fashion mnist\n",
    "\n",
    "for layer in alexnet_f:\n",
    "    X_f=layer(X_f)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the datas:  \n",
    "    1. FashionMNIST\n",
    "    2. CIFAR100\n",
    "    3. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# fashion mnist\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集, 然后将其加载到内存中\n",
    "\n",
    "    Defined in :numref:`sec_fashion_mnist`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "def load_data_cifar100(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    \n",
    "def load_data_cifar10(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [256]\n",
    "epochs = [5]\n",
    "rounds = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cuda\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('The device is:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using pynvml to get the GPU power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvml_sampling_thread(handle, filename, stop_event, sampling_interval):\n",
    "    \"\"\"\n",
    "    在单独的线程中定期调用 NVML, 获取功耗数据并存储到 data_queue 中。\n",
    "    参数：\n",
    "    - handle: nvmlDeviceGetHandleByIndex(0) 得到的 GPU 句柄\n",
    "    - data_queue: 用于存放 (timestamp, power_in_watts) 数据的队列\n",
    "    - stop_event: 当此事件被设置时，线程应结束循环\n",
    "    - sampling_interval: 采样间隔（秒）\n",
    "    \"\"\"\n",
    "    with open(filename/'energy_consumption_file.csv', 'a') as f:  # 追加模式\n",
    "        # 写入列名\n",
    "        f.write(\"timestamp,power_in_watts\\n\")\n",
    "        while not stop_event.is_set():\n",
    "            try:\n",
    "                # 采集功率和时间戳\n",
    "                current_time = time.time()\n",
    "                current_power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # 转换 mW -> W\n",
    "                # 写入文件\n",
    "                f.write(f\"{current_time},{current_power}\\n\")\n",
    "                # 等待下一次采样\n",
    "                time.sleep(sampling_interval)\n",
    "            except pynvml.NVMLError as e:\n",
    "                print(f\"NVML Error: {e}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the interval of the power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_power_over_interval(samples, start_time, end_time):\n",
    "    # 假定 samples是按时间升序排序的 (t, p)\n",
    "    # 若未排序，请先排序:\n",
    "    # samples = sorted(samples, key=lambda x: x[0])\n",
    "    \n",
    "    def interpolate(samples, target_time):\n",
    "        # 在 samples 中找到 target_time 左右最近的两个点，并进行线性插值\n",
    "        # 若 target_time 恰好等于某个样本点时间，直接返回该点功率\n",
    "        # 若无法找到两侧点（如 target_time在样本时间轴外），根据情况返回None或边界点\n",
    "        n = len(samples)\n",
    "        if n == 0:\n",
    "            return None\n",
    "        # 若 target_time 小于第一个样本点时间，无法向左插值，这里直接返回第一个点的功率值(或None)\n",
    "        if target_time <= samples[0][0]:\n",
    "            # 简化处理：返回最早样本点的功率（或None）\n",
    "            return samples[0][1]\n",
    "        # 若 target_time 大于最后一个样本点时间，无法向右插值，返回最后一个点的功率（或None）\n",
    "        if target_time >= samples[-1][0]:\n",
    "            return samples[-1][1]\n",
    "\n",
    "        # 否则，在中间插值\n",
    "        # 使用二分查找快速定位\n",
    "        import bisect\n",
    "        times = [t for t, _ in samples]\n",
    "        pos = bisect.bisect_left(times, target_time)\n",
    "        # pos是使times保持有序插入target_time的位置\n",
    "        # 因为target_time不在已有样本点中，pos不会越界且pos>0且pos<n\n",
    "        t1, p1 = samples[pos-1]\n",
    "        t2, p2 = samples[pos]\n",
    "        # 线性插值： p = p1 + (p2 - p1)*((target_time - t1)/(t2 - t1))\n",
    "        ratio = (target_time - t1) / (t2 - t1)\n",
    "        p = p1 + (p2 - p1)*ratio\n",
    "        return p\n",
    "\n",
    "    # 从原始 samples 中筛选出位于[start_time, end_time]内的点\n",
    "    filtered = [(t, p) for t, p in samples if start_time <= t <= end_time]\n",
    "\n",
    "    # 如果不足2个点，则尝试使用插值\n",
    "    if len(filtered) < 2:\n",
    "        # 无论如何都需要在边界处插值出两个点(起码start和end)\n",
    "        start_power = interpolate(samples, start_time)\n",
    "        end_power = interpolate(samples, end_time)\n",
    "\n",
    "        # 如果从样本中无法插值出任何有意义的点（比如samples为空或无法插值），返回0.0\n",
    "        if start_power is None or end_power is None:\n",
    "            return 0.0\n",
    "\n",
    "        # 将插值的边界点加入到 filtered\n",
    "        # 注意：如果filtered中有一个点在区间内，我们也需要确保边界有两点以上\n",
    "        # 例如filtered只有一个点在中间，则需要在start和end插值点全部加入。\n",
    "        # 若filtered为空，则只用start/end两点插值点求积分\n",
    "        new_filtered = [(start_time, start_power)] + filtered + [(end_time, end_power)]\n",
    "        # 确保按时间排序\n",
    "        new_filtered.sort(key=lambda x: x[0])\n",
    "        filtered = new_filtered\n",
    "\n",
    "    # 正常积分计算\n",
    "    if len(filtered) < 2:\n",
    "        # 经过插值仍不够，返回0\n",
    "        return 0.0\n",
    "\n",
    "    total_energy = 0.0\n",
    "    for i in range(len(filtered)-1):\n",
    "        t1, p1 = filtered[i]\n",
    "        t2, p2 = filtered[i+1]\n",
    "        dt = t2 - t1\n",
    "        avg_p = (p1 + p2)/2.0\n",
    "        total_energy += avg_p * dt\n",
    "\n",
    "    return total_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(net, train_iter, test_iter, num_epochs, lr, device, filename):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    # print(f'The name of the layers are: {alexlayer}')\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # save all epochs time data using list\n",
    "    to_device_intervals_total = []\n",
    "    forward_intervals_total = []\n",
    "    loss_intervals_total = []\n",
    "    backward_intervals_total = []\n",
    "    optimize_intervals_total = []\n",
    "    \n",
    "    # 初始化NVML和采样线程\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    power_data_queue = queue.Queue()\n",
    "    stop_event = threading.Event()\n",
    "    sampling_interval = 0.002 # 2ms\n",
    "    sampler_thread = threading.Thread(target=nvml_sampling_thread, args=(handle, filename, stop_event, sampling_interval))\n",
    "    sampler_thread.start()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('The epoch is:', epoch+1)\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "        to_device_intervals_epoch = []  # 用来记录本epoch每个batch的to_device时间段\n",
    "        forward_intervals_epoch = []  # 用来记录本epoch每个batch的forward时间段\n",
    "        loss_intervals_epoch = []  # 用来记录本epoch每个batch的loss时间段\n",
    "        backward_intervals_epoch = [] \n",
    "        optimize_intervals_epoch = []\n",
    "\n",
    "        to_device_time_consump_total = 0\n",
    "        forward_time_consump_total = 0\n",
    "        loss_time_consump_total = 0\n",
    "        backward_time_consump_total = 0\n",
    "        optimize_time_consump_total = 0\n",
    "\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            print('The batch is:', i+1)\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # 记录to_device前后的时间戳\n",
    "            start_ttd_time = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            end_ttd_time = time.time()\n",
    "            to_device_intervals_epoch.append((start_ttd_time, end_ttd_time))\n",
    "            ttd_time_consump = end_ttd_time - start_ttd_time\n",
    "            to_device_time_consump_total += ttd_time_consump\n",
    "\n",
    "            # forward\n",
    "            start_forward_time = time.time()\n",
    "            y_hat = net(X)\n",
    "            torch.cuda.synchronize()\n",
    "            end_forward_time = time.time()\n",
    "            forward_intervals_epoch.append((start_forward_time, end_forward_time))\n",
    "            forward_time_consump = end_forward_time - start_forward_time\n",
    "            forward_time_consump_total += forward_time_consump\n",
    "\n",
    "            # loss\n",
    "            start_loss_time = time.time()\n",
    "            l = loss_fn(y_hat, y)\n",
    "            torch.cuda.synchronize()\n",
    "            end_loss_time = time.time()\n",
    "            loss_intervals_epoch.append((start_loss_time, end_loss_time))\n",
    "            loss_time_consump = end_loss_time - start_loss_time\n",
    "            loss_time_consump_total += loss_time_consump\n",
    "\n",
    "            # backward\n",
    "            start_backward_time = time.time()\n",
    "            l.backward()\n",
    "            torch.cuda.synchronize()\n",
    "            end_backward_time = time.time()\n",
    "            backward_intervals_epoch.append((start_backward_time, end_backward_time))\n",
    "            backward_time_consump = end_backward_time - start_backward_time\n",
    "            backward_time_consump_total += backward_time_consump\n",
    "\n",
    "            # optimize\n",
    "            start_optimize_time = time.time()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            end_optimize_time = time.time()\n",
    "            optimize_intervals_epoch.append((start_optimize_time, end_optimize_time))\n",
    "            optimize_time_consump = end_optimize_time - start_optimize_time\n",
    "            optimize_time_consump_total += optimize_time_consump\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "\n",
    "        # data need to be saved\n",
    "        # add the intervals_epoch to intervals_total\n",
    "        to_device_intervals_total.append(to_device_intervals_epoch)\n",
    "        forward_intervals_total.append(forward_intervals_epoch)\n",
    "        loss_intervals_total.append(loss_intervals_epoch)\n",
    "        backward_intervals_total.append(backward_intervals_epoch)\n",
    "        optimize_intervals_total.append(optimize_intervals_epoch)\n",
    "\n",
    "\n",
    "    # 训练结束后关闭线程\n",
    "    stop_event.set()\n",
    "    sampler_thread.join()\n",
    "\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    return to_device_intervals_total, forward_intervals_total, loss_intervals_total, backward_intervals_total, optimize_intervals_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set a function to train the model with FashionMNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_f(main_folder, batch_size, num_epochs, round, lr, device):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    # epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    epoch_batch_folder = main_folder\n",
    "    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "    # show the shape of the data\n",
    "    list_of_i = []\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        if i < 3:\n",
    "            print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "        else:\n",
    "            pass\n",
    "        list_of_i.append(i)\n",
    "    print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "    to_device_intervals_total, forward_intervals_total, loss_intervals_total,\\\n",
    "          backward_intervals_total, optimize_intervals_total = train_func(alexnet_f, train_iter, test_iter, num_epochs, lr, device, epoch_batch_folder)\n",
    "\n",
    "    # transfer the data to the numpy array\n",
    "    to_device_data = np.array(to_device_intervals_total)\n",
    "    forward_time = np.array(forward_intervals_total)\n",
    "    loss_time = np.array(loss_intervals_total)\n",
    "    backward_time = np.array(backward_intervals_total)\n",
    "    optimize_time = np.array(optimize_intervals_total)\n",
    "\n",
    "    # save the data\n",
    "    np.save(epoch_batch_folder/'to_device.npy', to_device_data, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'forward.npy', forward_time, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'loss.npy', loss_time, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'backward.npy', backward_time, allow_pickle=True)\n",
    "    np.save(epoch_batch_folder/'optimize.npy', optimize_time, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is: /home/GreenAI/3080/ModelsData/alexnet\n",
      "文件不存在，已创建。\n",
      "文件创建于： /home/GreenAI/3080/ModelsData/alexnet\n",
      "The epoch is set: 5, batch is set: 256, is in 1th running\n",
      "the shape of the 0 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 1 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 2 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches is: (235,)\n",
      "training on cuda\n",
      "The epoch is: 1\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.428, test acc 0.679\n",
      "The epoch is: 2\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.709, test acc 0.763\n",
      "The epoch is: 3\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.769, test acc 0.793\n",
      "The epoch is: 4\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.797, test acc 0.806\n",
      "The epoch is: 5\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.818, test acc 0.830\n"
     ]
    }
   ],
   "source": [
    "# create the folder to store the data\n",
    "main_folder = DataList[0]\n",
    "print('The folder is:', main_folder)\n",
    "# find out that if the folder exists in the data path\n",
    "# 判断文件是否存在\n",
    "if main_folder.exists():\n",
    "    print(\"文件存在。\")\n",
    "else:\n",
    "    os.makedirs(main_folder)\n",
    "    print(\"文件不存在，已创建。\")\n",
    "    print(\"文件创建于：\", main_folder)\n",
    "for epoch in epochs:\n",
    "    for batch in batch_size:\n",
    "        for round in range(rounds):\n",
    "            train_model_f(main_folder, batch, epoch, round, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n        to_device_energy_consump_total = 0\\n        forward_energy_consump_total = 0\\n        loss_energy_intervals_total = 0\\n        backward_energy_consump_total = 0\\n        optimize_energy_consump_total = 0\\n        \\n\\n        # 记录epoch开始时队列中已有的数据条数，用于后面区分本epoch的数据\\n        initial_queue_size = power_data_queue.qsize()\\n\\n        # data display\\n        # 从队列中取出本epoch采样数据\\n        epoch_samples = []\\n        # 这里取出initial_queue_size之后加入的所有数据\\n        total_samples_in_epoch = power_data_queue.qsize() - initial_queue_size\\n        for _ in range(total_samples_in_epoch):\\n            epoch_samples.append(power_data_queue.get())\\n\\n        # 对epoch中每个batch的to_device间隔计算能耗\\n        for idx, (s_time, e_time) in enumerate(to_device_intervals_epoch):\\n            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\\n            to_device_energy_consump_total += batch_energy\\n\\n            print(f\"Epoch {epoch+1}, Batch {idx+1}, to_device Energy: {batch_energy} J\")\\n        \\n        # 对epoch中每个batch的forward间隔计算能耗\\n        for idx, (s_time, e_time) in enumerate(forward_intervals_epoch):\\n            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\\n            forward_energy_consump_total += batch_energy\\n            print(f\"Epoch {epoch+1}, Batch {idx+1}, forward Energy: {batch_energy} J\")\\n\\n        # 对epoch中每个batch的loss间隔计算能耗\\n        for idx, (s_time, e_time) in enumerate(loss_intervals_epoch):\\n            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\\n            loss_energy_intervals_total += batch_energy\\n            print(f\"Epoch {epoch+1}, Batch {idx+1}, loss Energy: {batch_energy} J\")\\n\\n        # 对epoch中每个batch的backward间隔计算能耗\\n        for idx, (s_time, e_time) in enumerate(backward_intervals_epoch):\\n            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\\n            backward_energy_consump_total += batch_energy\\n            print(f\"Epoch {epoch+1}, Batch {idx+1}, backward Energy: {batch_energy} J\")\\n\\n        # 对epoch中每个batch的optimize间隔计算能耗\\n        for idx, (s_time, e_time) in enumerate(optimize_intervals_epoch):\\n            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\\n            optimize_energy_consump_total += batch_energy\\n            print(f\"Epoch {epoch+1}, Batch {idx+1}, optimize Energy: {batch_energy} J\")\\n        \\n\\n        print(f\\'The total energy consumption in to_device part is: {to_device_energy_consump_total}, and the total time consumed is: {to_device_time_consump_total}.\\')\\n        print(f\\'The total energy consumption in forward part is: {forward_energy_consump_total}, and the total time consumed is: {forward_time_consump_total}.\\')\\n        print(f\\'The total energy consumption in loss part is: {loss_energy_intervals_total}, and the total time consumed is: {loss_time_consump_total}.\\')\\n        print(f\\'The total energy consumption in backward part is: {backward_energy_consump_total}, and the total time consumed is: {backward_time_consump_total}\\')\\n        print(f\\'The total energy consumption in optimize part is: {optimize_energy_consump_total}, and the total time consumed is: {optimize_time_consump_total}\\')\\n        '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "        to_device_energy_consump_total = 0\n",
    "        forward_energy_consump_total = 0\n",
    "        loss_energy_intervals_total = 0\n",
    "        backward_energy_consump_total = 0\n",
    "        optimize_energy_consump_total = 0\n",
    "        \n",
    "\n",
    "        # 记录epoch开始时队列中已有的数据条数，用于后面区分本epoch的数据\n",
    "        initial_queue_size = power_data_queue.qsize()\n",
    "\n",
    "        # data display\n",
    "        # 从队列中取出本epoch采样数据\n",
    "        epoch_samples = []\n",
    "        # 这里取出initial_queue_size之后加入的所有数据\n",
    "        total_samples_in_epoch = power_data_queue.qsize() - initial_queue_size\n",
    "        for _ in range(total_samples_in_epoch):\n",
    "            epoch_samples.append(power_data_queue.get())\n",
    "\n",
    "        # 对epoch中每个batch的to_device间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(to_device_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            to_device_energy_consump_total += batch_energy\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, to_device Energy: {batch_energy} J\")\n",
    "        \n",
    "        # 对epoch中每个batch的forward间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(forward_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            forward_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, forward Energy: {batch_energy} J\")\n",
    "\n",
    "        # 对epoch中每个batch的loss间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(loss_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            loss_energy_intervals_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, loss Energy: {batch_energy} J\")\n",
    "\n",
    "        # 对epoch中每个batch的backward间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(backward_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            backward_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, backward Energy: {batch_energy} J\")\n",
    "\n",
    "        # 对epoch中每个batch的optimize间隔计算能耗\n",
    "        for idx, (s_time, e_time) in enumerate(optimize_intervals_epoch):\n",
    "            batch_energy = integrate_power_over_interval(epoch_samples, s_time, e_time)\n",
    "            optimize_energy_consump_total += batch_energy\n",
    "            print(f\"Epoch {epoch+1}, Batch {idx+1}, optimize Energy: {batch_energy} J\")\n",
    "        \n",
    "\n",
    "        print(f'The total energy consumption in to_device part is: {to_device_energy_consump_total}, and the total time consumed is: {to_device_time_consump_total}.')\n",
    "        print(f'The total energy consumption in forward part is: {forward_energy_consump_total}, and the total time consumed is: {forward_time_consump_total}.')\n",
    "        print(f'The total energy consumption in loss part is: {loss_energy_intervals_total}, and the total time consumed is: {loss_time_consump_total}.')\n",
    "        print(f'The total energy consumption in backward part is: {backward_energy_consump_total}, and the total time consumed is: {backward_time_consump_total}')\n",
    "        print(f'The total energy consumption in optimize part is: {optimize_energy_consump_total}, and the total time consumed is: {optimize_time_consump_total}')\n",
    "        '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreenAI",
   "language": "python",
   "name": "greenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
