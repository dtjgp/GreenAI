{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to gather the information of the energy consumption of the whole training process of different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ptflops import get_model_complexity_info\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import pynvml\n",
    "import threading\n",
    "import queue# type: ignore\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current path is: /root/autodl-tmp/GreenAI/3080\n",
      "The data path is: /root/autodl-tmp/GreenAI/3080/ModelsData\n"
     ]
    }
   ],
   "source": [
    "'''find the Model path'''\n",
    "# find the current path\n",
    "from pathlib import Path\n",
    "\n",
    "# find the current path\n",
    "current_path = Path.cwd()\n",
    "print('The current path is:', current_path)\n",
    "\n",
    "# find the data path\n",
    "data_path = Path(current_path / 'ModelsData')\n",
    "print('The data path is:', data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = ['vgg11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/alexnet'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/vgg11'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/vgg13'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/vgg16'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/resnet18'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/resnet34'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/resnet50'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_origin'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod1'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod2'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod3'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod4'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod5'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod6'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod7'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod8'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/googlenet_mod9'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/mobilenetv1_path'), PosixPath('/root/autodl-tmp/GreenAI/3080/ModelsData/mobilenetv2_path')]\n"
     ]
    }
   ],
   "source": [
    "DataList = [Path(f\"{data_path}/{i}\") for i in models_name]\n",
    "print(DataList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG11 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def vgg11(input_channels, output_channels):\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    conv_arch = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    in_channels = input_channels  # For RGB images\n",
    "    # Create convolutional layers\n",
    "    conv_layers = []\n",
    "    for num_convs, out_channels in conv_arch:\n",
    "        conv_layers.append(vgg11_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_layers, nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, output_channels)  # Output layer for 1000 classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usea function to call the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for all the models to run\n",
    "# image channel for fashion mnist \n",
    "channel_f = 1\n",
    "# image channel for cifar100 and cifar10\n",
    "channel_c = 3\n",
    "\n",
    "# number of labels for fashion mnist\n",
    "num_labels_f = 10\n",
    "# number of labels for cifar100 \n",
    "num_labels_c100 = 100\n",
    "# number of labels for cifar10\n",
    "num_labels_c10 = 10\n",
    "\n",
    "def get_model_info(model, img_channel, num_labels):\n",
    "    net = model(img_channel, num_labels)\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(net, (img_channel, 224, 224), as_strings=True,\n",
    "                                                print_per_layer_stat=True, verbose=True)\n",
    "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    return net, macs, params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Sequential(\n",
      "  128.81 M, 100.000% Params, 7.57 GMac, 99.821% MACs, \n",
      "  (0): Sequential(\n",
      "    640, 0.000% Params, 38.54 MMac, 0.508% MACs, \n",
      "    (0): Conv2d(640, 0.000% Params, 32.11 MMac, 0.424% MACs, 1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 3.21 MMac, 0.042% MACs, )\n",
      "    (2): MaxPool2d(0, 0.000% Params, 3.21 MMac, 0.042% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    73.86 k, 0.057% Params, 929.66 MMac, 12.262% MACs, \n",
      "    (0): Conv2d(73.86 k, 0.057% Params, 926.45 MMac, 12.219% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 1.61 MMac, 0.021% MACs, )\n",
      "    (2): MaxPool2d(0, 0.000% Params, 1.61 MMac, 0.021% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    885.25 k, 0.687% Params, 2.78 GMac, 36.648% MACs, \n",
      "    (0): Conv2d(295.17 k, 0.229% Params, 925.65 MMac, 12.209% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 802.82 KMac, 0.011% MACs, )\n",
      "    (2): Conv2d(590.08 k, 0.458% Params, 1.85 GMac, 24.407% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 802.82 KMac, 0.011% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.011% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    3.54 M, 2.748% Params, 2.78 GMac, 36.621% MACs, \n",
      "    (0): Conv2d(1.18 M, 0.916% Params, 925.25 MMac, 12.204% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 401.41 KMac, 0.005% MACs, )\n",
      "    (2): Conv2d(2.36 M, 1.832% Params, 1.85 GMac, 24.402% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 401.41 KMac, 0.005% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 401.41 KMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    4.72 M, 3.664% Params, 925.35 MMac, 12.205% MACs, \n",
      "    (0): Conv2d(2.36 M, 1.832% Params, 462.52 MMac, 6.100% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 100.35 KMac, 0.001% MACs, )\n",
      "    (2): Conv2d(2.36 M, 1.832% Params, 462.52 MMac, 6.100% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 100.35 KMac, 0.001% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (5): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (6): Linear(102.76 M, 79.782% Params, 102.76 MMac, 1.355% MACs, in_features=25088, out_features=4096, bias=True)\n",
      "  (7): ReLU(0, 0.000% Params, 4.1 KMac, 0.000% MACs, )\n",
      "  (8): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (9): Linear(16.78 M, 13.028% Params, 16.78 MMac, 0.221% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "  (10): ReLU(0, 0.000% Params, 4.1 KMac, 0.000% MACs, )\n",
      "  (11): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (12): Linear(40.97 k, 0.032% Params, 40.97 KMac, 0.001% MACs, in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       7.58 GMac\n",
      "Number of parameters:           128.81 M\n",
      "--------------------------------------------------\n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Sequential(\n",
      "  129.18 M, 100.000% Params, 7.63 GMac, 99.823% MACs, \n",
      "  (0): Sequential(\n",
      "    1.79 k, 0.001% Params, 96.34 MMac, 1.261% MACs, \n",
      "    (0): Conv2d(1.79 k, 0.001% Params, 89.92 MMac, 1.177% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 3.21 MMac, 0.042% MACs, )\n",
      "    (2): MaxPool2d(0, 0.000% Params, 3.21 MMac, 0.042% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    73.86 k, 0.057% Params, 929.66 MMac, 12.168% MACs, \n",
      "    (0): Conv2d(73.86 k, 0.057% Params, 926.45 MMac, 12.126% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 1.61 MMac, 0.021% MACs, )\n",
      "    (2): MaxPool2d(0, 0.000% Params, 1.61 MMac, 0.021% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    885.25 k, 0.685% Params, 2.78 GMac, 36.369% MACs, \n",
      "    (0): Conv2d(295.17 k, 0.229% Params, 925.65 MMac, 12.116% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 802.82 KMac, 0.011% MACs, )\n",
      "    (2): Conv2d(590.08 k, 0.457% Params, 1.85 GMac, 24.221% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 802.82 KMac, 0.011% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.011% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    3.54 M, 2.740% Params, 2.78 GMac, 36.342% MACs, \n",
      "    (0): Conv2d(1.18 M, 0.914% Params, 925.25 MMac, 12.111% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 401.41 KMac, 0.005% MACs, )\n",
      "    (2): Conv2d(2.36 M, 1.827% Params, 1.85 GMac, 24.216% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 401.41 KMac, 0.005% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 401.41 KMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    4.72 M, 3.654% Params, 925.35 MMac, 12.112% MACs, \n",
      "    (0): Conv2d(2.36 M, 1.827% Params, 462.52 MMac, 6.054% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 100.35 KMac, 0.001% MACs, )\n",
      "    (2): Conv2d(2.36 M, 1.827% Params, 462.52 MMac, 6.054% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 100.35 KMac, 0.001% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (5): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (6): Linear(102.76 M, 79.554% Params, 102.76 MMac, 1.345% MACs, in_features=25088, out_features=4096, bias=True)\n",
      "  (7): ReLU(0, 0.000% Params, 4.1 KMac, 0.000% MACs, )\n",
      "  (8): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (9): Linear(16.78 M, 12.991% Params, 16.78 MMac, 0.220% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "  (10): ReLU(0, 0.000% Params, 4.1 KMac, 0.000% MACs, )\n",
      "  (11): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (12): Linear(409.7 k, 0.317% Params, 409.7 KMac, 0.005% MACs, in_features=4096, out_features=100, bias=True)\n",
      ")\n",
      "Computational complexity:       7.64 GMac\n",
      "Number of parameters:           129.18 M\n",
      "--------------------------------------------------\n",
      "Warning: module Flatten is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Sequential(\n",
      "  128.81 M, 100.000% Params, 7.63 GMac, 99.823% MACs, \n",
      "  (0): Sequential(\n",
      "    1.79 k, 0.001% Params, 96.34 MMac, 1.261% MACs, \n",
      "    (0): Conv2d(1.79 k, 0.001% Params, 89.92 MMac, 1.177% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 3.21 MMac, 0.042% MACs, )\n",
      "    (2): MaxPool2d(0, 0.000% Params, 3.21 MMac, 0.042% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    73.86 k, 0.057% Params, 929.66 MMac, 12.169% MACs, \n",
      "    (0): Conv2d(73.86 k, 0.057% Params, 926.45 MMac, 12.127% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 1.61 MMac, 0.021% MACs, )\n",
      "    (2): MaxPool2d(0, 0.000% Params, 1.61 MMac, 0.021% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    885.25 k, 0.687% Params, 2.78 GMac, 36.370% MACs, \n",
      "    (0): Conv2d(295.17 k, 0.229% Params, 925.65 MMac, 12.116% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 802.82 KMac, 0.011% MACs, )\n",
      "    (2): Conv2d(590.08 k, 0.458% Params, 1.85 GMac, 24.222% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 802.82 KMac, 0.011% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 802.82 KMac, 0.011% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    3.54 M, 2.748% Params, 2.78 GMac, 36.344% MACs, \n",
      "    (0): Conv2d(1.18 M, 0.916% Params, 925.25 MMac, 12.111% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 401.41 KMac, 0.005% MACs, )\n",
      "    (2): Conv2d(2.36 M, 1.832% Params, 1.85 GMac, 24.217% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 401.41 KMac, 0.005% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 401.41 KMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    4.72 M, 3.664% Params, 925.35 MMac, 12.113% MACs, \n",
      "    (0): Conv2d(2.36 M, 1.832% Params, 462.52 MMac, 6.054% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0, 0.000% Params, 100.35 KMac, 0.001% MACs, )\n",
      "    (2): Conv2d(2.36 M, 1.832% Params, 462.52 MMac, 6.054% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0, 0.000% Params, 100.35 KMac, 0.001% MACs, )\n",
      "    (4): MaxPool2d(0, 0.000% Params, 100.35 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (5): Flatten(0, 0.000% Params, 0.0 Mac, 0.000% MACs, start_dim=1, end_dim=-1)\n",
      "  (6): Linear(102.76 M, 79.782% Params, 102.76 MMac, 1.345% MACs, in_features=25088, out_features=4096, bias=True)\n",
      "  (7): ReLU(0, 0.000% Params, 4.1 KMac, 0.000% MACs, )\n",
      "  (8): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (9): Linear(16.78 M, 13.028% Params, 16.78 MMac, 0.220% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "  (10): ReLU(0, 0.000% Params, 4.1 KMac, 0.000% MACs, )\n",
      "  (11): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)\n",
      "  (12): Linear(40.97 k, 0.032% Params, 40.97 KMac, 0.001% MACs, in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "Computational complexity:       7.64 GMac\n",
      "Number of parameters:           128.81 M\n"
     ]
    }
   ],
   "source": [
    "vgg11_f, macs_vgg11_f, paras_vgg11_f = get_model_info(vgg11, channel_f, num_labels_f)\n",
    "print('-'*50)\n",
    "vgg11_c100, macs_vgg11_c100, paras_vgg11_c100 = get_model_info(vgg11, channel_c, num_labels_c100)\n",
    "print('-'*50)\n",
    "vgg11_c10, macs_vgg11_c10, paras_vgg11_c10 = get_model_info(vgg11, channel_c, num_labels_c10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the output size of each layers after the picture is passed through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet', 'vgg11', 'vgg13', 'vgg16', 'resnet18', 'resnet34', 'resnet50', 'googlenet_origin', 'googlenet_mod1', 'googlenet_mod2', 'googlenet_mod3', 'googlenet_mod4', 'googlenet_mod5', 'googlenet_mod6', 'googlenet_mod7', 'googlenet_mod8', 'googlenet_mod9', 'mobilenetv1_path', 'mobilenetv2_path']\n"
     ]
    }
   ],
   "source": [
    "print(models_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model list according to models_name order\n",
    "models_f_list = [vgg11_f]\n",
    "\n",
    "models_c100_list = [vgg11_c100]\n",
    "\n",
    "models_c10_list = [vgg11_c10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_f = torch.randn(size=(1, 1, 224, 224), dtype=torch.float32) # fashion mnist\n",
    "\n",
    "# for model in models_f_list:\n",
    "#     print(model)\n",
    "#     for layer in model:\n",
    "#         X_f=layer(X_f)\n",
    "#         print(layer.__class__.__name__,'output shape:\\t',X_f.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all the datas:  \n",
    "    1. FashionMNIST\n",
    "    2. CIFAR100\n",
    "    3. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# fashion mnist\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集, 然后将其加载到内存中\n",
    "\n",
    "    Defined in :numref:`sec_fashion_mnist`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=get_dataloader_workers()))\n",
    "\n",
    "def load_data_cifar100(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR100(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    \n",
    "def load_data_cifar10(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    # import the cifar100 dataset\n",
    "    cifar_train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(cifar_train, batch_size, shuffle=True,\n",
    "                                        num_workers=get_dataloader_workers()),\n",
    "            torch.utils.data.DataLoader(cifar_test, batch_size, shuffle=False,\n",
    "                                        num_workers=get_dataloader_workers()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [128]\n",
    "epochs = [5]\n",
    "rounds = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is: cuda\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('The device is:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using pynvml to get the GPU power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvml_sampling_thread(handle, filename, stop_event, sampling_interval):\n",
    "    \"\"\"\n",
    "    在单独的线程中定期调用 NVML, 获取功耗数据并存储到 data_queue 中。\n",
    "    参数：\n",
    "    - handle: nvmlDeviceGetHandleByIndex(0) 得到的 GPU 句柄\n",
    "    - data_queue: 用于存放 (timestamp, power_in_watts) 数据的队列\n",
    "    - stop_event: 当此事件被设置时，线程应结束循环\n",
    "    - sampling_interval: 采样间隔（秒）\n",
    "    \"\"\"\n",
    "    with open(filename/'energy_consumption_file.csv', 'a') as f:  # 追加模式\n",
    "        # 写入列名\n",
    "        f.write(\"timestamp,power_in_watts\\n\")\n",
    "        while not stop_event.is_set():\n",
    "            try:\n",
    "                # 采集功率和时间戳\n",
    "                current_time = time.time()\n",
    "                current_power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # 转换 mW -> W\n",
    "                # 写入文件\n",
    "                f.write(f\"{current_time},{current_power}\\n\")\n",
    "                # 等待下一次采样\n",
    "                time.sleep(sampling_interval)\n",
    "            except pynvml.NVMLError as e:\n",
    "                print(f\"NVML Error: {e}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the interval of the power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_power_over_interval(samples, start_time, end_time):\n",
    "    # 假定 samples是按时间升序排序的 (t, p)\n",
    "    # 若未排序，请先排序:\n",
    "    # samples = sorted(samples, key=lambda x: x[0])\n",
    "    \n",
    "    def interpolate(samples, target_time):\n",
    "        # 在 samples 中找到 target_time 左右最近的两个点，并进行线性插值\n",
    "        # 若 target_time 恰好等于某个样本点时间，直接返回该点功率\n",
    "        # 若无法找到两侧点（如 target_time在样本时间轴外），根据情况返回None或边界点\n",
    "        n = len(samples)\n",
    "        if n == 0:\n",
    "            return None\n",
    "        # 若 target_time 小于第一个样本点时间，无法向左插值，这里直接返回第一个点的功率值(或None)\n",
    "        if target_time <= samples[0][0]:\n",
    "            # 简化处理：返回最早样本点的功率（或None）\n",
    "            return samples[0][1]\n",
    "        # 若 target_time 大于最后一个样本点时间，无法向右插值，返回最后一个点的功率（或None）\n",
    "        if target_time >= samples[-1][0]:\n",
    "            return samples[-1][1]\n",
    "\n",
    "        # 否则，在中间插值\n",
    "        # 使用二分查找快速定位\n",
    "        import bisect\n",
    "        times = [t for t, _ in samples]\n",
    "        pos = bisect.bisect_left(times, target_time)\n",
    "        # pos是使times保持有序插入target_time的位置\n",
    "        # 因为target_time不在已有样本点中，pos不会越界且pos>0且pos<n\n",
    "        t1, p1 = samples[pos-1]\n",
    "        t2, p2 = samples[pos]\n",
    "        # 线性插值： p = p1 + (p2 - p1)*((target_time - t1)/(t2 - t1))\n",
    "        ratio = (target_time - t1) / (t2 - t1)\n",
    "        p = p1 + (p2 - p1)*ratio\n",
    "        return p\n",
    "\n",
    "    # 从原始 samples 中筛选出位于[start_time, end_time]内的点\n",
    "    filtered = [(t, p) for t, p in samples if start_time <= t <= end_time]\n",
    "\n",
    "    # 如果不足2个点，则尝试使用插值\n",
    "    if len(filtered) < 2:\n",
    "        # 无论如何都需要在边界处插值出两个点(起码start和end)\n",
    "        start_power = interpolate(samples, start_time)\n",
    "        end_power = interpolate(samples, end_time)\n",
    "\n",
    "        # 如果从样本中无法插值出任何有意义的点（比如samples为空或无法插值），返回0.0\n",
    "        if start_power is None or end_power is None:\n",
    "            return 0.0\n",
    "\n",
    "        # 将插值的边界点加入到 filtered\n",
    "        # 注意：如果filtered中有一个点在区间内，我们也需要确保边界有两点以上\n",
    "        # 例如filtered只有一个点在中间，则需要在start和end插值点全部加入。\n",
    "        # 若filtered为空，则只用start/end两点插值点求积分\n",
    "        new_filtered = [(start_time, start_power)] + filtered + [(end_time, end_power)]\n",
    "        # 确保按时间排序\n",
    "        new_filtered.sort(key=lambda x: x[0])\n",
    "        filtered = new_filtered\n",
    "\n",
    "    # 正常积分计算\n",
    "    if len(filtered) < 2:\n",
    "        # 经过插值仍不够，返回0\n",
    "        return 0.0\n",
    "\n",
    "    total_energy = 0.0\n",
    "    for i in range(len(filtered)-1):\n",
    "        t1, p1 = filtered[i]\n",
    "        t2, p2 = filtered[i+1]\n",
    "        dt = t2 - t1\n",
    "        avg_p = (p1 + p2)/2.0\n",
    "        total_energy += avg_p * dt\n",
    "\n",
    "    return total_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(net, train_iter, test_iter, num_epochs, lr, device, filename, sampling_interval):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize GradScaler for mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # save all epochs time data using list\n",
    "    to_device_intervals_total = []\n",
    "    forward_intervals_total = []\n",
    "    loss_intervals_total = []\n",
    "    backward_intervals_total = []\n",
    "    optimize_intervals_total = []\n",
    "    test_intervals_total = []\n",
    "\n",
    "    # create a list to store the epoch time data\n",
    "    epoch_intervals_total = []\n",
    "    \n",
    "    # Initialize NVML and sampling thread\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    stop_event = threading.Event()\n",
    "    sampler_thread = threading.Thread(target=nvml_sampling_thread, args=(handle, filename, stop_event, sampling_interval))\n",
    "    sampler_thread.start()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('The epoch is:', epoch+1)\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "        to_device_intervals_epoch = []  # 用来记录本epoch每个batch的to_device时间段\n",
    "        forward_intervals_epoch = []  # 用来记录本epoch每个batch的forward时间段\n",
    "        loss_intervals_epoch = []  # 用来记录本epoch每个batch的loss时间段\n",
    "        backward_intervals_epoch = [] \n",
    "        optimize_intervals_epoch = []\n",
    "        test_intervals_epoch = []   \n",
    "        epoch_intervals_epoch = []  # 用来记录本epoch的时间段\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            print('The batch is:', i+1)\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # 记录to_device前后的时间戳\n",
    "            start_ttd_time = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            torch.cuda.synchronize()\n",
    "            end_ttd_time = time.time()\n",
    "            to_device_intervals_epoch.append((start_ttd_time, end_ttd_time))\n",
    "\n",
    "            # forward with autocast\n",
    "            start_forward_time = time.time()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_hat = net(X)\n",
    "                l = loss_fn(y_hat, y)  # loss inside autocast\n",
    "            torch.cuda.synchronize()\n",
    "            end_forward_time = time.time()\n",
    "            forward_intervals_epoch.append((start_forward_time, end_forward_time))\n",
    "\n",
    "            # backward with scaler\n",
    "            start_backward_time = time.time()\n",
    "            scaler.scale(l).backward()\n",
    "            torch.cuda.synchronize()\n",
    "            end_backward_time = time.time()\n",
    "            backward_intervals_epoch.append((start_backward_time, end_backward_time))\n",
    "\n",
    "            # optimize with scaler\n",
    "            start_optimize_time = time.time()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            torch.cuda.synchronize()\n",
    "            end_optimize_time = time.time()\n",
    "            optimize_intervals_epoch.append((start_optimize_time, end_optimize_time))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.item() * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "            # Free memory for the batch\n",
    "            del X, y, y_hat, l\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Evaluation (test)\n",
    "        start_test_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        end_test_time = time.time()\n",
    "        print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "        test_intervals_epoch.append((start_test_time, end_test_time))\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_intervals_epoch.append((epoch_start_time, epoch_end_time))\n",
    "\n",
    "        # data need to be saved\n",
    "        # add the intervals_epoch to intervals_total\n",
    "        to_device_intervals_total.append(to_device_intervals_epoch)\n",
    "        forward_intervals_total.append(forward_intervals_epoch)\n",
    "        loss_intervals_total.append(loss_intervals_epoch)\n",
    "        backward_intervals_total.append(backward_intervals_epoch)\n",
    "        optimize_intervals_total.append(optimize_intervals_epoch)\n",
    "        test_intervals_total.append(test_intervals_epoch)\n",
    "        epoch_intervals_total.append(epoch_intervals_epoch)\n",
    "\n",
    "    # End training and close thread\n",
    "    stop_event.set()\n",
    "    sampler_thread.join()\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    return to_device_intervals_total, forward_intervals_total, loss_intervals_total, backward_intervals_total, optimize_intervals_total, test_intervals_total, epoch_intervals_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_func(net, train_iter, test_iter, num_epochs, lr, device, filename, sampling_interval):\n",
    "#     def init_weights(m):\n",
    "#         if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "#     net.apply(init_weights)\n",
    "#     print('training on', device)\n",
    "#     net.to(device)\n",
    "#     # print(f'The name of the layers are: {alexlayer}')\n",
    "#     optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "#     loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#     scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#     # save all epochs time data using list\n",
    "#     to_device_intervals_total = []\n",
    "#     forward_intervals_total = []\n",
    "#     loss_intervals_total = []\n",
    "#     backward_intervals_total = []\n",
    "#     optimize_intervals_total = []\n",
    "#     test_intervals_total = []\n",
    "\n",
    "#     # create a list to store the epoch time data\n",
    "#     epoch_intervals_total = []\n",
    "    \n",
    "#     # 初始化NVML和采样线程\n",
    "#     pynvml.nvmlInit()\n",
    "#     handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "#     stop_event = threading.Event()\n",
    "#     sampler_thread = threading.Thread(target=nvml_sampling_thread, args=(handle, filename, stop_event, sampling_interval))\n",
    "#     sampler_thread.start()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print('The epoch is:', epoch+1)\n",
    "#         metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "#         to_device_intervals_epoch = []  # 用来记录本epoch每个batch的to_device时间段\n",
    "#         forward_intervals_epoch = []  # 用来记录本epoch每个batch的forward时间段\n",
    "#         loss_intervals_epoch = []  # 用来记录本epoch每个batch的loss时间段\n",
    "#         backward_intervals_epoch = [] \n",
    "#         optimize_intervals_epoch = []\n",
    "#         test_intervals_epoch = []   \n",
    "#         epoch_intervals_epoch = []  # 用来记录本epoch的时间段\n",
    "\n",
    "#         epoch_start_time = time.time()\n",
    "\n",
    "#         net.train()\n",
    "#         for i, (X, y) in enumerate(train_iter):\n",
    "#             print('The batch is:', i+1)\n",
    "#             optimizer.zero_grad()\n",
    "#             torch.cuda.synchronize()\n",
    "\n",
    "#             # 记录to_device前后的时间戳\n",
    "#             start_ttd_time = time.time()\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             torch.cuda.synchronize()\n",
    "#             end_ttd_time = time.time()\n",
    "#             to_device_intervals_epoch.append((start_ttd_time, end_ttd_time))\n",
    "\n",
    "#             # forward\n",
    "#             start_forward_time = time.time()\n",
    "#             y_hat = net(X)\n",
    "#             torch.cuda.synchronize()\n",
    "#             end_forward_time = time.time()\n",
    "#             forward_intervals_epoch.append((start_forward_time, end_forward_time))\n",
    "\n",
    "#             # loss\n",
    "#             start_loss_time = time.time()\n",
    "#             l = loss_fn(y_hat, y)\n",
    "#             torch.cuda.synchronize()\n",
    "#             end_loss_time = time.time()\n",
    "#             loss_intervals_epoch.append((start_loss_time, end_loss_time))\n",
    "\n",
    "#             # backward\n",
    "#             start_backward_time = time.time()\n",
    "#             l.backward()\n",
    "#             torch.cuda.synchronize()\n",
    "#             end_backward_time = time.time()\n",
    "#             backward_intervals_epoch.append((start_backward_time, end_backward_time))\n",
    "\n",
    "#             # optimize\n",
    "#             start_optimize_time = time.time()\n",
    "#             optimizer.step()\n",
    "#             torch.cuda.synchronize()\n",
    "#             end_optimize_time = time.time()\n",
    "#             optimize_intervals_epoch.append((start_optimize_time, end_optimize_time))\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 metric.add(l*X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "#             train_acc = metric[1] / metric[2]\n",
    "\n",
    "#             del X, y, y_hat, l\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         start_test_time = time.time()\n",
    "#         test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "#         end_test_time = time.time()\n",
    "#         print(f'train acc {train_acc:.3f}, test acc {test_acc:.3f}')\n",
    "#         test_intervals_epoch.append((start_test_time, end_test_time))\n",
    "\n",
    "#         epoch_end_time = time.time()\n",
    "#         epoch_intervals_epoch.append((epoch_start_time, epoch_end_time))\n",
    "\n",
    "#         # data need to be saved\n",
    "#         # add the intervals_epoch to intervals_total\n",
    "#         to_device_intervals_total.append(to_device_intervals_epoch)\n",
    "#         forward_intervals_total.append(forward_intervals_epoch)\n",
    "#         loss_intervals_total.append(loss_intervals_epoch)\n",
    "#         backward_intervals_total.append(backward_intervals_epoch)\n",
    "#         optimize_intervals_total.append(optimize_intervals_epoch)\n",
    "#         test_intervals_total.append(test_intervals_epoch)\n",
    "#         epoch_intervals_total.append(epoch_intervals_epoch)\n",
    "\n",
    "\n",
    "#     # 训练结束后关闭线程\n",
    "#     stop_event.set()\n",
    "#     sampler_thread.join()\n",
    "\n",
    "#     pynvml.nvmlShutdown()\n",
    "\n",
    "#     return to_device_intervals_total, forward_intervals_total, loss_intervals_total, backward_intervals_total, optimize_intervals_total, test_intervals_total, epoch_intervals_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set a function to train the model with FashionMNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_f(main_folder, batch_size, num_epochs, round, lr, device, sample_interval, net):\n",
    "    print(f'The epoch is set: {num_epochs}, batch is set: {batch_size}, is in {round+1}th running')\n",
    "    # create the folder to store the data\n",
    "    # epoch_batch_folder = main_folder/f'E{num_epochs}_B{batch_size}_R{round}'\n",
    "    sr_number = int(sample_interval*1000)\n",
    "    epoch_batch_folder = f'E{num_epochs}_B{batch_size}_R{round}_SR{sr_number}'\n",
    "    dataset_dir = 'fashion_mnist'\n",
    "\n",
    "    # the folder path is main_folder/epoch_batch_folder\n",
    "    folder_path = main_folder/epoch_batch_folder/dataset_dir\n",
    "    print(f'The folder path is: {folder_path}')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)\n",
    "    # show the shape of the data\n",
    "    list_of_i = []\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        if i < 3:\n",
    "            print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "        else:\n",
    "            pass\n",
    "        list_of_i.append(i)\n",
    "    print(f'The number of batches is: {np.array(list_of_i).shape}')\n",
    "    to_device_intervals_total, forward_intervals_total, loss_intervals_total,\\\n",
    "          backward_intervals_total, optimize_intervals_total, test_intervals_total, epoch_intervals_total = train_func(net, train_iter, test_iter, num_epochs, lr, device, folder_path, sample_interval)\n",
    "\n",
    "    # transfer the data to the numpy array\n",
    "    to_device_data = np.array(to_device_intervals_total)\n",
    "    forward_time = np.array(forward_intervals_total)\n",
    "    loss_time = np.array(loss_intervals_total)\n",
    "    backward_time = np.array(backward_intervals_total)\n",
    "    optimize_time = np.array(optimize_intervals_total)\n",
    "    test_time = np.array(test_intervals_total)\n",
    "    epoch_time = np.array(epoch_intervals_total)\n",
    "\n",
    "    # save the data\n",
    "    np.save(folder_path/'to_device.npy', to_device_data, allow_pickle=True)\n",
    "    np.save(folder_path/'forward.npy', forward_time, allow_pickle=True)\n",
    "    np.save(folder_path/'loss.npy', loss_time, allow_pickle=True)\n",
    "    np.save(folder_path/'backward.npy', backward_time, allow_pickle=True)\n",
    "    np.save(folder_path/'optimize.npy', optimize_time, allow_pickle=True)\n",
    "    np.save(folder_path/'test.npy', test_time, allow_pickle=True)\n",
    "    np.save(folder_path/'epoch.npy', epoch_time, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models_f_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# create the folder to store the data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfashion_mnist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar100\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mmodels_f_list\u001b[49m)):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# for each start, clear the cache in the gpu \u001b[39;00m\n\u001b[1;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# for each start, clear the memory in the gpu using the torch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models_f_list' is not defined"
     ]
    }
   ],
   "source": [
    "sampling_interval = 0.002 # 2ms\n",
    "# create the folder to store the data\n",
    "datasets = ['fashion_mnist', 'cifar100', 'cifar10']\n",
    "\n",
    "for i in range(len(models_f_list)):\n",
    "    # for each start, clear the cache in the gpu \n",
    "    torch.cuda.empty_cache()\n",
    "    # for each start, clear the memory in the gpu using the torch\n",
    "    net = models_f_list[i]\n",
    "    main_folder = DataList[i]\n",
    "    main_folder = os.path.join(main_folder, datasets[0])  \n",
    "    # main_folder = os.path.join(main_folder, datasets[1])  \n",
    "    # main_folder = os.path.join(main_folder, datasets[2])  \n",
    "\n",
    "    print('The folder is:', main_folder)\n",
    "    if main_folder.exists():\n",
    "        print(\"文件存在。\")\n",
    "    else:\n",
    "        os.makedirs(main_folder)\n",
    "        print(\"文件不存在，已创建。\")\n",
    "        print(\"文件创建于：\", main_folder)\n",
    "    for epoch in epochs:\n",
    "        for batch in batch_size:\n",
    "            for round in range(rounds):\n",
    "                train_model_f(main_folder, batch, epoch, round, lr, device, sampling_interval, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder is: /root/autodl-tmp/GreenAI/3080/ModelsData/mobilenetv2_path\n",
      "文件存在。\n",
      "The epoch is set: 5, batch is set: 256, is in 1th running\n",
      "The folder path is: /root/autodl-tmp/GreenAI/3080/ModelsData/mobilenetv2_path/E5_B256_R0_SR2\n",
      "the shape of the 0 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 1 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "the shape of the 2 batch of the train_iter is: torch.Size([256, 1, 224, 224])\n",
      "The number of batches is: (235,)\n",
      "training on cuda\n",
      "The epoch is: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5606/2728429223.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5606/2728429223.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.624, test acc 0.771\n",
      "The epoch is: 2\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.823, test acc 0.786\n",
      "The epoch is: 3\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.861, test acc 0.796\n",
      "The epoch is: 4\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.881, test acc 0.807\n",
      "The epoch is: 5\n",
      "The batch is: 1\n",
      "The batch is: 2\n",
      "The batch is: 3\n",
      "The batch is: 4\n",
      "The batch is: 5\n",
      "The batch is: 6\n",
      "The batch is: 7\n",
      "The batch is: 8\n",
      "The batch is: 9\n",
      "The batch is: 10\n",
      "The batch is: 11\n",
      "The batch is: 12\n",
      "The batch is: 13\n",
      "The batch is: 14\n",
      "The batch is: 15\n",
      "The batch is: 16\n",
      "The batch is: 17\n",
      "The batch is: 18\n",
      "The batch is: 19\n",
      "The batch is: 20\n",
      "The batch is: 21\n",
      "The batch is: 22\n",
      "The batch is: 23\n",
      "The batch is: 24\n",
      "The batch is: 25\n",
      "The batch is: 26\n",
      "The batch is: 27\n",
      "The batch is: 28\n",
      "The batch is: 29\n",
      "The batch is: 30\n",
      "The batch is: 31\n",
      "The batch is: 32\n",
      "The batch is: 33\n",
      "The batch is: 34\n",
      "The batch is: 35\n",
      "The batch is: 36\n",
      "The batch is: 37\n",
      "The batch is: 38\n",
      "The batch is: 39\n",
      "The batch is: 40\n",
      "The batch is: 41\n",
      "The batch is: 42\n",
      "The batch is: 43\n",
      "The batch is: 44\n",
      "The batch is: 45\n",
      "The batch is: 46\n",
      "The batch is: 47\n",
      "The batch is: 48\n",
      "The batch is: 49\n",
      "The batch is: 50\n",
      "The batch is: 51\n",
      "The batch is: 52\n",
      "The batch is: 53\n",
      "The batch is: 54\n",
      "The batch is: 55\n",
      "The batch is: 56\n",
      "The batch is: 57\n",
      "The batch is: 58\n",
      "The batch is: 59\n",
      "The batch is: 60\n",
      "The batch is: 61\n",
      "The batch is: 62\n",
      "The batch is: 63\n",
      "The batch is: 64\n",
      "The batch is: 65\n",
      "The batch is: 66\n",
      "The batch is: 67\n",
      "The batch is: 68\n",
      "The batch is: 69\n",
      "The batch is: 70\n",
      "The batch is: 71\n",
      "The batch is: 72\n",
      "The batch is: 73\n",
      "The batch is: 74\n",
      "The batch is: 75\n",
      "The batch is: 76\n",
      "The batch is: 77\n",
      "The batch is: 78\n",
      "The batch is: 79\n",
      "The batch is: 80\n",
      "The batch is: 81\n",
      "The batch is: 82\n",
      "The batch is: 83\n",
      "The batch is: 84\n",
      "The batch is: 85\n",
      "The batch is: 86\n",
      "The batch is: 87\n",
      "The batch is: 88\n",
      "The batch is: 89\n",
      "The batch is: 90\n",
      "The batch is: 91\n",
      "The batch is: 92\n",
      "The batch is: 93\n",
      "The batch is: 94\n",
      "The batch is: 95\n",
      "The batch is: 96\n",
      "The batch is: 97\n",
      "The batch is: 98\n",
      "The batch is: 99\n",
      "The batch is: 100\n",
      "The batch is: 101\n",
      "The batch is: 102\n",
      "The batch is: 103\n",
      "The batch is: 104\n",
      "The batch is: 105\n",
      "The batch is: 106\n",
      "The batch is: 107\n",
      "The batch is: 108\n",
      "The batch is: 109\n",
      "The batch is: 110\n",
      "The batch is: 111\n",
      "The batch is: 112\n",
      "The batch is: 113\n",
      "The batch is: 114\n",
      "The batch is: 115\n",
      "The batch is: 116\n",
      "The batch is: 117\n",
      "The batch is: 118\n",
      "The batch is: 119\n",
      "The batch is: 120\n",
      "The batch is: 121\n",
      "The batch is: 122\n",
      "The batch is: 123\n",
      "The batch is: 124\n",
      "The batch is: 125\n",
      "The batch is: 126\n",
      "The batch is: 127\n",
      "The batch is: 128\n",
      "The batch is: 129\n",
      "The batch is: 130\n",
      "The batch is: 131\n",
      "The batch is: 132\n",
      "The batch is: 133\n",
      "The batch is: 134\n",
      "The batch is: 135\n",
      "The batch is: 136\n",
      "The batch is: 137\n",
      "The batch is: 138\n",
      "The batch is: 139\n",
      "The batch is: 140\n",
      "The batch is: 141\n",
      "The batch is: 142\n",
      "The batch is: 143\n",
      "The batch is: 144\n",
      "The batch is: 145\n",
      "The batch is: 146\n",
      "The batch is: 147\n",
      "The batch is: 148\n",
      "The batch is: 149\n",
      "The batch is: 150\n",
      "The batch is: 151\n",
      "The batch is: 152\n",
      "The batch is: 153\n",
      "The batch is: 154\n",
      "The batch is: 155\n",
      "The batch is: 156\n",
      "The batch is: 157\n",
      "The batch is: 158\n",
      "The batch is: 159\n",
      "The batch is: 160\n",
      "The batch is: 161\n",
      "The batch is: 162\n",
      "The batch is: 163\n",
      "The batch is: 164\n",
      "The batch is: 165\n",
      "The batch is: 166\n",
      "The batch is: 167\n",
      "The batch is: 168\n",
      "The batch is: 169\n",
      "The batch is: 170\n",
      "The batch is: 171\n",
      "The batch is: 172\n",
      "The batch is: 173\n",
      "The batch is: 174\n",
      "The batch is: 175\n",
      "The batch is: 176\n",
      "The batch is: 177\n",
      "The batch is: 178\n",
      "The batch is: 179\n",
      "The batch is: 180\n",
      "The batch is: 181\n",
      "The batch is: 182\n",
      "The batch is: 183\n",
      "The batch is: 184\n",
      "The batch is: 185\n",
      "The batch is: 186\n",
      "The batch is: 187\n",
      "The batch is: 188\n",
      "The batch is: 189\n",
      "The batch is: 190\n",
      "The batch is: 191\n",
      "The batch is: 192\n",
      "The batch is: 193\n",
      "The batch is: 194\n",
      "The batch is: 195\n",
      "The batch is: 196\n",
      "The batch is: 197\n",
      "The batch is: 198\n",
      "The batch is: 199\n",
      "The batch is: 200\n",
      "The batch is: 201\n",
      "The batch is: 202\n",
      "The batch is: 203\n",
      "The batch is: 204\n",
      "The batch is: 205\n",
      "The batch is: 206\n",
      "The batch is: 207\n",
      "The batch is: 208\n",
      "The batch is: 209\n",
      "The batch is: 210\n",
      "The batch is: 211\n",
      "The batch is: 212\n",
      "The batch is: 213\n",
      "The batch is: 214\n",
      "The batch is: 215\n",
      "The batch is: 216\n",
      "The batch is: 217\n",
      "The batch is: 218\n",
      "The batch is: 219\n",
      "The batch is: 220\n",
      "The batch is: 221\n",
      "The batch is: 222\n",
      "The batch is: 223\n",
      "The batch is: 224\n",
      "The batch is: 225\n",
      "The batch is: 226\n",
      "The batch is: 227\n",
      "The batch is: 228\n",
      "The batch is: 229\n",
      "The batch is: 230\n",
      "The batch is: 231\n",
      "The batch is: 232\n",
      "The batch is: 233\n",
      "The batch is: 234\n",
      "The batch is: 235\n",
      "train acc 0.895, test acc 0.827\n"
     ]
    }
   ],
   "source": [
    "# sampling_interval = 0.002 # 2ms\n",
    "# # # create the folder to store the data\n",
    "\n",
    "# # for i in range(len(models_f_list)):\n",
    "# # 2024.12.18-yj: 由于之前的运行 gpu 内存爆了,需要继续后续训练\n",
    "# # 2024.12.19-yj: mobilenetv2 持续爆内存,需要继续后续训练\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "# torch.cuda.empty_cache()\n",
    "# # for each start, clear the memory in the gpu using the torch\n",
    "# # print(models_f_list[-1])\n",
    "# net = mobilenetv2_f\n",
    "# main_folder = DataList[-1]  \n",
    "# print('The folder is:', main_folder)\n",
    "# if main_folder.exists():\n",
    "#     print(\"文件存在。\")\n",
    "# else:\n",
    "#     os.makedirs(main_folder)\n",
    "#     print(\"文件不存在，已创建。\")\n",
    "#     print(\"文件创建于：\", main_folder)\n",
    "# for epoch in epochs:\n",
    "#     for batch in batch_size:\n",
    "#         for round in range(rounds):\n",
    "#             train_model_f(main_folder, batch, epoch, round, lr, device, sampling_interval, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greenai",
   "language": "python",
   "name": "greenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
