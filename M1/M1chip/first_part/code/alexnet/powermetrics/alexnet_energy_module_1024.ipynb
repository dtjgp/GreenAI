{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ad276f",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 深度卷积神经网络（AlexNet）\n",
    ":label:`sec_alexnet`\n",
    "\n",
    "在LeNet提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但卷积神经网络并没有主导这些领域。这是因为虽然LeNet在小数据集上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究。事实上，在上世纪90年代初到2012年之间的大部分时间里，神经网络往往被其他机器学习方法超越，如支持向量机（support vector machines）。\n",
    "\n",
    "在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。\n",
    "\n",
    "虽然上世纪90年代就有了一些神经网络加速卡，但仅靠它们还不足以开发出有大量参数的深层多通道多层卷积神经网络。此外，当时的数据集仍然相对较小。除了这些障碍，训练神经网络的一些关键技巧仍然缺失，包括启发式参数初始化、随机梯度下降的变体、非挤压激活函数和有效的正则化技术。\n",
    "\n",
    "因此，与训练*端到端*（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：\n",
    "\n",
    "1. 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）。\n",
    "2. 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。\n",
    "3. 通过标准的特征提取算法，如SIFT（尺度不变特征变换） :cite:`Lowe.2004`和SURF（加速鲁棒特征） :cite:`Bay.Tuytelaars.Van-Gool.2006`或其他手动调整的流水线来输入数据。\n",
    "4. 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。\n",
    "\n",
    "当人们和机器学习研究人员交谈时，会发现机器学习研究人员相信机器学习既重要又美丽：优雅的理论去证明各种模型的性质。机器学习是一个正在蓬勃发展、严谨且非常有用的领域。然而，当人们和计算机视觉研究人员交谈，会听到一个完全不同的故事。计算机视觉研究人员会告诉一个诡异事实————推动领域进步的是数据特征，而不是学习算法。计算机视觉研究人员相信，从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。\n",
    "\n",
    "## 学习表征\n",
    "\n",
    "另一种预测这个领域发展的方法————观察图像特征的提取方法。在2012年前，图像特征都是机械地计算出来的。事实上，设计一套新的特征函数、改进结果，并撰写论文是盛极一时的潮流。SIFT :cite:`Lowe.2004`、SURF :cite:`Bay.Tuytelaars.Van-Gool.2006`、HOG（定向梯度直方图） :cite:`Dalal.Triggs.2005`、[bags of visual words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision)和类似的特征提取方法占据了主导地位。\n",
    "\n",
    "另一组研究人员，包括Yann LeCun、Geoff Hinton、Yoshua Bengio、Andrew Ng、Shun ichi Amari和Juergen Schmidhuber，想法则与众不同：他们认为特征本身应该被学习。此外，他们还认为，在合理地复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理。事实上，Alex Krizhevsky、Ilya Sutskever和Geoff Hinton提出了一种新的卷积神经网络变体*AlexNet*。在2012年ImageNet挑战赛中取得了轰动一时的成绩。AlexNet以Alex Krizhevsky的名字命名，他是论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`的第一作者。\n",
    "\n",
    "有趣的是，在网络的最底层，模型学习到了一些类似于传统滤波器的特征抽取器。 :numref:`fig_filters`是从AlexNet论文 :cite:`Krizhevsky.Sutskever.Hinton.2012`复制的，描述了底层图像特征。\n",
    "\n",
    "![AlexNet第一层学习到的特征抽取器。](../img/filters.png)\n",
    ":width:`400px`\n",
    ":label:`fig_filters`\n",
    "\n",
    "AlexNet的更高层建立在这些底层表示的基础上，以表示更大的特征，如眼睛、鼻子、草叶等等。而更高的层可以检测整个物体，如人、飞机、狗或飞盘。最终的隐藏神经元可以学习图像的综合表示，从而使属于不同类别的数据易于区分。尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些尝试都未有突破。深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。\n",
    "\n",
    "### 缺少的成分：数据\n",
    "\n",
    "包含许多特征的深度模型需要大量的有标签数据，才能显著优于基于凸优化的传统方法（如线性方法和核方法）。\n",
    "然而，限于早期计算机有限的存储和90年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校（UCI）提供的若干个公开数据集，其中许多数据集只有几百至几千张在非自然环境下以低分辨率拍摄的图像。这一状况在2010年前后兴起的大数据浪潮中得到改善。2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。这种规模是前所未有的。这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。\n",
    "\n",
    "### 缺少的成分：硬件\n",
    "\n",
    "深度学习对计算资源要求很高，训练可能需要数百个迭代轮数，每次迭代都需要通过代价高昂的许多线性代数层传递数据。这也是为什么在20世纪90年代至21世纪初，优化凸目标的简单算法是研究人员的首选。然而，用GPU训练神经网络改变了这一格局。*图形处理器*（Graphics Processing Unit，GPU）早年用来加速图形处理，使电脑游戏玩家受益。GPU可优化高吞吐量的$4 \\times 4$矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。由此，英伟达（NVIDIA）和ATI已经开始为通用计算操作优化gpu，甚至把它们作为*通用GPU*（general-purpose GPUs，GPGPU）来销售。\n",
    "\n",
    "那么GPU比CPU强在哪里呢？\n",
    "\n",
    "首先，我们深度理解一下中央处理器（Central Processing Unit，CPU）的*核心*。\n",
    "CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。\n",
    "它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。\n",
    "然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。\n",
    "它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。\n",
    "现代笔记本电脑最多有4核，即使是高端服务器也很少超过64核，因为它们的性价比不高。\n",
    "\n",
    "相比于CPU，GPU由$100 \\sim 1000$个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。\n",
    "虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级。\n",
    "例如，NVIDIA最近一代的Ampere GPU架构为每个芯片提供了高达312 TFlops的浮点性能，而CPU的浮点性能到目前为止还没有超过1 TFlops。\n",
    "之所以有如此大的差距，原因其实很简单：首先，功耗往往会随时钟频率呈二次方增长。\n",
    "对于一个CPU核心，假设它的运行速度比GPU快4倍，但可以使用16个GPU核代替，那么GPU的综合性能就是CPU的$16 \\times 1/4 = 4$倍。\n",
    "其次，GPU内核要简单得多，这使得它们更节能。\n",
    "此外，深度学习中的许多操作需要相对较高的内存带宽，而GPU拥有10倍于CPU的带宽。\n",
    "\n",
    "回到2012年的重大突破，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。\n",
    "于是，他们使用两个显存为3GB的NVIDIA GTX580 GPU实现了快速卷积运算。他们的创新[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)几年来它一直是行业标准，并推动了深度学习热潮。\n",
    "\n",
    "## AlexNet\n",
    "\n",
    "2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。\n",
    "AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。\n",
    "\n",
    "AlexNet和LeNet的架构非常相似，如 :numref:`fig_alexnet`所示。\n",
    "注意，本书在这里提供的是一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。\n",
    "\n",
    "![从LeNet（左）到AlexNet（右）](../img/alexnet.svg)\n",
    ":label:`fig_alexnet`\n",
    "\n",
    "AlexNet和LeNet的设计理念非常相似，但也存在显著差异。\n",
    "\n",
    "1. AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。\n",
    "2. AlexNet使用ReLU而不是sigmoid作为其激活函数。\n",
    "\n",
    "下面的内容将深入研究AlexNet的细节。\n",
    "\n",
    "### 模型设计\n",
    "\n",
    "在AlexNet的第一层，卷积窗口的形状是$11\\times11$。\n",
    "由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。\n",
    "第二层中的卷积窗口形状被缩减为$5\\times5$，然后是$3\\times3$。\n",
    "此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为$3\\times3$、步幅为2的最大汇聚层。\n",
    "而且，AlexNet的卷积通道数目是LeNet的10倍。\n",
    "\n",
    "在最后一个卷积层后有两个全连接层，分别有4096个输出。\n",
    "这两个巨大的全连接层拥有将近1GB的模型参数。\n",
    "由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数。\n",
    "幸运的是，现在GPU显存相对充裕，所以现在很少需要跨GPU分解模型（因此，本书的AlexNet模型在这方面与原始论文稍有不同）。\n",
    "\n",
    "### 激活函数\n",
    "\n",
    "此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。\n",
    "一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。\n",
    "另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。\n",
    "当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数。\n",
    "相反，ReLU激活函数在正区间的梯度总是1。\n",
    "因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。\n",
    "\n",
    "### 容量控制和预处理\n",
    "\n",
    "AlexNet通过暂退法（ :numref:`sec_dropout`）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。\n",
    "为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。\n",
    "这使得模型更健壮，更大的样本量有效地减少了过拟合。\n",
    "在 :numref:`sec_image_augmentation`中更详细地讨论数据扩增。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ae34b7",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 这里使用一个11*11的更大窗口来捕捉对象。\n",
    "    # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "    # 另外，输出通道的数目远大于LeNet\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "    # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "    nn.Linear(6400, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "    nn.Linear(4096, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efcff903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片大小为224*224，核大小为11，步幅为4，填充为1，输出通道为96，每次输出的大小为\n",
    "# (224-11+4*1)/4+1=54，即54*54*96，参数数量为(11*11*1+1)*96=11712\n",
    "# 接下来输入通道为96，输出通道为256，核大小为5，填充为2，步幅为1，输出大小为\n",
    "# (54-5+2*2)/1+1=27，即27*27*256，参数数量为(5*5*96+1)*256=614656\n",
    "# 接下来输入通道为256，输出通道为384，核大小为3，填充为1，步幅为1，输出大小为\n",
    "# (27-3+2*1)/1+1=27，即27*27*384，参数数量为(3*3*256+1)*384=885120\n",
    "# 接下来输入通道为384，输出通道为384，核大小为3，填充为1，步幅为1，输出大小为\n",
    "# (27-3+2*1)/1+1=27，即27*27*384，参数数量为(3*3*384+1)*384=1327488\n",
    "# 接下来输入通道为384，输出通道为256，核大小为3，填充为1，步幅为1，输出大小为\n",
    "# (27-3+2*1)/1+1=27，即27*27*256，参数数量为(3*3*384+1)*256=884992\n",
    "# 卷积层参数数量为11712+614656+885120+1327488+884992=3712968\n",
    "# 卷积层能源消耗值为2.32957448e+01J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d97a07",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "[**我们构造一个**]高度和宽度都为224的(**单通道数据，来观察每一层输出的形状**)。\n",
    "它与 :numref:`fig_alexnet`中的AlexNet架构相匹配。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a7ec36",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 6400])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c79a7",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## 读取数据集\n",
    "\n",
    "尽管原文中AlexNet是在ImageNet上进行训练的，但本书在这里使用的是Fashion-MNIST数据集。因为即使在现代GPU上，训练ImageNet模型，同时使其收敛可能需要数小时或数天的时间。\n",
    "将AlexNet直接应用于Fashion-MNIST的一个问题是，[**Fashion-MNIST图像的分辨率**]（$28 \\times 28$像素）(**低于ImageNet图像。**)\n",
    "为了解决这个问题，(**我们将它们增加到$224 \\times 224$**)（通常来讲这不是一个明智的做法，但在这里这样做是为了有效使用AlexNet架构）。\n",
    "这里需要使用`d2l.load_data_fashion_mnist`函数中的`resize`参数执行此调整。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1552a8",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the train_iter is: (59,)\n",
      "the shape of the 0 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 1 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 2 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 3 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 4 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 5 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 6 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 7 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 8 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n",
      "the shape of the 9 batch of the train_iter is: torch.Size([1024, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "# print the shape of the train_iter\n",
    "list_of_i = []\n",
    "for i, (X, y) in enumerate(train_iter):\n",
    "    list_of_i.append(i)\n",
    "\n",
    "print('the shape of the train_iter is:', np.array(list_of_i).shape)\n",
    "# print(list_of_i)\n",
    "# print the first 10 batch of the train_iter\n",
    "for i, (X, y) in enumerate(train_iter):\n",
    "    if i < 10:\n",
    "        print('the shape of the', i, 'batch of the train_iter is:', X.shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484d7f3",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## [**训练AlexNet**]\n",
    "\n",
    "现在AlexNet可以开始被训练了。与 :numref:`sec_lenet`中的LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff5843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 定义一个forwar_hook函数，目标是在运行到该层的时候，进行显示\n",
    "# 更新：forward_hook函数的参数是固定的，不能随意更改，增加time模块不太好\n",
    "#     需要改为设定循环来一步步的进行计算\n",
    "# '''\n",
    "\n",
    "# def forward_hook(module, input, output):\n",
    "#     if isinstance(module, nn.Conv2d):\n",
    "#         print(\"正在运行卷积层!\")\n",
    "#     elif isinstance(module, nn.MaxPool2d):\n",
    "#         print(\"正在运行池化层!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843da99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\n",
    "\n",
    "    Defined in :numref:`sec_lenet`\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需的（之后将介绍）\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c78cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(data_iter, net, loss_fn, device):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    net.eval()\n",
    "    size = len(data_iter.dataset)\n",
    "    num_batches = len(data_iter)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            acc = d2l.accuracy(net(X), y)\n",
    "            size = d2l.size(y)\n",
    "    return acc/size\n",
    "    #         pred = net(X)\n",
    "    #         test_loss += loss_fn(pred, y).item()\n",
    "    #         correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d888ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 这段是将d2l中的train_ch6()函数拷贝过来，然后修改了一下，目的是为了能够显示在进行每一层的前向传播的时候，能够显示出来\n",
    "# '''\n",
    "# def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "#     \"\"\"用GPU训练模型(在第六章定义)\n",
    "\n",
    "#     Defined in :numref:`sec_lenet`\"\"\"\n",
    "#     def init_weights(m):\n",
    "#         if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "#     net.apply(init_weights)\n",
    "\n",
    "#     list_layer_name = ['Conv2d','ReLU', 'MaxPool2d','Linear','Dropout','Flatten']\n",
    "\n",
    "#     # create a numpy array to store the time and energy, the shape is (num_epochs, len(list_layer_name), 2)\n",
    "#     # the 2 means time and energy, respectively\n",
    "#     time_energy_data = np.zeros((num_epochs, len(list_layer_name), 2)) # for epoch = 1, the shape is (6,2)\n",
    "\n",
    "#     # create another numpy array to store the time and energy for loss, backward and optimizer\n",
    "#     # the shape is (num_epochs, 3)\n",
    "#     time_energy_data_loss_backward_optimizer = np.zeros((num_epochs, 3))\n",
    "\n",
    "#     print('training on', device)\n",
    "#     net.to(device)\n",
    "#     optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "#                             legend=['train loss', 'train acc', 'test acc'])\n",
    "#     timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # 训练损失之和，训练准确率之和，样本数\n",
    "#         metric = d2l.Accumulator(3)\n",
    "#         net.train() # 设置为训练模式\n",
    "\n",
    "#         # 将time_energy_data的第一个维度设置为epoch\n",
    "#         time_energy_data[epoch,:,:] = epoch\n",
    "\n",
    "#         # 将time_energy_data_backward_optimizer的第一个维度设置为epoch\n",
    "#         time_energy_data_loss_backward_optimizer[epoch,:] = epoch\n",
    "\n",
    "#         for i, (X, y) in enumerate(train_iter):\n",
    "#             timer.start()\n",
    "#             optimizer.zero_grad()\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "\n",
    "#             # 增加一个time_energy_data_i, 用于记录每一次前向传播的时间和能量\n",
    "#             time_energy_data_i = np.zeros((len(list_layer_name),2))\n",
    "\n",
    "#             # 增加一个time_energy_data_backward_optimizer_i, 用于记录每一次backward和optimizer的时间和能量\n",
    "#             time_energy_data_loss_backward_optimizer_i = np.zeros((3))\n",
    "\n",
    "#             # 将原本的y_hat = net(X)改为下面的形式，目的是为了让模型逐层运行，并且在这个过程中，记录起运行的时间以及消耗的能量\n",
    "#             y_hat = X\n",
    "#             for layer in net:\n",
    "                \n",
    "#                 # print(layer.__class__.__name__,'output shape:\\t',y_hat.shape)\n",
    "#                 layer_name = layer.__class__.__name__\n",
    "#                 # print(layer_name)\n",
    "#                 # # check if the layer_name is in the list_layer_name，if yes, then act the code below, else, continue\n",
    "#                 # if layer_name in list_layer_name:\n",
    "#                 # find out the layer name is in where of the list\n",
    "#                 layer_index = list_layer_name.index(layer_name)\n",
    "                \n",
    "#                 # calculate the energy and time\n",
    "#                 # 这里的时间和能量都是在每一层的前向传播的时候，进行计算的\n",
    "#                 time_start_layer = time.time()\n",
    "#                 '''\n",
    "#                 energy部分后续加入，先进行测试时间计算\n",
    "#                 '''\n",
    "#                 # energy_start = 0\n",
    "#                 y_hat = layer(y_hat)\n",
    "#                 time_end_layer = time.time()\n",
    "#                 time_cost_layer = time_end_layer - time_start_layer\n",
    "#                 # 将对应的layer_index的时间和能量加入到time_energy_data中\n",
    "#                 time_energy_data_i[layer_index,0] += time_cost_layer\n",
    "#                 # time_energy_data_i[layer_index,1] += energy_cost_layer\n",
    "\n",
    "#                 # print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "#                 # print(time_energy_data_i)\n",
    "            \n",
    "#             # 将time_energy_data_i加入到time_energy_data中\n",
    "#             time_energy_data[epoch,:,:] += time_energy_data_i\n",
    "\n",
    "#             # y_hat = net(X)\n",
    "\n",
    "#             # loss部分\n",
    "#             time_start_loss = time.time()\n",
    "#             l = loss(y_hat, y)\n",
    "#             time_end_loss = time.time()\n",
    "#             time_cost_loss = time_end_loss - time_start_loss\n",
    "#             # 对time_cost_loss数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[2] += time_cost_loss\n",
    "\n",
    "#             # backward部分\n",
    "#             time_start_backward = time.time()\n",
    "#             l.backward()\n",
    "#             time_end_backward = time.time()\n",
    "#             time_cost_backward = time_end_backward - time_start_backward\n",
    "#             # 对time_cost_backward数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[0] += time_cost_backward\n",
    "\n",
    "#             # optimizer部分\n",
    "#             time_start_optimizer = time.time()\n",
    "#             optimizer.step()\n",
    "#             time_end_optimizer = time.time()\n",
    "#             time_cost_optimizer = time_end_optimizer - time_start_optimizer\n",
    "#             # 对time_cost_optimizer数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[1] += time_cost_optimizer\n",
    "\n",
    "#             # 将time_energy_data_backward_optimizer_i加入到time_energy_data_backward_optimizer中\n",
    "#             time_energy_data_loss_backward_optimizer[epoch,:] += time_energy_data_loss_backward_optimizer_i\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "#             timer.stop()\n",
    "#             train_l = metric[0] / metric[2]\n",
    "#             train_acc = metric[1] / metric[2]\n",
    "#             if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "#                 animator.add(epoch + (i + 1) / num_batches,\n",
    "#                              (train_l, train_acc, None))\n",
    "#         test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "#         animator.add(epoch + 1, (None, None, test_acc))\n",
    "\n",
    "#         # 输出一下每种层的训练时间\n",
    "#         print('epoch %d, time %f sec' % (epoch, timer.sum()))\n",
    "#         for j in range(len(list_layer_name)):\n",
    "#             print(list_layer_name[j], 'time %f sec' % (time_energy_data[epoch,j,0]))\n",
    "\n",
    "#         # 输出一下backward和optimizer的时间\n",
    "#         print('backward time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,0]))\n",
    "#         print('optimizer time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,1]))\n",
    "#         print('loss time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,2]))\n",
    "#     print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "#           f'test acc {test_acc:.3f}')\n",
    "#     print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "#           f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56310da0",
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# lr, num_epochs = 0.01, 1\n",
    "# train_ch6(net, train_iter, test_iter, num_epochs, lr, 'mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee0bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 这段是将d2l中的train_ch6()函数拷贝过来，然后修改了一下，目的是为了能够显示在进行每一层的前向传播的时候，能够显示出来\n",
    "# 修改了其中的metric部分，对该部分进行了注释，而是调用了简单的optimizer.no_grad()\n",
    "# '''\n",
    "# lr, num_epochs = 0.01, 1\n",
    "# def init_weights(m):\n",
    "#     if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "#         nn.init.xavier_uniform_(m.weight)\n",
    "# net.apply(init_weights)\n",
    "\n",
    "# list_layer_name = ['Conv2d','ReLU', 'MaxPool2d','Linear','Dropout','Flatten']\n",
    "\n",
    "# # create a numpy array to store the time and energy, the shape is (num_epochs, len(list_layer_name), 2)\n",
    "# # the 2 means time and energy, respectively\n",
    "# time_energy_data = np.zeros((num_epochs, len(list_layer_name), 2)) # for epoch = 1, the shape is (6,2)\n",
    "\n",
    "# # create another numpy array to store the time and energy for loss, backward and optimizer\n",
    "# # the shape is (num_epochs, 3)\n",
    "# time_energy_data_loss_backward_optimizer = np.zeros((num_epochs, 3))\n",
    "\n",
    "# time_cost_other = 0\n",
    "\n",
    "# time_avg_cost = 0\n",
    "\n",
    "# device = 'mps'\n",
    "# print('training on', 'mps')\n",
    "# net.to('mps')\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "# animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "#                         legend=['train loss', 'train acc', 'test acc'])\n",
    "# timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "# for epoch in range(num_epochs):\n",
    "#     timer.start() # 开始计时\n",
    "#     # 训练损失之和，训练准确率之和，样本数\n",
    "#     metric = d2l.Accumulator(3)\n",
    "#     net.train() # 设置为训练模式\n",
    "\n",
    "#     # 将time_energy_data的第一个维度设置为epoch\n",
    "#     time_energy_data[epoch,:,:] = epoch\n",
    "\n",
    "#     # 将time_energy_data_backward_optimizer的第一个维度设置为epoch\n",
    "#     time_energy_data_loss_backward_optimizer[epoch,:] = epoch\n",
    "\n",
    "#     # initialize a time_to_device_cost to store the time cost of the device\n",
    "#     time_to_device_cost = 0\n",
    "\n",
    "\n",
    "#     for i, (X, y) in enumerate(train_iter):\n",
    "\n",
    "#         if i < 500:\n",
    "\n",
    "#             time_round = time.time()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             time_to_device = time.time()\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             time_to_device_end = time.time()\n",
    "#             time_to_device_cost_i = time_to_device_end - time_to_device\n",
    "#             time_to_device_cost += time_to_device_cost_i\n",
    "\n",
    "#             # 增加一个time_energy_data_i, 用于记录每一次前向传播的时间和能量\n",
    "#             time_energy_data_i = np.zeros((len(list_layer_name),2))\n",
    "\n",
    "#             # 增加一个time_energy_data_backward_optimizer_i, 用于记录每一次backward和optimizer的时间和能量\n",
    "#             time_energy_data_loss_backward_optimizer_i = np.zeros((3))\n",
    "\n",
    "#             time_cost_other_i = 0\n",
    "\n",
    "#             time_avg_cost_i = 0\n",
    "\n",
    "#             # 将原本的y_hat = net(X)改为下面的形式，目的是为了让模型逐层运行，并且在这个过程中，记录起运行的时间以及消耗的能量\n",
    "#             y_hat = X\n",
    "#             for layer in net:\n",
    "                \n",
    "#                 # print(layer.__class__.__name__,'output shape:\\t',y_hat.shape)\n",
    "#                 layer_name = layer.__class__.__name__\n",
    "#                 # print(layer_name)\n",
    "#                 # # check if the layer_name is in the list_layer_name，if yes, then act the code below, else, continue\n",
    "#                 # if layer_name in list_layer_name:\n",
    "#                 # find out the layer name is in where of the list\n",
    "#                 layer_index = list_layer_name.index(layer_name)\n",
    "                \n",
    "#                 # calculate the energy and time\n",
    "#                 # 这里的时间和能量都是在每一层的前向传播的时候，进行计算的\n",
    "#                 time_start_layer = time.time()\n",
    "#                 '''\n",
    "#                 energy部分后续加入，先进行测试时间计算\n",
    "#                 '''\n",
    "#                 # energy_start = 0\n",
    "#                 y_hat = layer(y_hat)\n",
    "#                 time_end_layer = time.time()\n",
    "#                 time_cost_layer = time_end_layer - time_start_layer\n",
    "\n",
    "#                 # print the result\n",
    "#                 print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "#                 print('*'*50)\n",
    "\n",
    "\n",
    "#                 # 将对应的layer_index的时间和能量加入到time_energy_data中\n",
    "#                 time_energy_data_i[layer_index,0] += time_cost_layer\n",
    "#                 # time_energy_data_i[layer_index,1] += energy_cost_layer\n",
    "\n",
    "#                 # print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "#                 # print(time_energy_data_i)\n",
    "            \n",
    "#             # 将time_energy_data_i加入到time_energy_data中\n",
    "#             time_energy_data[epoch,:,:] += time_energy_data_i\n",
    "\n",
    "#             # y_hat = net(X)\n",
    "\n",
    "#             # loss部分\n",
    "#             time_start_loss = time.time()\n",
    "#             l = loss(y_hat, y)\n",
    "#             time_end_loss = time.time()\n",
    "#             time_cost_loss = time_end_loss - time_start_loss\n",
    "#             # 对time_cost_loss数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[0] += time_cost_loss\n",
    "\n",
    "#             # backward部分\n",
    "#             time_start_backward = time.time()\n",
    "#             l.backward()\n",
    "#             time_end_backward = time.time()\n",
    "#             time_cost_backward = time_end_backward - time_start_backward\n",
    "#             # 对time_cost_backward数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[1] += time_cost_backward\n",
    "\n",
    "#             # optimizer部分\n",
    "#             time_start_optimizer = time.time()\n",
    "#             optimizer.step()\n",
    "#             time_end_optimizer = time.time()\n",
    "#             time_cost_optimizer = time_end_optimizer - time_start_optimizer\n",
    "#             # 对time_cost_optimizer数据进行累加\n",
    "#             time_energy_data_loss_backward_optimizer_i[2] += time_cost_optimizer\n",
    "\n",
    "#             # 将time_energy_data_backward_optimizer_i加入到time_energy_data_backward_optimizer中\n",
    "#             time_energy_data_loss_backward_optimizer[epoch,:] += time_energy_data_loss_backward_optimizer_i\n",
    "            \n",
    "\n",
    "#             '''\n",
    "#             该部分是optimizer.zero_grad()部分，并且计算相对应的时间\n",
    "#             不计算梯度：torch.no_grad()上下文管理器确保不计算任何梯度。这实际上减少了计算和内存需求，因为它不会保留计算图中的中间状态。从这个角度看，使用torch.no_grad()会节省能源。\n",
    "#             理论上应该节省能源，但是在计算过程中，该部分消耗的时间是最长的\n",
    "#             为了进行测试，将李沐原本的torch.no_grad()进行替换，更换为了optimizer.zero_grad()\n",
    "#             '''\n",
    "#             # time about torch.no_grad()\n",
    "#             time_other_start = time.time()\n",
    "#             optimizer.zero_grad()\n",
    "#             # with torch.no_grad():\n",
    "#             #     metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "#             time_other_end = time.time()\n",
    "\n",
    "\n",
    "#             '''\n",
    "#             该部分是计算train_l和train_acc的部分，以及其运行过程所需时间的代码\n",
    "#             '''\n",
    "#             # time about avg the train_l and train_acc\n",
    "#             # time_avg_start = time.time()\n",
    "#             # train_l = metric[0] / metric[2]\n",
    "#             # train_acc = metric[1] / metric[2]\n",
    "#             # time_avg_end = time.time()\n",
    "#             # time_avg_cost_i = time_avg_end - time_avg_start\n",
    "#             # time_avg_cost += time_avg_cost_i\n",
    "\n",
    "#             # if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "#             #     animator.add(epoch + (i + 1) / num_batches,\n",
    "#             #                     (train_l, train_acc, None))\n",
    "#             # time_other_end = time.time()\n",
    "#             time_cost_other_i = time_other_end - time_other_start\n",
    "#             print('other time %f sec' % (time_cost_other_i))\n",
    "#             time_cost_other += time_cost_other_i\n",
    "\n",
    "#             time_end_round = time.time()\n",
    "#             time_cost_round = time_end_round - time_round\n",
    "#             print('round %d, time %f sec' % (i, time_cost_round))\n",
    "        \n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "#     time_test_acc_start = time.time()\n",
    "#     test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "#     time_test_acc_end = time.time()\n",
    "#     time_test_acc_cost = time_test_acc_end - time_test_acc_start\n",
    "#     # animator.add(epoch + 1, (None, None, test_acc))\n",
    "#     timer.stop() # 停止计时 \n",
    "    \n",
    "#     print('epoch %d, time %f sec' % (epoch, timer.sum()))\n",
    "#     # 输出一下每种层的训练时间\n",
    "#     for j in range(len(list_layer_name)):\n",
    "#         print(list_layer_name[j], 'time %f sec' % (time_energy_data[epoch,j,0]))\n",
    "    \n",
    "#     # 输出一下device的时间\n",
    "#     print('device time %f sec' % (time_to_device_cost))\n",
    "\n",
    "#     # 输出一下backward和optimizer的时间\n",
    "#     print('backward time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,1]))\n",
    "#     print('optimizer time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,2]))\n",
    "#     print('loss time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,0]))\n",
    "# print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "#         f'test acc {test_acc:.3f}')\n",
    "# print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "#         f'on {str(device)}')\n",
    "\n",
    "# print(time_energy_data)\n",
    "# print(time_energy_data_loss_backward_optimizer)\n",
    "# print('other time %f sec' % (time_cost_other))\n",
    "# print('time to device %f sec' % (time_to_device_cost))\n",
    "# # print('time avg %f sec' % (time_avg_cost))\n",
    "# print('time test acc %f sec' % (time_test_acc_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e649e",
   "metadata": {},
   "source": [
    "## 修改代码——Test1\n",
    "\n",
    "该段代码作为这个jupyter文件的最终运行部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516c82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_powermetrics(file_path):\n",
    "    \"\"\"\n",
    "    Run powermetrics and retrieve the output.\n",
    "\n",
    "    :param interval: Sampling interval in milliseconds.\n",
    "    :param count: Number of samples to retrieve.s\n",
    "    :return: The output from powermetrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the command as a list of arguments\n",
    "    cmd = [\"sudo\", \"powermetrics\",  \"-i\", \"1000\", \"--samplers\", \"cpu_power,gpu_power\", \"-a\", \"1\", \"-o\", file_path]\n",
    "    \n",
    "    process = subprocess.Popen(cmd)\n",
    "    return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89638b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_data_process(file_path):\n",
    "    \"\"\"\n",
    "    Read the output file of powermetric and extract the power value\n",
    "\n",
    "    :param file_path: The path of the output file of powermetric.\n",
    "    :return: The list of power values.\n",
    "    \"\"\"\n",
    "\n",
    "    list_power = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'Combined Power' in line:\n",
    "                power_value = line.split(':')[1].strip()\n",
    "                print(power_value)\n",
    "\n",
    "                # Remove the unit\n",
    "                power_value = power_value.replace('mW', '')\n",
    "\n",
    "                # Convert to integer\n",
    "                power_value = int(power_value)\n",
    "                list_power.append(power_value)\n",
    "\n",
    "    print(list_power)\n",
    "    print(len(list_power))\n",
    "\n",
    "    # do the data process\n",
    "    '''\n",
    "    The data from list_power is the Conbined Power of each second.\n",
    "    The data is the Power of each second.\n",
    "    we need to calculate the energy consumption of the whole process.\n",
    "    and need to change the J to kWh.\n",
    "    '''\n",
    "    # calculate the energy consumption\n",
    "    energy_consumption = 0\n",
    "    for i in range(len(list_power)):\n",
    "       energy_consumption += list_power[i]\n",
    "    print(energy_consumption)\n",
    "\n",
    "    # change the mW to W\n",
    "    energy_consumption = energy_consumption / 1000\n",
    "\n",
    "    # calculate the energy consumption, the interval is 1 second, and the energy unit is J\n",
    "    energy_consumption = energy_consumption * 1\n",
    "    \n",
    "    # change the J to kWh\n",
    "    energy_consumption = energy_consumption / 3600000\n",
    "    print(energy_consumption)\n",
    "    \n",
    "    return energy_consumption, list_power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41bfa77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_file = 'energy.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad309dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on mps\n",
      "round 0\n",
      "time to device 0.067137 sec\n",
      "sum_time_cost_round_layer 1.299237 sec\n",
      "loss time 0.387078 sec\n",
      "backward time 1.630220 sec\n",
      "optimizer time 2.484222 sec\n",
      "round 0, time 5.889112 sec\n",
      "the calculation result for the round is:  5.867894411087036\n",
      "round 1\n",
      "time to device 0.602859 sec\n",
      "sum_time_cost_round_layer 0.108898 sec\n",
      "loss time 0.019182 sec\n",
      "backward time 0.061019 sec\n",
      "optimizer time 0.021397 sec\n",
      "round 1, time 0.821778 sec\n",
      "the calculation result for the round is:  0.8133552074432373\n",
      "round 2\n",
      "time to device 5.068534 sec\n",
      "sum_time_cost_round_layer 0.074110 sec\n",
      "loss time 0.010640 sec\n",
      "backward time 0.052828 sec\n",
      "optimizer time 0.013126 sec\n",
      "round 2, time 5.228176 sec\n",
      "the calculation result for the round is:  5.219238042831421\n",
      "round 3\n",
      "time to device 5.186257 sec\n",
      "sum_time_cost_round_layer 0.054960 sec\n",
      "loss time 0.012394 sec\n",
      "backward time 0.030633 sec\n",
      "optimizer time 0.015964 sec\n",
      "round 3, time 5.313737 sec\n",
      "the calculation result for the round is:  5.3002073764801025\n",
      "round 4\n",
      "time to device 5.132742 sec\n",
      "sum_time_cost_round_layer 0.082159 sec\n",
      "loss time 0.010746 sec\n",
      "backward time 0.043281 sec\n",
      "optimizer time 0.023281 sec\n",
      "round 4, time 5.308584 sec\n",
      "the calculation result for the round is:  5.292208909988403\n",
      "round 5\n",
      "time to device 5.023215 sec\n",
      "sum_time_cost_round_layer 0.032286 sec\n",
      "loss time 0.003516 sec\n",
      "backward time 0.029674 sec\n",
      "optimizer time 0.012764 sec\n",
      "round 5, time 5.107502 sec\n",
      "the calculation result for the round is:  5.101455211639404\n",
      "round 6\n",
      "time to device 5.083824 sec\n",
      "sum_time_cost_round_layer 0.025319 sec\n",
      "loss time 0.002038 sec\n",
      "backward time 0.016626 sec\n",
      "optimizer time 0.009086 sec\n",
      "round 6, time 5.144691 sec\n",
      "the calculation result for the round is:  5.136893033981323\n",
      "round 7\n",
      "time to device 5.322844 sec\n",
      "sum_time_cost_round_layer 0.068839 sec\n",
      "loss time 0.009488 sec\n",
      "backward time 0.046327 sec\n",
      "optimizer time 0.020449 sec\n",
      "round 7, time 5.481171 sec\n",
      "the calculation result for the round is:  5.467946529388428\n",
      "round 8\n",
      "time to device 5.172109 sec\n",
      "sum_time_cost_round_layer 0.073676 sec\n",
      "loss time 0.009824 sec\n",
      "backward time 0.031142 sec\n",
      "optimizer time 0.013362 sec\n",
      "round 8, time 5.316163 sec\n",
      "the calculation result for the round is:  5.300113201141357\n",
      "round 9\n",
      "time to device 5.470334 sec\n",
      "sum_time_cost_round_layer 0.066189 sec\n",
      "loss time 0.017653 sec\n",
      "backward time 0.054631 sec\n",
      "optimizer time 0.019283 sec\n",
      "round 9, time 5.642371 sec\n",
      "the calculation result for the round is:  5.628090143203735\n",
      "round 10\n",
      "time to device 5.198884 sec\n",
      "sum_time_cost_round_layer 0.031775 sec\n",
      "loss time 0.003566 sec\n",
      "backward time 0.023277 sec\n",
      "optimizer time 0.009549 sec\n",
      "round 10, time 5.277411 sec\n",
      "the calculation result for the round is:  5.267051696777344\n",
      "round 11\n",
      "time to device 5.414330 sec\n",
      "sum_time_cost_round_layer 0.027770 sec\n",
      "loss time 0.003529 sec\n",
      "backward time 0.023099 sec\n",
      "optimizer time 0.010523 sec\n",
      "round 11, time 5.485802 sec\n",
      "the calculation result for the round is:  5.47925066947937\n",
      "round 12\n",
      "time to device 5.718500 sec\n",
      "sum_time_cost_round_layer 0.139768 sec\n",
      "loss time 0.007490 sec\n",
      "backward time 0.032528 sec\n",
      "optimizer time 0.013378 sec\n",
      "round 12, time 5.962465 sec\n",
      "the calculation result for the round is:  5.911664247512817\n",
      "round 13\n",
      "time to device 5.256921 sec\n",
      "sum_time_cost_round_layer 0.139016 sec\n",
      "loss time 0.009078 sec\n",
      "backward time 0.030801 sec\n",
      "optimizer time 0.019216 sec\n",
      "round 13, time 5.478484 sec\n",
      "the calculation result for the round is:  5.455031871795654\n",
      "round 14\n",
      "time to device 5.198423 sec\n",
      "sum_time_cost_round_layer 0.110006 sec\n",
      "loss time 0.026087 sec\n",
      "backward time 0.209804 sec\n",
      "optimizer time 0.034251 sec\n",
      "round 14, time 5.603862 sec\n",
      "the calculation result for the round is:  5.578571319580078\n",
      "round 15\n",
      "time to device 5.398718 sec\n",
      "sum_time_cost_round_layer 0.107059 sec\n",
      "loss time 0.032995 sec\n",
      "backward time 0.129398 sec\n",
      "optimizer time 0.032584 sec\n",
      "round 15, time 5.737829 sec\n",
      "the calculation result for the round is:  5.700753688812256\n",
      "round 16\n",
      "time to device 4.938270 sec\n",
      "sum_time_cost_round_layer 0.028901 sec\n",
      "loss time 0.003868 sec\n",
      "backward time 0.023837 sec\n",
      "optimizer time 0.010754 sec\n",
      "round 16, time 5.010194 sec\n",
      "the calculation result for the round is:  5.0056304931640625\n",
      "round 17\n",
      "time to device 5.085201 sec\n",
      "sum_time_cost_round_layer 0.023813 sec\n",
      "loss time 0.002341 sec\n",
      "backward time 0.021433 sec\n",
      "optimizer time 0.006900 sec\n",
      "round 17, time 5.146635 sec\n",
      "the calculation result for the round is:  5.139687538146973\n",
      "round 18\n",
      "time to device 5.445379 sec\n",
      "sum_time_cost_round_layer 0.052090 sec\n",
      "loss time 0.007080 sec\n",
      "backward time 0.042371 sec\n",
      "optimizer time 0.017409 sec\n",
      "round 18, time 5.575063 sec\n",
      "the calculation result for the round is:  5.564329624176025\n",
      "round 19\n",
      "time to device 5.091699 sec\n",
      "sum_time_cost_round_layer 0.079559 sec\n",
      "loss time 0.010578 sec\n",
      "backward time 0.035971 sec\n",
      "optimizer time 0.014495 sec\n",
      "round 19, time 5.246453 sec\n",
      "the calculation result for the round is:  5.232301950454712\n",
      "round 20\n",
      "time to device 5.250236 sec\n",
      "sum_time_cost_round_layer 0.062198 sec\n",
      "loss time 0.008056 sec\n",
      "backward time 0.049344 sec\n",
      "optimizer time 0.016827 sec\n",
      "round 20, time 5.403514 sec\n",
      "the calculation result for the round is:  5.386661052703857\n",
      "round 21\n",
      "time to device 5.193447 sec\n",
      "sum_time_cost_round_layer 0.035420 sec\n",
      "loss time 0.003692 sec\n",
      "backward time 0.019705 sec\n",
      "optimizer time 0.006585 sec\n",
      "round 21, time 5.264008 sec\n",
      "the calculation result for the round is:  5.258849143981934\n",
      "round 22\n",
      "time to device 5.053596 sec\n",
      "sum_time_cost_round_layer 0.028805 sec\n",
      "loss time 0.004128 sec\n",
      "backward time 0.025859 sec\n",
      "optimizer time 0.012357 sec\n",
      "round 22, time 5.131155 sec\n",
      "the calculation result for the round is:  5.124744892120361\n",
      "round 23\n",
      "time to device 5.028637 sec\n",
      "sum_time_cost_round_layer 0.033125 sec\n",
      "loss time 0.002646 sec\n",
      "backward time 0.022665 sec\n",
      "optimizer time 0.010962 sec\n",
      "round 23, time 5.107473 sec\n",
      "the calculation result for the round is:  5.09803581237793\n",
      "round 24\n",
      "time to device 5.144409 sec\n",
      "sum_time_cost_round_layer 0.067876 sec\n",
      "loss time 0.011154 sec\n",
      "backward time 0.031730 sec\n",
      "optimizer time 0.014785 sec\n",
      "round 24, time 5.277872 sec\n",
      "the calculation result for the round is:  5.269953727722168\n",
      "round 25\n",
      "time to device 5.161548 sec\n",
      "sum_time_cost_round_layer 0.060783 sec\n",
      "loss time 0.006837 sec\n",
      "backward time 0.029427 sec\n",
      "optimizer time 0.017660 sec\n",
      "round 25, time 5.285539 sec\n",
      "the calculation result for the round is:  5.276254653930664\n",
      "round 26\n",
      "time to device 5.136693 sec\n",
      "sum_time_cost_round_layer 0.071775 sec\n",
      "loss time 0.013222 sec\n",
      "backward time 0.044717 sec\n",
      "optimizer time 0.022338 sec\n",
      "round 26, time 5.305724 sec\n",
      "the calculation result for the round is:  5.288744926452637\n",
      "round 27\n",
      "time to device 6.229564 sec\n",
      "sum_time_cost_round_layer 0.080427 sec\n",
      "loss time 0.011803 sec\n",
      "backward time 0.165149 sec\n",
      "optimizer time 0.056678 sec\n",
      "round 27, time 6.562932 sec\n",
      "the calculation result for the round is:  6.543621301651001\n",
      "round 28\n",
      "time to device 6.413839 sec\n",
      "sum_time_cost_round_layer 0.123077 sec\n",
      "loss time 0.012703 sec\n",
      "backward time 0.036158 sec\n",
      "optimizer time 0.020142 sec\n",
      "round 28, time 6.642128 sec\n",
      "the calculation result for the round is:  6.605919361114502\n",
      "round 29\n",
      "time to device 5.201908 sec\n",
      "sum_time_cost_round_layer 0.172160 sec\n",
      "loss time 0.011484 sec\n",
      "backward time 0.031286 sec\n",
      "optimizer time 0.009275 sec\n",
      "round 29, time 5.441394 sec\n",
      "the calculation result for the round is:  5.4261133670806885\n",
      "round 30\n",
      "time to device 5.356862 sec\n",
      "sum_time_cost_round_layer 0.170439 sec\n",
      "loss time 0.007566 sec\n",
      "backward time 0.033073 sec\n",
      "optimizer time 0.021911 sec\n",
      "round 30, time 5.609547 sec\n",
      "the calculation result for the round is:  5.589851140975952\n",
      "round 31\n",
      "time to device 5.242436 sec\n",
      "sum_time_cost_round_layer 0.079476 sec\n",
      "loss time 0.017560 sec\n",
      "backward time 0.042938 sec\n",
      "optimizer time 0.021870 sec\n",
      "round 31, time 5.424836 sec\n",
      "the calculation result for the round is:  5.404279947280884\n",
      "round 32\n",
      "time to device 5.494958 sec\n",
      "sum_time_cost_round_layer 0.064001 sec\n",
      "loss time 0.009088 sec\n",
      "backward time 0.042326 sec\n",
      "optimizer time 0.019932 sec\n",
      "round 32, time 5.643800 sec\n",
      "the calculation result for the round is:  5.630305290222168\n",
      "round 33\n",
      "time to device 5.483921 sec\n",
      "sum_time_cost_round_layer 0.065163 sec\n",
      "loss time 0.009872 sec\n",
      "backward time 0.050162 sec\n",
      "optimizer time 0.021978 sec\n",
      "round 33, time 5.645421 sec\n",
      "the calculation result for the round is:  5.6310954093933105\n",
      "round 34\n",
      "time to device 5.677413 sec\n",
      "sum_time_cost_round_layer 0.061598 sec\n",
      "loss time 0.011757 sec\n",
      "backward time 0.133732 sec\n",
      "optimizer time 0.014325 sec\n",
      "round 34, time 5.912369 sec\n",
      "the calculation result for the round is:  5.898825407028198\n",
      "round 35\n",
      "time to device 5.466606 sec\n",
      "sum_time_cost_round_layer 0.535679 sec\n",
      "loss time 0.066694 sec\n",
      "backward time 0.120300 sec\n",
      "optimizer time 0.077643 sec\n",
      "round 35, time 6.358173 sec\n",
      "the calculation result for the round is:  6.266921281814575\n",
      "round 36\n",
      "time to device 4.988074 sec\n",
      "sum_time_cost_round_layer 0.118108 sec\n",
      "loss time 0.017073 sec\n",
      "backward time 0.032507 sec\n",
      "optimizer time 0.011982 sec\n",
      "round 36, time 5.191309 sec\n",
      "the calculation result for the round is:  5.167744159698486\n",
      "round 37\n",
      "time to device 5.556446 sec\n",
      "sum_time_cost_round_layer 0.138798 sec\n",
      "loss time 0.012483 sec\n",
      "backward time 0.025307 sec\n",
      "optimizer time 0.009308 sec\n",
      "round 37, time 5.762515 sec\n",
      "the calculation result for the round is:  5.742341756820679\n",
      "round 38\n",
      "time to device 5.321888 sec\n",
      "sum_time_cost_round_layer 0.100330 sec\n",
      "loss time 0.013365 sec\n",
      "backward time 0.029050 sec\n",
      "optimizer time 0.010171 sec\n",
      "round 38, time 5.527932 sec\n",
      "the calculation result for the round is:  5.474804162979126\n",
      "round 39\n",
      "time to device 5.416896 sec\n",
      "sum_time_cost_round_layer 0.132584 sec\n",
      "loss time 0.013882 sec\n",
      "backward time 0.044487 sec\n",
      "optimizer time 0.018390 sec\n",
      "round 39, time 5.671388 sec\n",
      "the calculation result for the round is:  5.626238822937012\n",
      "round 40\n",
      "time to device 5.496622 sec\n",
      "sum_time_cost_round_layer 0.123160 sec\n",
      "loss time 0.013568 sec\n",
      "backward time 0.132024 sec\n",
      "optimizer time 0.036551 sec\n",
      "round 40, time 5.833638 sec\n",
      "the calculation result for the round is:  5.801925420761108\n",
      "round 41\n",
      "time to device 7.635302 sec\n",
      "sum_time_cost_round_layer 0.083092 sec\n",
      "loss time 0.013784 sec\n",
      "backward time 0.060537 sec\n",
      "optimizer time 0.038370 sec\n",
      "round 41, time 7.856726 sec\n",
      "the calculation result for the round is:  7.831086158752441\n",
      "round 42\n",
      "time to device 7.055654 sec\n",
      "sum_time_cost_round_layer 0.109796 sec\n",
      "loss time 0.011182 sec\n",
      "backward time 0.042196 sec\n",
      "optimizer time 0.105225 sec\n",
      "round 42, time 7.346763 sec\n",
      "the calculation result for the round is:  7.324052810668945\n",
      "round 43\n",
      "time to device 5.656108 sec\n",
      "sum_time_cost_round_layer 0.121135 sec\n",
      "loss time 0.010548 sec\n",
      "backward time 0.025123 sec\n",
      "optimizer time 0.014071 sec\n",
      "round 43, time 5.893129 sec\n",
      "the calculation result for the round is:  5.826985597610474\n",
      "round 44\n",
      "time to device 5.296538 sec\n",
      "sum_time_cost_round_layer 0.066207 sec\n",
      "loss time 0.010537 sec\n",
      "backward time 0.028427 sec\n",
      "optimizer time 0.008722 sec\n",
      "round 44, time 5.447810 sec\n",
      "the calculation result for the round is:  5.410431385040283\n",
      "round 45\n",
      "time to device 5.055930 sec\n",
      "sum_time_cost_round_layer 0.078651 sec\n",
      "loss time 0.017074 sec\n",
      "backward time 0.020008 sec\n",
      "optimizer time 0.017776 sec\n",
      "round 45, time 5.218697 sec\n",
      "the calculation result for the round is:  5.189438581466675\n",
      "round 46\n",
      "time to device 5.125518 sec\n",
      "sum_time_cost_round_layer 0.131987 sec\n",
      "loss time 0.007884 sec\n",
      "backward time 0.027633 sec\n",
      "optimizer time 0.009407 sec\n",
      "round 46, time 5.321224 sec\n",
      "the calculation result for the round is:  5.3024280071258545\n",
      "round 47\n",
      "time to device 5.161608 sec\n",
      "sum_time_cost_round_layer 0.118568 sec\n",
      "loss time 0.010169 sec\n",
      "backward time 0.039681 sec\n",
      "optimizer time 0.020167 sec\n",
      "round 47, time 5.376007 sec\n",
      "the calculation result for the round is:  5.350193500518799\n",
      "round 48\n",
      "time to device 5.249829 sec\n",
      "sum_time_cost_round_layer 0.173598 sec\n",
      "loss time 0.013099 sec\n",
      "backward time 0.037544 sec\n",
      "optimizer time 0.012541 sec\n",
      "round 48, time 5.516626 sec\n",
      "the calculation result for the round is:  5.486609935760498\n",
      "round 49\n",
      "time to device 5.387688 sec\n",
      "sum_time_cost_round_layer 0.136937 sec\n",
      "loss time 0.011154 sec\n",
      "backward time 0.037611 sec\n",
      "optimizer time 0.018777 sec\n",
      "round 49, time 5.662835 sec\n",
      "the calculation result for the round is:  5.592167377471924\n",
      "round 50\n",
      "time to device 5.173948 sec\n",
      "sum_time_cost_round_layer 0.067991 sec\n",
      "loss time 0.011198 sec\n",
      "backward time 0.028590 sec\n",
      "optimizer time 0.019940 sec\n",
      "round 50, time 5.317841 sec\n",
      "the calculation result for the round is:  5.301666975021362\n",
      "round 51\n",
      "time to device 5.114372 sec\n",
      "sum_time_cost_round_layer 0.101219 sec\n",
      "loss time 0.008034 sec\n",
      "backward time 0.045803 sec\n",
      "optimizer time 0.011763 sec\n",
      "round 51, time 5.301062 sec\n",
      "the calculation result for the round is:  5.281190872192383\n",
      "round 52\n",
      "time to device 5.042713 sec\n",
      "sum_time_cost_round_layer 0.080493 sec\n",
      "loss time 0.007472 sec\n",
      "backward time 0.029274 sec\n",
      "optimizer time 0.009227 sec\n",
      "round 52, time 5.185265 sec\n",
      "the calculation result for the round is:  5.169179201126099\n",
      "round 53\n",
      "time to device 5.186942 sec\n",
      "sum_time_cost_round_layer 0.179229 sec\n",
      "loss time 0.016655 sec\n",
      "backward time 0.071797 sec\n",
      "optimizer time 0.026317 sec\n",
      "round 53, time 5.533789 sec\n",
      "the calculation result for the round is:  5.480940103530884\n",
      "round 54\n",
      "time to device 5.134290 sec\n",
      "sum_time_cost_round_layer 0.189907 sec\n",
      "loss time 0.014745 sec\n",
      "backward time 0.073127 sec\n",
      "optimizer time 0.034437 sec\n",
      "round 54, time 5.485074 sec\n",
      "the calculation result for the round is:  5.446504831314087\n",
      "round 55\n",
      "time to device 4.912830 sec\n",
      "sum_time_cost_round_layer 0.099991 sec\n",
      "loss time 0.012436 sec\n",
      "backward time 0.046315 sec\n",
      "optimizer time 0.042923 sec\n",
      "round 55, time 5.139399 sec\n",
      "the calculation result for the round is:  5.114495277404785\n",
      "round 56\n",
      "time to device 5.053684 sec\n",
      "sum_time_cost_round_layer 0.070629 sec\n",
      "loss time 0.012902 sec\n",
      "backward time 0.025004 sec\n",
      "optimizer time 0.008185 sec\n",
      "round 56, time 5.242287 sec\n",
      "the calculation result for the round is:  5.170403242111206\n",
      "round 57\n",
      "time to device 5.093074 sec\n",
      "sum_time_cost_round_layer 0.067978 sec\n",
      "loss time 0.006424 sec\n",
      "backward time 0.028246 sec\n",
      "optimizer time 0.015621 sec\n",
      "round 57, time 5.229557 sec\n",
      "the calculation result for the round is:  5.21134352684021\n",
      "round 58\n",
      "time to device 5.003008 sec\n",
      "sum_time_cost_round_layer 1.171340 sec\n",
      "loss time 0.156288 sec\n",
      "backward time 1.264482 sec\n",
      "optimizer time 0.009441 sec\n",
      "round 58, time 7.621375 sec\n",
      "the calculation result for the round is:  7.604558706283569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtjgp/miniconda3/envs/d2l/lib/python3.8/site-packages/d2l/torch.py:3507: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, time 346.381335 sec\n",
      "Conv2d time 3.747996 sec\n",
      "ReLU time 1.117456 sec\n",
      "MaxPool2d time 0.819395 sec\n",
      "Linear time 1.053016 sec\n",
      "Dropout time 1.127495 sec\n",
      "Flatten time 0.133803 sec\n",
      "time to device 305.531216 sec\n",
      "[[[3.74799562 0.        ]\n",
      "  [1.11745596 0.        ]\n",
      "  [0.8193953  0.        ]\n",
      "  [1.05301571 0.        ]\n",
      "  [1.12749529 0.        ]\n",
      "  [0.13380313 0.        ]]]\n",
      "loss time 1.219389 sec\n",
      "backward time 5.642242 sec\n",
      "optimizer time 3.676573 sec\n",
      "time test acc 17.777669 sec\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "这段是将d2l中的train_ch6()函数拷贝过来，然后修改了一下，目的是为了能够显示在进行每一层的前向传播的时候，能够显示出来\n",
    "修改了其中的metric部分，对该部分进行了注释，而是调用了简单的optimizer.no_grad()\n",
    "得调整一下整体的代码，需要对循环内部的东西全部重写一下，然后再进行测试\n",
    "'''\n",
    "lr, num_epochs = 0.01, 1 # learning rate and number of epochs\n",
    "def init_weights(m): # 初始化权重\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "net.apply(init_weights)\n",
    "\n",
    "list_layer_name = ['Conv2d','ReLU', 'MaxPool2d','Linear','Dropout','Flatten'] # 该模型中包括的所有的层的名字\n",
    "\n",
    "# create a numpy array to store the time and energy, the shape is (num_epochs, len(list_layer_name), 2)\n",
    "# the 2 means time and energy, respectively\n",
    "# 目标是将每次循环过程中，在运行每一个层的时候的所需时间全部记录下来（第一步测试）\n",
    "# 如果测试成功，则对后续的能耗进行测试\n",
    "time_energy_data = np.zeros((num_epochs, len(list_layer_name), 2)) # for epoch = 1, the shape is (6,2)，第一列是时间，第二列是能耗\n",
    "\n",
    "# create another numpy array to store the time and energy for loss, backward and optimizer\n",
    "# the shape is (num_epochs, 3)\n",
    "# # 增加这个的目的是为了记录loss，backward和optimizer的时间，看看整体计算的过程中是否有问题\n",
    "# time_energy_data_loss_backward_optimizer = np.zeros((num_epochs, 3))\n",
    "'''将loss, backward and optimizer的时间改为单独计算，先进行测试，看看结果如何'''\n",
    "time_cost_loss = 0\n",
    "time_cost_backward = 0\n",
    "time_cost_optimizer = 0\n",
    "\n",
    "# initialize a time_to_device_cost to store the time cost of the device\n",
    "time_to_device_cost = 0\n",
    "\n",
    "# 记录其他部分的时间，先加上，看看结果\n",
    "# time_cost_other = 0\n",
    "\n",
    "# 记录计算平均loss和平均准确度的时间，目前暂时不需要\n",
    "# time_avg_cost = 0\n",
    "\n",
    "# 记录计算test_acc的时间\n",
    "time_test_acc_cost = 0\n",
    "\n",
    "# 设定运行的设备\n",
    "device = 'mps'\n",
    "print('training on', 'mps')\n",
    "net.to('mps') # 将模型放到对应的设备上\n",
    "\n",
    "# 初始化optimizer和loss\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 对动画部分先进行注释，暂时先不对该部分进行考虑\n",
    "# animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "#                         legend=['train loss', 'train acc', 'test acc'])\n",
    "\n",
    "# 初始化计时器\n",
    "timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    timer.start() # 开始计时\n",
    "    # run powermetrics\n",
    "    powermetrics_process = run_powermetrics(energy_file)\n",
    "    # 训练损失之和，训练准确率之和，样本数\n",
    "    # metric = d2l.Accumulator(3)\n",
    "    net.train() # 设置为训练模式\n",
    "\n",
    "    # 将time_energy_data的第一个维度设置为epoch\n",
    "    time_energy_data[epoch,:,:] = epoch\n",
    "    # 将time_energy_data_backward_optimizer的第一个维度设置为epoch\n",
    "    # time_energy_data_loss_backward_optimizer[epoch,:] = epoch\n",
    "    # 增加一个time_energy_data_backward_optimizer_i, 用于记录每一次backward和optimizer的时间和能量\n",
    "    # time_energy_data_loss_backward_optimizer_i = np.zeros((3))\n",
    "\n",
    "    for i, (X, y) in enumerate(train_iter): # 每个循环结束的时候有个100s的睡眠时间，用于较长时间显示一下每一轮的数据\n",
    "        '''每一轮的运行结束之后，都会有一个100s的睡眠时间，用于显示一下每一轮的数据\n",
    "        数据包括：\n",
    "        1. 运行该轮的时间\n",
    "        2. 在该轮运行中，每一层的运行时间\n",
    "        3. 在该轮运行中，loss，backward和optimizer的运行时间\n",
    "        4. 在该轮运行中，将数据放到对应的设备上的时间'''\n",
    "        if i < 500:\n",
    "            # 显示是第几轮\n",
    "            print('round %d' % (i))\n",
    "            time_round = time.time() # 计算每一轮的时间\n",
    "            optimizer.zero_grad() # 将optimizer的梯度清零\n",
    "\n",
    "            # 增加一个time_energy_data_i, 用于记录每一次前向传播的时间和能量\n",
    "            time_energy_data_i = np.zeros((len(list_layer_name),2))\n",
    "            sum_time_cost_round = 0 # 用于记录每一轮的时间\n",
    "\n",
    "            # 计算将数据放到对应的设备上的时间\n",
    "            time_to_device_cost_i = 0\n",
    "            time_to_device = time.time()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            time_to_device_end = time.time()\n",
    "            time_to_device_cost_i = time_to_device_end - time_to_device\n",
    "            print('time to device %f sec' % (time_to_device_cost_i))\n",
    "            time_to_device_cost += time_to_device_cost_i\n",
    "\n",
    "            # initialize a time_cost_other_i to store the time cost of the other part\n",
    "            time_cost_other_i = 0\n",
    "            # initialize a time_avg_cost_i to store the time cost of the avg part\n",
    "            # time_avg_cost_i = 0\n",
    "\n",
    "            # 将原本的y_hat = net(X)改为下面的形式，目的是为了让模型逐层运行，并且在这个过程中，记录起运行的时间以及消耗的能量\n",
    "            y_hat = X\n",
    "            for layer in net:\n",
    "                # 每次循环开始的时候进行初始化，防止后面计算过程中出现错误\n",
    "                time_cost_layer = 0\n",
    "                # print(layer.__class__.__name__,'output shape:\\t',y_hat.shape)\n",
    "                layer_name = layer.__class__.__name__\n",
    "                # print(layer_name)\n",
    "                # check if the layer_name is in the list_layer_name，if yes, then act the code below, else, continue\n",
    "                # if layer_name in list_layer_name:\n",
    "                # find out the layer name is in where of the list\n",
    "                layer_index = list_layer_name.index(layer_name)\n",
    "                \n",
    "                # calculate the energy and time of each layer\n",
    "                # 这里的时间和能量都是在每一层的前向传播的时候，进行计算的\n",
    "                time_start_layer = time.time()\n",
    "                '''energy部分后续加入，先进行测试时间计算'''\n",
    "                # energy_start = 0\n",
    "                y_hat = layer(y_hat)\n",
    "                time_end_layer = time.time()\n",
    "                time_cost_layer = time_end_layer - time_start_layer\n",
    "                # print the result\n",
    "                # print(layer_name, 'time %f sec' % (time_cost_layer))\n",
    "                # print('*'*50)\n",
    "                # 将对应的layer_index的时间和能量加入到time_energy_data中\n",
    "                time_energy_data_i[layer_index,0] += time_cost_layer\n",
    "                # time_energy_data_i[layer_index,1] += energy_cost_layer\n",
    "                # print(time_energy_data_i)\n",
    "\n",
    "                # 加入一个sleep，用于测试\n",
    "                # time.sleep(2)\n",
    "\n",
    "            # 将time_energy_data_i加入到time_energy_data中\n",
    "            time_energy_data[epoch,:,:] += time_energy_data_i\n",
    "            # 求time_energy_data_i的和，用于计算每一轮的时间\n",
    "            sum_time_cost_round_layer = np.sum(time_energy_data_i[:,0])\n",
    "            print('sum_time_cost_round_layer %f sec' % (sum_time_cost_round_layer))\n",
    "            # 显示一下time_energy_data\n",
    "            # print(time_energy_data[epoch,:,:])\n",
    "            # print(\"-\"*50)\n",
    "            # time.sleep(3)\n",
    "\n",
    "            # loss部分\n",
    "            time_cost_loss_i = 0 # 初始化loss的时间\n",
    "            time_start_loss = time.time()\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            time_end_loss = time.time()\n",
    "            time_cost_loss_i = time_end_loss - time_start_loss\n",
    "            print('loss time %f sec' % (time_cost_loss_i))\n",
    "            time_cost_loss += time_cost_loss_i\n",
    "            # 对time_cost_loss数据进行累加\n",
    "            # time_energy_data_loss_backward_optimizer_i[0] += time_cost_loss\n",
    "\n",
    "            # backward部分\n",
    "            time_cost_backward_i = 0 # 初始化backward的时间\n",
    "            time_start_backward = time.time()\n",
    "            loss.backward()\n",
    "            time_end_backward = time.time()\n",
    "            time_cost_backward_i = time_end_backward - time_start_backward\n",
    "            print('backward time %f sec' % (time_cost_backward_i))\n",
    "            time_cost_backward += time_cost_backward_i\n",
    "            # 对time_cost_backward数据进行累加\n",
    "            # time_energy_data_loss_backward_optimizer_i[1] += time_cost_backward\n",
    "\n",
    "            # optimizer部分\n",
    "            time_cost_optimizer_i = 0 # 初始化optimizer的时间\n",
    "            time_start_optimizer = time.time()\n",
    "            optimizer.step()\n",
    "            time_end_optimizer = time.time()\n",
    "            time_cost_optimizer_i = time_end_optimizer - time_start_optimizer\n",
    "            print('optimizer time %f sec' % (time_cost_optimizer_i))\n",
    "            time_cost_optimizer += time_cost_optimizer_i\n",
    "            # 对time_cost_optimizer数据进行累加\n",
    "            # time_energy_data_loss_backward_optimizer_i[2] += time_cost_optimizer\n",
    "\n",
    "            # 将time_energy_data_backward_optimizer_i加入到time_energy_data_backward_optimizer中\n",
    "            # time_energy_data_loss_backward_optimizer[epoch,:] += time_energy_data_loss_backward_optimizer_i\n",
    "            \n",
    "            '''\n",
    "            该部分是optimizer.zero_grad()部分，并且计算相对应的时间\n",
    "            不计算梯度：torch.no_grad()上下文管理器确保不计算任何梯度。这实际上减少了计算和内存需求，因为它不会保留计算图中的中间状态。从这个角度看，使用torch.no_grad()会节省能源。\n",
    "            理论上应该节省能源，但是在计算过程中，该部分消耗的时间是最长的\n",
    "            为了进行测试，将李沐原本的torch.no_grad()进行替换，更换为了optimizer.zero_grad()\n",
    "            更新10.17：删除掉该部分的optimizer.zero_grad()，因为在循环开始的部分已经申明了\n",
    "            '''\n",
    "            # other time about torch.no_grad()\n",
    "            # time_cost_other_i = 0 # 初始化other的时间\n",
    "            # time about torch.no_grad()\n",
    "            # time_other_start = time.time()\n",
    "            # with torch.no_grad():\n",
    "            #     metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            # if i % 100 == 0:\n",
    "            #     loss, current = l.item(), (i + 1) * len(X)\n",
    "            #     print(f\"loss: {loss:>7f}  [{i:>5d}/{num_batches:>5d}]\")\n",
    "            '''该部分是计算train_l和train_acc的部分，以及其运行过程所需时间的代码'''\n",
    "            # time about avg the train_l and train_acc\n",
    "            # time_avg_start = time.time()\n",
    "            # train_l = metric[0] / metric[2]\n",
    "            # train_acc = metric[1] / metric[2]\n",
    "            # time_avg_end = time.time()\n",
    "            # time_avg_cost_i = time_avg_end - time_avg_start\n",
    "            # time_avg_cost += time_avg_cost_i\n",
    "            # if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "            #     animator.add(epoch + (i + 1) / num_batches,\n",
    "            #                     (train_l, train_acc, None))\n",
    "            # time_other_end = time.time()\n",
    "            # time_cost_other_i = time_other_end - time_other_start\n",
    "            # print('other time %f sec' % (time_cost_other_i))\n",
    "            # time_cost_other += time_cost_other_i\n",
    "\n",
    "            time_end_round = time.time()\n",
    "            time_cost_round = time_end_round - time_round\n",
    "            print('round %d, time %f sec' % (i, time_cost_round))\n",
    "            sum_time_cost_round = time_to_device_cost_i + time_cost_loss_i + time_cost_backward_i + time_cost_optimizer_i\n",
    "            sum_time_cost_round += sum_time_cost_round_layer\n",
    "            print('the calculation result for the round is: ', sum_time_cost_round)\n",
    "\n",
    "            # 设定一个time.sleep, 用于测试\n",
    "            # time.sleep(10)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    time_test_acc_cost_epoch = 0\n",
    "    time_test_acc_start = time.time()\n",
    "    # test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "    test_round = test_loop(test_iter, net, loss_fn, device)\n",
    "    time_test_acc_end = time.time()\n",
    "    time_test_acc_cost_epoch = time_test_acc_end - time_test_acc_start\n",
    "    time_test_acc_cost += time_test_acc_cost_epoch\n",
    "    # animator.add(epoch + 1, (None, None, test_acc))\n",
    "\n",
    "    powermetrics_process.terminate()\n",
    "    powermetrics_process.wait()\n",
    "    \n",
    "    timer.stop() # 停止计时 \n",
    "    \n",
    "    print('epoch %d, time %f sec' % (epoch, timer.sum()))\n",
    "    # 输出一下每种层的训练时间\n",
    "    for j in range(len(list_layer_name)):\n",
    "        print(list_layer_name[j], 'time %f sec' % (time_energy_data[epoch,j,0]))\n",
    "    \n",
    "    # # 输出一下device的时间\n",
    "    # print('device time %f sec' % (time_to_device_cost))\n",
    "\n",
    "    # 输出一下backward和optimizer的时间\n",
    "    # print('backward time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,1]))\n",
    "    # print('optimizer time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,2]))\n",
    "    # print('loss time %f sec' % (time_energy_data_loss_backward_optimizer[epoch,0]))\n",
    "# print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "#         f'test acc {test_acc:.3f}')\n",
    "# print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "#         f'on {str(device)}')\n",
    "\n",
    "# print('other time %f sec' % (time_cost_other))\n",
    "print('time to device %f sec' % (time_to_device_cost))\n",
    "print(time_energy_data)\n",
    "# print(time_energy_data_loss_backward_optimizer)\n",
    "print('loss time %f sec' % (time_cost_loss))\n",
    "print('backward time %f sec' % (time_cost_backward))\n",
    "print('optimizer time %f sec' % (time_cost_optimizer))\n",
    "# print('time avg %f sec' % (time_avg_cost))\n",
    "print('time test acc %f sec' % (time_test_acc_cost))\n",
    "# 显示总体运行时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8117a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum of time_energy_data is 7.999161 sec\n",
      "the sum of total is 341.846251 sec\n"
     ]
    }
   ],
   "source": [
    "time_energy_data_sum = np.sum(time_energy_data[:,:,0])\n",
    "print('the sum of time_energy_data is %f sec' % (time_energy_data_sum))\n",
    "\n",
    "sum_total = time_to_device_cost + time_energy_data_sum + time_cost_loss + time_cost_backward + time_cost_optimizer + time_test_acc_cost\n",
    "print('the sum of total is %f sec' % (sum_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "817e0c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9670 mW\n",
      "12007 mW\n",
      "15242 mW\n",
      "15576 mW\n",
      "19591 mW\n",
      "10163 mW\n",
      "11380 mW\n",
      "12847 mW\n",
      "10473 mW\n",
      "12881 mW\n",
      "9389 mW\n",
      "9096 mW\n",
      "14508 mW\n",
      "11387 mW\n",
      "14304 mW\n",
      "11328 mW\n",
      "10337 mW\n",
      "12018 mW\n",
      "10727 mW\n",
      "10497 mW\n",
      "12279 mW\n",
      "10278 mW\n",
      "9824 mW\n",
      "10739 mW\n",
      "10328 mW\n",
      "11370 mW\n",
      "9837 mW\n",
      "10997 mW\n",
      "13318 mW\n",
      "11028 mW\n",
      "12560 mW\n",
      "10132 mW\n",
      "9271 mW\n",
      "12634 mW\n",
      "12527 mW\n",
      "12747 mW\n",
      "9520 mW\n",
      "10068 mW\n",
      "14554 mW\n",
      "10675 mW\n",
      "11090 mW\n",
      "11868 mW\n",
      "10763 mW\n",
      "12453 mW\n",
      "13735 mW\n",
      "11284 mW\n",
      "14508 mW\n",
      "10996 mW\n",
      "8891 mW\n",
      "15881 mW\n",
      "10070 mW\n",
      "11598 mW\n",
      "11394 mW\n",
      "11638 mW\n",
      "11434 mW\n",
      "11410 mW\n",
      "11173 mW\n",
      "14180 mW\n",
      "12309 mW\n",
      "9170 mW\n",
      "17409 mW\n",
      "12978 mW\n",
      "14261 mW\n",
      "14657 mW\n",
      "12190 mW\n",
      "16050 mW\n",
      "13465 mW\n",
      "11444 mW\n",
      "15046 mW\n",
      "13965 mW\n",
      "12403 mW\n",
      "15209 mW\n",
      "11351 mW\n",
      "16031 mW\n",
      "14551 mW\n",
      "11495 mW\n",
      "11430 mW\n",
      "16182 mW\n",
      "13438 mW\n",
      "13965 mW\n",
      "10842 mW\n",
      "8850 mW\n",
      "12772 mW\n",
      "10437 mW\n",
      "11809 mW\n",
      "11382 mW\n",
      "10478 mW\n",
      "12108 mW\n",
      "10423 mW\n",
      "10780 mW\n",
      "11880 mW\n",
      "10999 mW\n",
      "10203 mW\n",
      "12087 mW\n",
      "11726 mW\n",
      "12991 mW\n",
      "12528 mW\n",
      "11556 mW\n",
      "13593 mW\n",
      "10357 mW\n",
      "12063 mW\n",
      "12324 mW\n",
      "11049 mW\n",
      "17566 mW\n",
      "11868 mW\n",
      "11587 mW\n",
      "13146 mW\n",
      "10615 mW\n",
      "9925 mW\n",
      "11098 mW\n",
      "10638 mW\n",
      "12125 mW\n",
      "10066 mW\n",
      "11239 mW\n",
      "12485 mW\n",
      "10693 mW\n",
      "12021 mW\n",
      "10230 mW\n",
      "10836 mW\n",
      "11611 mW\n",
      "11456 mW\n",
      "13909 mW\n",
      "10259 mW\n",
      "9698 mW\n",
      "12846 mW\n",
      "10380 mW\n",
      "11657 mW\n",
      "10932 mW\n",
      "8657 mW\n",
      "14938 mW\n",
      "10461 mW\n",
      "11712 mW\n",
      "11905 mW\n",
      "9763 mW\n",
      "13372 mW\n",
      "12590 mW\n",
      "12159 mW\n",
      "14400 mW\n",
      "10429 mW\n",
      "11264 mW\n",
      "12055 mW\n",
      "14257 mW\n",
      "17687 mW\n",
      "17082 mW\n",
      "16742 mW\n",
      "7995 mW\n",
      "12155 mW\n",
      "16585 mW\n",
      "13452 mW\n",
      "17145 mW\n",
      "15863 mW\n",
      "15960 mW\n",
      "15081 mW\n",
      "15044 mW\n",
      "12015 mW\n",
      "14369 mW\n",
      "12972 mW\n",
      "12480 mW\n",
      "16282 mW\n",
      "11745 mW\n",
      "14764 mW\n",
      "13213 mW\n",
      "16436 mW\n",
      "13210 mW\n",
      "11807 mW\n",
      "10853 mW\n",
      "15420 mW\n",
      "11210 mW\n",
      "10419 mW\n",
      "16069 mW\n",
      "11821 mW\n",
      "12164 mW\n",
      "12299 mW\n",
      "11440 mW\n",
      "11872 mW\n",
      "12849 mW\n",
      "11088 mW\n",
      "14581 mW\n",
      "11406 mW\n",
      "10530 mW\n",
      "15632 mW\n",
      "12372 mW\n",
      "10874 mW\n",
      "16492 mW\n",
      "11202 mW\n",
      "11287 mW\n",
      "13779 mW\n",
      "11713 mW\n",
      "13680 mW\n",
      "14813 mW\n",
      "14961 mW\n",
      "7599 mW\n",
      "15409 mW\n",
      "13637 mW\n",
      "15613 mW\n",
      "14427 mW\n",
      "13335 mW\n",
      "15150 mW\n",
      "13446 mW\n",
      "12141 mW\n",
      "14823 mW\n",
      "12780 mW\n",
      "14140 mW\n",
      "15794 mW\n",
      "11731 mW\n",
      "12628 mW\n",
      "15215 mW\n",
      "13926 mW\n",
      "14123 mW\n",
      "12693 mW\n",
      "11274 mW\n",
      "15641 mW\n",
      "10833 mW\n",
      "11423 mW\n",
      "17238 mW\n",
      "11937 mW\n",
      "11812 mW\n",
      "14089 mW\n",
      "14359 mW\n",
      "11479 mW\n",
      "11126 mW\n",
      "15579 mW\n",
      "15008 mW\n",
      "15343 mW\n",
      "16823 mW\n",
      "9093 mW\n",
      "15223 mW\n",
      "9245 mW\n",
      "6339 mW\n",
      "15136 mW\n",
      "14401 mW\n",
      "15901 mW\n",
      "15539 mW\n",
      "15165 mW\n",
      "4687 mW\n",
      "13706 mW\n",
      "13072 mW\n",
      "11751 mW\n",
      "16218 mW\n",
      "11833 mW\n",
      "11574 mW\n",
      "16273 mW\n",
      "12267 mW\n",
      "14000 mW\n",
      "13531 mW\n",
      "10018 mW\n",
      "14404 mW\n",
      "12615 mW\n",
      "12801 mW\n",
      "14106 mW\n",
      "12868 mW\n",
      "15616 mW\n",
      "14004 mW\n",
      "12046 mW\n",
      "15608 mW\n",
      "12350 mW\n",
      "11898 mW\n",
      "13109 mW\n",
      "14033 mW\n",
      "15965 mW\n",
      "13411 mW\n",
      "13837 mW\n",
      "16574 mW\n",
      "13896 mW\n",
      "16251 mW\n",
      "15037 mW\n",
      "14325 mW\n",
      "13555 mW\n",
      "13044 mW\n",
      "11208 mW\n",
      "16080 mW\n",
      "13117 mW\n",
      "9323 mW\n",
      "14933 mW\n",
      "11069 mW\n",
      "14733 mW\n",
      "14034 mW\n",
      "13496 mW\n",
      "11862 mW\n",
      "11683 mW\n",
      "16212 mW\n",
      "15686 mW\n",
      "12533 mW\n",
      "13171 mW\n",
      "11512 mW\n",
      "10688 mW\n",
      "14674 mW\n",
      "11851 mW\n",
      "11626 mW\n",
      "15172 mW\n",
      "11272 mW\n",
      "13062 mW\n",
      "11488 mW\n",
      "10553 mW\n",
      "14666 mW\n",
      "11457 mW\n",
      "12351 mW\n",
      "13322 mW\n",
      "13269 mW\n",
      "14892 mW\n",
      "13659 mW\n",
      "11387 mW\n",
      "14813 mW\n",
      "11845 mW\n",
      "10708 mW\n",
      "12882 mW\n",
      "12829 mW\n",
      "14048 mW\n",
      "11218 mW\n",
      "10140 mW\n",
      "16726 mW\n",
      "13997 mW\n",
      "14061 mW\n",
      "12907 mW\n",
      "10992 mW\n",
      "13693 mW\n",
      "10790 mW\n",
      "12814 mW\n",
      "12428 mW\n",
      "11161 mW\n",
      "11202 mW\n",
      "8626 mW\n",
      "15896 mW\n",
      "17569 mW\n",
      "11521 mW\n",
      "9150 mW\n",
      "11138 mW\n",
      "11540 mW\n",
      "9176 mW\n",
      "11556 mW\n",
      "10793 mW\n",
      "8274 mW\n",
      "11551 mW\n",
      "9544 mW\n",
      "8471 mW\n",
      "14470 mW\n",
      "12037 mW\n",
      "9645 mW\n",
      "9202 mW\n",
      "13016 mW\n",
      "[9670, 12007, 15242, 15576, 19591, 10163, 11380, 12847, 10473, 12881, 9389, 9096, 14508, 11387, 14304, 11328, 10337, 12018, 10727, 10497, 12279, 10278, 9824, 10739, 10328, 11370, 9837, 10997, 13318, 11028, 12560, 10132, 9271, 12634, 12527, 12747, 9520, 10068, 14554, 10675, 11090, 11868, 10763, 12453, 13735, 11284, 14508, 10996, 8891, 15881, 10070, 11598, 11394, 11638, 11434, 11410, 11173, 14180, 12309, 9170, 17409, 12978, 14261, 14657, 12190, 16050, 13465, 11444, 15046, 13965, 12403, 15209, 11351, 16031, 14551, 11495, 11430, 16182, 13438, 13965, 10842, 8850, 12772, 10437, 11809, 11382, 10478, 12108, 10423, 10780, 11880, 10999, 10203, 12087, 11726, 12991, 12528, 11556, 13593, 10357, 12063, 12324, 11049, 17566, 11868, 11587, 13146, 10615, 9925, 11098, 10638, 12125, 10066, 11239, 12485, 10693, 12021, 10230, 10836, 11611, 11456, 13909, 10259, 9698, 12846, 10380, 11657, 10932, 8657, 14938, 10461, 11712, 11905, 9763, 13372, 12590, 12159, 14400, 10429, 11264, 12055, 14257, 17687, 17082, 16742, 7995, 12155, 16585, 13452, 17145, 15863, 15960, 15081, 15044, 12015, 14369, 12972, 12480, 16282, 11745, 14764, 13213, 16436, 13210, 11807, 10853, 15420, 11210, 10419, 16069, 11821, 12164, 12299, 11440, 11872, 12849, 11088, 14581, 11406, 10530, 15632, 12372, 10874, 16492, 11202, 11287, 13779, 11713, 13680, 14813, 14961, 7599, 15409, 13637, 15613, 14427, 13335, 15150, 13446, 12141, 14823, 12780, 14140, 15794, 11731, 12628, 15215, 13926, 14123, 12693, 11274, 15641, 10833, 11423, 17238, 11937, 11812, 14089, 14359, 11479, 11126, 15579, 15008, 15343, 16823, 9093, 15223, 9245, 6339, 15136, 14401, 15901, 15539, 15165, 4687, 13706, 13072, 11751, 16218, 11833, 11574, 16273, 12267, 14000, 13531, 10018, 14404, 12615, 12801, 14106, 12868, 15616, 14004, 12046, 15608, 12350, 11898, 13109, 14033, 15965, 13411, 13837, 16574, 13896, 16251, 15037, 14325, 13555, 13044, 11208, 16080, 13117, 9323, 14933, 11069, 14733, 14034, 13496, 11862, 11683, 16212, 15686, 12533, 13171, 11512, 10688, 14674, 11851, 11626, 15172, 11272, 13062, 11488, 10553, 14666, 11457, 12351, 13322, 13269, 14892, 13659, 11387, 14813, 11845, 10708, 12882, 12829, 14048, 11218, 10140, 16726, 13997, 14061, 12907, 10992, 13693, 10790, 12814, 12428, 11161, 11202, 8626, 15896, 17569, 11521, 9150, 11138, 11540, 9176, 11556, 10793, 8274, 11551, 9544, 8471, 14470, 12037, 9645, 9202, 13016]\n",
      "340\n",
      "4289836\n",
      "0.0011916211111111111\n",
      "the total training time is 340 sec\n"
     ]
    }
   ],
   "source": [
    "energy_consumption, power_list_model = txt_data_process(energy_file)\n",
    "total_training_time = len(power_list_model)\n",
    "print('the total training time is %d sec' % (total_training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94764ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4289.836"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# power_list_model\n",
    "energy_consumption_J = energy_consumption * 3600000\n",
    "energy_consumption_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cbd6b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.617164705882354"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_energy_cost_persec = energy_consumption_J / total_training_time\n",
    "avg_energy_cost_persec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d317a463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.74799562, 0.        ],\n",
       "        [1.11745596, 0.        ],\n",
       "        [0.8193953 , 0.        ],\n",
       "        [1.05301571, 0.        ],\n",
       "        [1.12749529, 0.        ],\n",
       "        [0.13380313, 0.        ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change time_energy_data to np array\n",
    "time_energy_data_np = np.array(time_energy_data)\n",
    "time_energy_data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a734c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abfbd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.74799562 47.28907799]\n",
      "  [ 1.11745596 14.09912589]\n",
      "  [ 0.8193953  10.33844551]\n",
      "  [ 1.05301571 13.28607264]\n",
      "  [ 1.12749529 14.22579376]\n",
      "  [ 0.13380313  1.68821612]]]\n"
     ]
    }
   ],
   "source": [
    "# calculate the energy consumption of each type of layer\n",
    "for i in range(len(time_energy_data_np[epoch,:,0])):\n",
    "    time_energy_data_np[epoch, i, 1] = time_energy_data_np[epoch, i, 0] * avg_energy_cost_persec\n",
    "\n",
    "print(time_energy_data_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b1c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the energy consumption of loss and backward and optimizer\n",
    "loss_energy_consumption = time_cost_loss * avg_energy_cost_persec\n",
    "backward_energy_consumption = time_cost_backward * avg_energy_cost_persec\n",
    "optimizer_energy_consumption = time_cost_optimizer * avg_energy_cost_persec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1876aca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "# x axis is the list_layer_name and the \n",
    "x = list_layer_name\n",
    "# print(x)\n",
    "# y axis\n",
    "y = time_energy_data_np[epoch,:,1]\n",
    "# print(y)\n",
    "# plot the bar chart using different colors\n",
    "plt.figure(figsize=(20,10))\n",
    "# plt.bar(x, y, label='energy consumption of each layer')\n",
    "plt.bar(x, y, label='energy consumption of each layer', color=['b','g','r','c','m','y'])\n",
    "plt.xlabel('layer name')\n",
    "plt.ylabel('energy consumption (J)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52c308cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total energy consumption of forward is 100.926732 J\n"
     ]
    }
   ],
   "source": [
    "# calculate the total sumation of the energy consumption\n",
    "total_energy_consumption_forward = np.sum(time_energy_data_np[epoch,:,1])\n",
    "print('the total energy consumption of forward is %f J' % (total_energy_consumption_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "074342c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.385237387797414\n",
      "71.18910207052792\n",
      "46.38792757492907\n",
      "100.92673190921055\n"
     ]
    }
   ],
   "source": [
    "print(loss_energy_consumption)\n",
    "print(backward_energy_consumption)\n",
    "print(optimizer_energy_consumption)\n",
    "print(total_energy_consumption_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c99d47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results of forward, loss, backward and optimizer\n",
    "x = ['forward', 'loss', 'backward', 'optimizer']\n",
    "y = [total_energy_consumption_forward, loss_energy_consumption, backward_energy_consumption, optimizer_energy_consumption]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x, y, label='energy consumption of each part', color=['b','g','r','c'])\n",
    "plt.xlabel('part name')\n",
    "plt.ylabel('energy consumption (J)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c8094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
