{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is to get the labeled_energy_data_layer csv file of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numba as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def interpolate_point(times, powers, target_time):\n",
    "    n = len(times)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    if target_time <= times[0]:\n",
    "        return powers[0]\n",
    "    if target_time >= times[-1]:\n",
    "        return powers[-1]\n",
    "    \n",
    "    # Binary search\n",
    "    left, right = 0, n-1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if times[mid] == target_time:\n",
    "            return powers[mid]\n",
    "        elif times[mid] < target_time:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "            \n",
    "    # Linear interpolation\n",
    "    pos = left\n",
    "    t1, p1 = times[pos-1], powers[pos-1]\n",
    "    t2, p2 = times[pos], powers[pos]\n",
    "    ratio = (target_time - t1) / (t2 - t1)\n",
    "    return p1 + (p2 - p1) * ratio\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def integrate_power_over_interval(samples, start_time, end_time):\n",
    "    times = samples[:, 0]\n",
    "    powers = samples[:, 1]\n",
    "    \n",
    "    # Get start and end powers through interpolation\n",
    "    start_power = interpolate_point(times, powers, start_time)\n",
    "    end_power = interpolate_point(times, powers, end_time)\n",
    "    \n",
    "    # Filter points within interval\n",
    "    mask = (times >= start_time) & (times <= end_time)\n",
    "    interval_times = times[mask]\n",
    "    interval_powers = powers[mask]\n",
    "    \n",
    "    # Create array including boundary points\n",
    "    n_points = len(interval_times)\n",
    "    full_times = np.zeros(n_points + 2)\n",
    "    full_powers = np.zeros(n_points + 2)\n",
    "    \n",
    "    # Add boundary points\n",
    "    full_times[0] = start_time\n",
    "    full_powers[0] = start_power\n",
    "    full_times[-1] = end_time\n",
    "    full_powers[-1] = end_power\n",
    "    \n",
    "    # Add interior points\n",
    "    if n_points > 0:\n",
    "        full_times[1:-1] = interval_times\n",
    "        full_powers[1:-1] = interval_powers\n",
    "    \n",
    "    # Integration using trapezoidal rule\n",
    "    total_energy = 0.0\n",
    "    for i in range(len(full_times)-1):\n",
    "        dt = full_times[i+1] - full_times[i]\n",
    "        avg_p = (full_powers[i] + full_powers[i+1]) / 2.0\n",
    "        total_energy += avg_p * dt\n",
    "        \n",
    "    return total_energy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label energy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_energy_consumption(energy_data, to_device, forward, loss, backward, optimize):\n",
    "    # Create a copy of the energy_data dataframe to avoid modifying the original\n",
    "    labeled_energy_data = energy_data.copy()\n",
    "    \n",
    "    # Initialize a new column for the step labels\n",
    "    labeled_energy_data['step'] = 'idle'\n",
    "    \n",
    "    # Define a helper function to label the steps\n",
    "    def label_steps(energy_data, step_energy, step_name):\n",
    "        for epoch in range(step_energy.shape[0]):\n",
    "            for batch in range(step_energy.shape[1]):\n",
    "                start_time = step_energy[epoch][batch][0]\n",
    "                end_time = step_energy[epoch][batch][1]\n",
    "                mask = (energy_data['timestamp'] >= start_time) & (energy_data['timestamp'] <= end_time)\n",
    "                labeled_energy_data.loc[mask, 'step'] = step_name\n",
    "    \n",
    "    # Label each step\n",
    "    label_steps(labeled_energy_data, to_device, 'to_device')\n",
    "    label_steps(labeled_energy_data, forward, 'forward')\n",
    "    label_steps(labeled_energy_data, loss, 'loss')\n",
    "    label_steps(labeled_energy_data, backward, 'backward')\n",
    "    label_steps(labeled_energy_data, optimize, 'optimize')\n",
    "    \n",
    "    return labeled_energy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_energy_consumption_layer(energy_data, to_device, loss, backward, optimize, layer_energy):\n",
    "    # Create a copy of the energy_data dataframe to avoid modifying the original\n",
    "    labeled_energy_data_layer = energy_data.copy()\n",
    "    \n",
    "    # Initialize a new column for the step labels\n",
    "    labeled_energy_data_layer['step'] = 'idle'\n",
    "    \n",
    "    # Define a helper function to label the steps\n",
    "    def label_steps(energy_data, step_energy, step_name):\n",
    "        for epoch in range(step_energy.shape[0]):\n",
    "            for batch in range(step_energy.shape[1]):\n",
    "                start_time = step_energy[epoch][batch][0]\n",
    "                end_time = step_energy[epoch][batch][1]\n",
    "                # print(type(start_time))\n",
    "                # print(type(energy_data['timestamp']))\n",
    "                mask = (energy_data['timestamp'] >= start_time) & (energy_data['timestamp'] <= end_time)\n",
    "                labeled_energy_data_layer.loc[mask, 'step'] = step_name\n",
    "\n",
    "    # define a helper function to label the layers\n",
    "    def label_layers(energy_data, layer_energy):\n",
    "        for i in range(layer_energy.shape[1]):\n",
    "            for j in range(layer_energy.shape[0]):\n",
    "                # each row in layer_energy is a batch, and in each batch is a dictionary with the layer names and the corresponding time\n",
    "                layer_batch = layer_energy.iloc[j][str(i)]\n",
    "                # transfer the layer_batch from string to dictionary\n",
    "                layer_batch = eval(layer_batch)\n",
    "                # iterate through the dictionary to get the start and end time of each layer\n",
    "                for layer, time_period in layer_batch.items():\n",
    "                    start_time = time_period[0]\n",
    "                    end_time = time_period[1]\n",
    "                    mask = (energy_data['timestamp'] >= start_time) & (energy_data['timestamp'] <= end_time)\n",
    "                    labeled_energy_data_layer.loc[mask, 'step'] = layer\n",
    "    \n",
    "    # Label each step\n",
    "    label_steps(labeled_energy_data_layer, to_device, 'to_device')\n",
    "    label_layers(labeled_energy_data_layer, layer_energy)\n",
    "    label_steps(labeled_energy_data_layer, loss, 'loss')\n",
    "    label_steps(labeled_energy_data_layer, backward, 'backward')\n",
    "    label_steps(labeled_energy_data_layer, optimize, 'optimize')\n",
    "    \n",
    "    return labeled_energy_data_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the label function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E5_B128_R0_SR2_performance_140', 'E5_B128_R0_SR2_performance_310', 'E5_B128_R0_SR2_performance_170', 'E5_B128_R0_SR2_performance_320', 'E5_B128_R0_SR2_performance_290', 'E5_B128_R0_SR2_performance_230', 'E5_B128_R0_SR2_performance_200', 'fashion_mnist', 'E5_B128_R0_SR2_performance_240', 'E5_B128_R0_SR2_performance_270', 'E5_B128_R0_SR2_performance_130', 'E5_B128_R0_SR2_performance_190', 'E5_B128_R0_SR2_performance_100', 'E5_B128_R0_SR2_performance_110', 'E5_B128_R0_SR2_performance_120', 'E5_B128_R0_SR2_performance_180', 'E5_B128_R0_SR2_performance_260', 'E5_B128_R0_SR2_layer', 'E5_B128_R0_SR2_performance_250', 'E5_B128_R0_SR2_performance_210', 'E5_B128_R0_SR2_performance_280', 'E5_B128_R0_SR2_performance_220', 'E5_B128_R0_SR2_performance_160', 'E5_B128_R0_SR2_performance_150', 'E5_B128_R0_SR2_performance_300']\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "data_path = os.path.join(current_path, 'ModelsData/resnet18')\n",
    "data_folders = os.listdir(data_path)\n",
    "print(data_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E5_B128_R0_SR2_performance_140', 'E5_B128_R0_SR2_performance_310', 'E5_B128_R0_SR2_performance_170', 'E5_B128_R0_SR2_performance_320', 'E5_B128_R0_SR2_performance_290', 'E5_B128_R0_SR2_performance_230', 'E5_B128_R0_SR2_performance_200', 'E5_B128_R0_SR2_performance_240', 'E5_B128_R0_SR2_performance_270', 'E5_B128_R0_SR2_performance_130', 'E5_B128_R0_SR2_performance_190', 'E5_B128_R0_SR2_performance_100', 'E5_B128_R0_SR2_performance_110', 'E5_B128_R0_SR2_performance_120', 'E5_B128_R0_SR2_performance_180', 'E5_B128_R0_SR2_performance_260', 'E5_B128_R0_SR2_performance_250', 'E5_B128_R0_SR2_performance_210', 'E5_B128_R0_SR2_performance_280', 'E5_B128_R0_SR2_performance_220', 'E5_B128_R0_SR2_performance_160', 'E5_B128_R0_SR2_performance_150', 'E5_B128_R0_SR2_performance_300']\n",
      "['E5_B128_R0_SR2_performance_100', 'E5_B128_R0_SR2_performance_110', 'E5_B128_R0_SR2_performance_120', 'E5_B128_R0_SR2_performance_130', 'E5_B128_R0_SR2_performance_140', 'E5_B128_R0_SR2_performance_150', 'E5_B128_R0_SR2_performance_160', 'E5_B128_R0_SR2_performance_170', 'E5_B128_R0_SR2_performance_180', 'E5_B128_R0_SR2_performance_190', 'E5_B128_R0_SR2_performance_200', 'E5_B128_R0_SR2_performance_210', 'E5_B128_R0_SR2_performance_220', 'E5_B128_R0_SR2_performance_230', 'E5_B128_R0_SR2_performance_240', 'E5_B128_R0_SR2_performance_250', 'E5_B128_R0_SR2_performance_260', 'E5_B128_R0_SR2_performance_270', 'E5_B128_R0_SR2_performance_280', 'E5_B128_R0_SR2_performance_290', 'E5_B128_R0_SR2_performance_300', 'E5_B128_R0_SR2_performance_310', 'E5_B128_R0_SR2_performance_320']\n"
     ]
    }
   ],
   "source": [
    "# find out all the folder names with performance\n",
    "performance_folders = []\n",
    "for folder in data_folders:\n",
    "    if 'performance' in folder:\n",
    "        performance_folders.append(folder)\n",
    "print(performance_folders)\n",
    "\n",
    "# reorder the performance folders based on the last number in the\n",
    "#  folder name\n",
    "performance_folders.sort(key=lambda x: int(x.split('_')[-1]))\n",
    "print(performance_folders)\n",
    "\n",
    "data_folders = performance_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load all the model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the epoch number, batch size and the round number as well as the sampling rate\n",
    "epoch = 5\n",
    "batch_size = 128\n",
    "round_num = 0\n",
    "sampling_rate = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for all the folder names in the data folder, generate the path to the folder\n",
    "# # and print the path\n",
    "# folder_name = f'E{epoch}_B{batch_size}_R{round_num}_SR{sampling_rate}_layer'\n",
    "\n",
    "# data_dir = 'fashion_mnist'\n",
    "# # data_dir = 'cifar100'\n",
    "# # data_dir = 'cifar10'\n",
    "\n",
    "\n",
    "\n",
    "# print(folder_name)\n",
    "# for folder in data_folders:\n",
    "#     folder_path = os.path.join(data_path, folder)\n",
    "#     folder_path = os.path.join(folder_path, folder_name)\n",
    "#     folder_path = os.path.join(folder_path, data_dir)\n",
    "#     print(folder_path)\n",
    "\n",
    "#     # load the csv files \n",
    "#     energy_data = pd.read_csv(os.path.join(folder_path, 'energy_consumption_file.csv'))\n",
    "\n",
    "#     # load the npy files\n",
    "#     to_device = np.load(os.path.join(folder_path, 'to_device.npy'), allow_pickle=True)\n",
    "#     forward = np.load(os.path.join(folder_path, 'forward.npy'), allow_pickle=True)\n",
    "#     loss = np.load(os.path.join(folder_path, 'loss.npy'), allow_pickle=True)\n",
    "#     backward = np.load(os.path.join(folder_path, 'backward.npy'), allow_pickle=True)\n",
    "#     optimize = np.load(os.path.join(folder_path, 'optimize.npy'), allow_pickle=True)\n",
    "\n",
    "#     # Set the display format for floating-point numbers to avoid scientific notation\n",
    "#     pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "#     # Use the function to label the energy consumption data\n",
    "#     labeled_energy_data = label_energy_consumption(energy_data, to_device, forward, loss, backward, optimize)\n",
    "#     print(labeled_energy_data.head())\n",
    "\n",
    "#     # save the file to the folder\n",
    "#     labeled_energy_data.to_csv(os.path.join(folder_path, 'labeled_energy_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for all the folder names in the data folder, generate the path to the folder\n",
    "# # Find the index of 'alexnet' in data_folders\n",
    "# # alexnet_index = data_folders.index('alexnet')\n",
    "# resnet18_index = data_folders.index('resnet18')\n",
    "\n",
    "# # print(f\"Index of 'alexnet': {alexnet_index}\")\n",
    "# print(f\"Index of 'resnet18': {resnet18_index}\")\n",
    "\n",
    "# # and print the path\n",
    "# folder_name = f'E{epoch}_B{batch_size}_R{round_num}_SR{sampling_rate}_layer'\n",
    "\n",
    "# data_dir = 'fashion_mnist'\n",
    "# # data_dir = 'cifar100'\n",
    "# # data_dir = 'cifar10'\n",
    "# print(folder_name)\n",
    "\n",
    "# folder = data_folders[resnet18_index]\n",
    "\n",
    "# folder_path = os.path.join(data_path, folder)\n",
    "# folder_path = os.path.join(folder_path, folder_name)\n",
    "# folder_path = os.path.join(folder_path, data_dir)\n",
    "# # print(folder_path)\n",
    "\n",
    "# print(folder_path)\n",
    "\n",
    "# # load the csv files \n",
    "# energy_data = pd.read_csv(os.path.join(folder_path, 'energy_consumption_file.csv'))\n",
    "# # print(type(energy_data.iloc[0]['timestamp']))\n",
    "# energy_data['timestamp'] = pd.to_numeric(energy_data['timestamp'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# forward_layer_data = pd.read_csv(os.path.join(folder_path, 'layer_time.csv'))\n",
    "\n",
    "# # load the npy files\n",
    "# to_device = np.load(os.path.join(folder_path, 'to_device.npy'), allow_pickle=True)\n",
    "# forward = np.load(os.path.join(folder_path, 'forward.npy'), allow_pickle=True)\n",
    "# loss = np.load(os.path.join(folder_path, 'loss.npy'), allow_pickle=True)\n",
    "# backward = np.load(os.path.join(folder_path, 'backward.npy'), allow_pickle=True)\n",
    "# optimize = np.load(os.path.join(folder_path, 'optimize.npy'), allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Set the display format for floating-point numbers to avoid scientific notation\n",
    "# pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "\n",
    "# # Use the function to label the energy consumption data\n",
    "# labeled_energy_data = label_energy_consumption(energy_data, to_device, forward, loss, backward, optimize)\n",
    "\n",
    "\n",
    "# # Use the function to label the energy consumption data with the layer names\n",
    "# # labeled_energy_data_layer = label_energy_consumption_layer(energy_data, to_device, loss, backward, optimize, forward_layer_data)\n",
    "# # print(labeled_energy_data_layer.head())\n",
    "\n",
    "# # save the file to the folder\n",
    "# # labeled_energy_data_layer.to_csv(os.path.join(folder_path, 'labeled_energy_data_layer.csv'), index=False)\n",
    "# labeled_energy_data.to_csv(os.path.join(folder_path, 'labeled_energy_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dtjgp/Projects/GreenAI/4070/ModelsData/resnet18/E5_B128_R0_SR2_performance_100\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dtjgp/Projects/GreenAI/4070/ModelsData/resnet18/E5_B128_R0_SR2_performance_100/energy_consumption_file.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(folder_path)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# load the csv files \u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m energy_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy_consumption_file.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# print(type(energy_data.iloc[0]['timestamp']))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m energy_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(energy_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/greenai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/greenai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/greenai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/greenai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/greenai/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dtjgp/Projects/GreenAI/4070/ModelsData/resnet18/E5_B128_R0_SR2_performance_100/energy_consumption_file.csv'"
     ]
    }
   ],
   "source": [
    "# for all the folder names in the data folder, generate the path to the folder\n",
    "# Find the index of 'alexnet' in data_folders\n",
    "# alexnet_index = data_folders.index('alexnet')\n",
    "# resnet18_index = data_folders.index('resnet18')\n",
    "\n",
    "# # print(f\"Index of 'alexnet': {alexnet_index}\")\n",
    "# print(f\"Index of 'resnet18': {resnet18_index}\")\n",
    "\n",
    "# # and print the path\n",
    "# folder_name = f'E{epoch}_B{batch_size}_R{round_num}_SR{sampling_rate}_layer'\n",
    "\n",
    "data_dir = 'fashion_mnist'\n",
    "# # data_dir = 'cifar100'\n",
    "# # data_dir = 'cifar10'\n",
    "# print(folder_name)\n",
    "\n",
    "# folder = data_folders[resnet18_index]\n",
    "\n",
    "# folder_path = os.path.join(data_path, folder)\n",
    "# folder_path = os.path.join(folder_path, folder_name)\n",
    "# folder_path = os.path.join(folder_path, data_dir)\n",
    "# # print(folder_path)\n",
    "\n",
    "# print(folder_path)\n",
    "\n",
    "for folder in data_folders:\n",
    "    \n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    folder_path = os.path.join(folder_path, data_dir)\n",
    "    print(folder_path)\n",
    "    # load the csv files \n",
    "    energy_data = pd.read_csv(os.path.join(folder_path, 'energy_consumption_file.csv'))\n",
    "    # print(type(energy_data.iloc[0]['timestamp']))\n",
    "    energy_data['timestamp'] = pd.to_numeric(energy_data['timestamp'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "    forward_layer_data = pd.read_csv(os.path.join(folder_path, 'layer_time.csv'))\n",
    "\n",
    "    # load the npy files\n",
    "    to_device = np.load(os.path.join(folder_path, 'to_device.npy'), allow_pickle=True)\n",
    "    forward = np.load(os.path.join(folder_path, 'forward.npy'), allow_pickle=True)\n",
    "    loss = np.load(os.path.join(folder_path, 'loss.npy'), allow_pickle=True)\n",
    "    backward = np.load(os.path.join(folder_path, 'backward.npy'), allow_pickle=True)\n",
    "    optimize = np.load(os.path.join(folder_path, 'optimize.npy'), allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Set the display format for floating-point numbers to avoid scientific notation\n",
    "    pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "\n",
    "    # Use the function to label the energy consumption data\n",
    "    labeled_energy_data = label_energy_consumption(energy_data, to_device, forward, loss, backward, optimize)\n",
    "\n",
    "\n",
    "    # Use the function to label the energy consumption data with the layer names\n",
    "    # labeled_energy_data_layer = label_energy_consumption_layer(energy_data, to_device, loss, backward, optimize, forward_layer_data)\n",
    "    # print(labeled_energy_data_layer.head())\n",
    "\n",
    "    # save the file to the folder\n",
    "    # labeled_energy_data_layer.to_csv(os.path.join(folder_path, 'labeled_energy_data_layer.csv'), index=False)\n",
    "    labeled_energy_data.to_csv(os.path.join(folder_path, 'labeled_energy_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreenAI",
   "language": "python",
   "name": "greenai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
